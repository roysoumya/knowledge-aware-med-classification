{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RqqBi37-1Oo5",
    "outputId": "fbf416a2-0eff-4ed2-e702-e21511745dd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 19 14:04:15 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   43C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bj_RnTd6pB5Q",
    "outputId": "c204108f-e3fa-4cec-8faf-c1b816020b9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 80990, done.\u001b[K\n",
      "remote: Counting objects: 100% (220/220), done.\u001b[K\n",
      "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
      "remote: Total 80990 (delta 96), reused 156 (delta 64), pack-reused 80770\u001b[K\n",
      "Receiving objects: 100% (80990/80990), 63.28 MiB | 28.77 MiB/s, done.\n",
      "Resolving deltas: 100% (57947/57947), done.\n",
      "/content/knowledge-aware-med-classification/transformers\n",
      "Note: checking out '896a0eb1fd861bc37097a9b669ebf4cb8d523de7'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b <new-branch-name>\n",
      "\n",
      "HEAD is now at 896a0eb1f Merge pull request #2459 from Perseus14/patch-4\n",
      "/content/knowledge-aware-med-classification\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.19.5)\n",
      "Collecting tokenizers==0.0.11\n",
      "  Downloading tokenizers-0.0.11-cp37-cp37m-manylinux1_x86_64.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 7.5 MB/s \n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading boto3-1.18.24-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 70.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.62.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (2019.12.20)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 56.2 MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 60.8 MB/s \n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 11.6 MB/s \n",
      "\u001b[?25hCollecting botocore<1.22.0,>=1.21.24\n",
      "  Downloading botocore-1.21.24-py3-none-any.whl (7.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8 MB 56.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.24->boto3->-r requirements.txt (line 3)) (2.8.2)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 76.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.24->boto3->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 5)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 5)) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 5)) (3.0.4)\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 70.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->-r requirements.txt (line 9)) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->-r requirements.txt (line 9)) (1.0.1)\n",
      "Installing collected packages: urllib3, jmespath, botocore, s3transfer, tokenizers, sentencepiece, sacremoses, boto3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Successfully installed boto3-1.18.24 botocore-1.21.24 jmespath-0.10.0 s3transfer-0.5.0 sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.0.11 urllib3-1.25.11\n",
      "/content/knowledge-aware-med-classification/transformers\n",
      "Processing /content/knowledge-aware-med-classification/transformers\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (1.19.5)\n",
      "Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (0.0.11)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (1.18.24)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (4.62.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (2019.12.20)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (0.1.96)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (0.0.45)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.3.0) (0.5.0)\n",
      "Requirement already satisfied: botocore<1.22.0,>=1.21.24 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.3.0) (1.21.24)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.3.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.24->boto3->transformers==2.3.0) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.24->boto3->transformers==2.3.0) (1.25.11)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.24->boto3->transformers==2.3.0) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.3.0) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.3.0) (1.0.1)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-2.3.0-py3-none-any.whl size=463151 sha256=5b4bb33a113270e79360abc2f24d2e7aa09eceaa1ff842d96ecead30b914a6d5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fednk5kf/wheels/c1/35/b1/8363a35d9044381a19e7e426208cdc8a032ea2d4fef3c4a2b3\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-2.3.0\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers.git #cloning hugging face transformer\n",
    "%cd transformers/\n",
    "!git checkout 896a0eb1fd861bc37097a9b669ebf4cb8d523de7\n",
    "%cd ..\n",
    "!pip install -r requirements.txt # installing all the requirements of the model\n",
    "!cp -a data* transformers/ # copying the data from the root directory to root/transformers/data/ directory\n",
    "!cp -a src* transformers/\n",
    "%cd transformers/\n",
    "!mkdir examples/ichi/  # creating this folder to store medbert codes\n",
    "\n",
    "!pip install .\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-D79M5NpS8vh"
   },
   "outputs": [],
   "source": [
    "# ########### Uncomment it to download Glove Vectors\n",
    "\n",
    "#!curl -O -J -L http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "#!unzip glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-EzqBB_PvmT"
   },
   "outputs": [],
   "source": [
    "# ########### Uncomment it to download BioASQ Embeddings\n",
    "\n",
    "# !curl -O -J -L http://bioasq.lip6.fr/tools/BioASQword2vec/\n",
    "# !tar -xvzf biomedicalWordVectors.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cG8YjUsTm2Eg",
    "outputId": "6f155e2a-170a-4c48-8eb1-146df11ab5e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTRIBUTING.md\t\t     examples\t  notebooks  templates\n",
      "data\t\t\t     hubconf.py   README.md  tests\n",
      "deploy_multi_version_doc.sh  LICENSE\t  setup.cfg  transformers-cli\n",
      "docker\t\t\t     Makefile\t  setup.py   utils\n",
      "docs\t\t\t     MANIFEST.in  src\t     valohai.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zs47BPeL5fbQ"
   },
   "source": [
    "#####creating utility functions to run th medBert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nGkNpSjDrXjh",
    "outputId": "b544a1f7-f716-4d16-b8b7-722926643b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./examples/ichi/utils_ichi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./examples/ichi/utils_ichi.py\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "from transformers.file_utils import is_tf_available \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "if is_tf_available():\n",
    "    import tensorflow as tf\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    return acc_and_f1(preds, labels)\n",
    "\n",
    "\n",
    "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='pre', truncating='post', value=0):\n",
    "    x = (np.ones(maxlen) * value).astype(dtype)\n",
    "    if truncating == 'pre':\n",
    "        trunc = sequence[-maxlen:]\n",
    "    else:\n",
    "        trunc = sequence[:maxlen]\n",
    "    trunc = np.asarray(trunc, dtype=dtype)\n",
    "    if padding == 'post':\n",
    "        x[:len(trunc)] = trunc\n",
    "    else:\n",
    "        x[-len(trunc):] = trunc\n",
    "    return x\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self, max_seq_len, max_num_words=None,lower=True):\n",
    "        self.lower = lower\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.word2idx = {}\n",
    "        self.word_freq = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx2word[0] = '<PAD>'\n",
    "        self.word2idx['<PAD>'] = 0\n",
    "        self.word_freq['<PAD>'] = 100000\n",
    "        self.idx = 1\n",
    "        self.max_num_words = max_num_words\n",
    "\n",
    "    def fit_on_text(self, text):\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = self.idx\n",
    "                self.word_freq[word] = 1\n",
    "                self.idx2word[self.idx] = word\n",
    "                self.idx += 1\n",
    "            else:\n",
    "                self.word_freq[word] = self.word_freq[word] + 1\n",
    "    \n",
    "    def update_tokenizer(self):\n",
    "        if self.max_num_words == None:\n",
    "            return\n",
    "        elif self.max_num_words >= self.idx:\n",
    "            return \n",
    "        else:\n",
    "            del self.word_freq['<PAD>']\n",
    "            self.word_freq = {k: v for k, v in sorted(self.word_freq.items(), key=lambda item: item[1], reverse=True)}\n",
    "            self.word2idx = {}\n",
    "            self.idx2word = {}\n",
    "            self.idx2word[0] = '<PAD>'\n",
    "            self.word2idx['<PAD>'] = 0\n",
    "            self.idx = 1\n",
    "            for i, key in enumerate(self.word_freq):\n",
    "                if i >= self.max_num_words:\n",
    "                    break\n",
    "                else:\n",
    "                    self.word2idx[key] = i+1\n",
    "                    self.idx2word[i+1] = key\n",
    "                    self.idx += 1\n",
    "            self.word_freq['<PAD>'] = 100000\n",
    "\n",
    "\n",
    "\n",
    "    def fit_on_examples(self, examples):\n",
    "        is_tf_dataset = False\n",
    "        if is_tf_available() and isinstance(examples, tf.data.Dataset):\n",
    "            is_tf_dataset = True\n",
    "        processor = ICHIProcessor()\n",
    "        for example in examples:\n",
    "            if is_tf_dataset:\n",
    "                example = processor.get_example_from_tensor_dict(example)\n",
    "                example = processor.tfds_map(example)\n",
    "            self.fit_on_text(example.clean_text)\n",
    "\n",
    "    def text_to_sequence(self, text, reverse=False, padding='pre', truncating='post'):\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "        words = text.split()\n",
    "        unknownidx = len(self.word2idx)+1\n",
    "        if (self.max_num_words == None) or (self.max_num_words >= self.idx):\n",
    "            sequence = [self.word2idx[w] if w in self.word2idx else unknownidx for w in words]\n",
    "        else:\n",
    "            sequence = []\n",
    "            for w in words:\n",
    "                if w in self.word2idx:\n",
    "                    if self.word2idx[w] > self.max_num_words:\n",
    "                        sequence.append(unknownidx)\n",
    "                    else:\n",
    "                        sequence.append(self.word2idx[w])\n",
    "                else:\n",
    "                    sequence.append(unknownidx)\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)\n",
    "\n",
    "def _load_word_vec_glove(path, word2idx=None):\n",
    "    fin = open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    word_vec = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split()\n",
    "        if word2idx is None or tokens[0] in word2idx.keys():\n",
    "            try:\n",
    "                word_vec[tokens[0]] = np.asarray(tokens[1:], dtype='float32')\n",
    "            except:\n",
    "                pass\n",
    "    return word_vec\n",
    "    \n",
    "def build_embedding_matrix_glove(word2idx, embed_dim, dat_fname, fname):\n",
    "    if os.path.exists(dat_fname):\n",
    "        print('loading embedding_matrix(glove): ', dat_fname)\n",
    "        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n",
    "    else:\n",
    "        print('loading word vectors...(glove)')\n",
    "        embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n",
    "        fname = fname + '/glove.twitter.27B/glove.twitter.27B.' + str(embed_dim) + 'd.txt' \\\n",
    "            if embed_dim != 300 else fname + '/glove.840B.300d.txt'\n",
    "        word_vec = _load_word_vec_glove(fname, word2idx=word2idx)\n",
    "        print('building embedding_matrix:', dat_fname)\n",
    "        for word, i in word2idx.items():\n",
    "            vec = word_vec.get(word)\n",
    "            if vec is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = vec\n",
    "        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))\n",
    "    return embedding_matrix\n",
    "\n",
    "def build_embedding_matrix_BioASQ(word2idx, embed_dim, dat_fname, fname):\n",
    "    if os.path.exists(dat_fname):\n",
    "        print('loading embedding_matrix(BioASQ):', dat_fname)\n",
    "        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n",
    "    else:\n",
    "        print('loading word vectors...(BioASQ)')\n",
    "        embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n",
    "        f = open(fname + \"/word2vecTools/types.txt\",\"r\")\n",
    "        i = 0\n",
    "        names = []\n",
    "        for line in f:\n",
    "            names.append(line.split('\\n')[0])\n",
    "            i = i + 1\n",
    "        vectors = np.loadtxt(fname + \"/word2vecTools/vectors.txt\")\n",
    "        word_vec = {}\n",
    "        for (index, name) in enumerate(names):\n",
    "            word_vec[name] = index\n",
    "\n",
    "        print('building embedding_matrix:', dat_fname)\n",
    "        for word, i in word2idx.items():\n",
    "            if word in word_vec.keys():\n",
    "                vec = vectors[word_vec[word]]\n",
    "            else:\n",
    "                vec = None\n",
    "            if vec is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = vec\n",
    "        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))\n",
    "    return embedding_matrix\n",
    "\n",
    "def convert_examples_to_features(\n",
    "    examples,\n",
    "    tokenizer,\n",
    "    tokenizer_cleantext,\n",
    "    max_length=512,\n",
    "    task=None,\n",
    "    label_list=None,\n",
    "    output_mode=None,\n",
    "    pad_on_left=False,\n",
    "    pad_token=0,\n",
    "    pad_token_segment_id=0,\n",
    "    mask_padding_with_zero=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a data file into a list of ``ICHI_InputFeatures``\n",
    "\n",
    "    Args:\n",
    "        examples: List of ``ICHI_InputExamples`` or ``tf.data.Dataset`` containing the examples.\n",
    "        tokenizer: Instance of a tokenizer that will tokenize the examples\n",
    "        tokenizer_cleantext: Instance of a tokenizer that will tokenize the examples clean text(used for our medical module) \n",
    "        max_length: Maximum example length\n",
    "        task: GLUE task\n",
    "        label_list: List of labels. Can be obtained from the processor using the ``processor.get_labels()`` method\n",
    "        output_mode: String indicating the output mode. Either ``regression`` or ``classification``\n",
    "        pad_on_left: If set to ``True``, the examples will be padded on the left rather than on the right (default)\n",
    "        pad_token: Padding token\n",
    "        pad_token_segment_id: The segment ID for the padding token (It is usually 0, but can vary such as for XLNet where it is 4)\n",
    "        mask_padding_with_zero: If set to ``True``, the attention mask will be filled by ``1`` for actual values\n",
    "            and by ``0`` for padded values. If set to ``False``, inverts it (``1`` for padded values, ``0`` for\n",
    "            actual values)\n",
    "\n",
    "    Returns:\n",
    "        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``\n",
    "        containing the task-specific features. If the input is a list of ``ICHI_InputExamples``, will return\n",
    "        a list of task-specific ``ICHI_InputFeatures`` which can be fed to the model.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    is_tf_dataset = False\n",
    "    if is_tf_available() and isinstance(examples, tf.data.Dataset):\n",
    "        is_tf_dataset = True\n",
    "\n",
    "    \"\"\"      Initialisation of Data Processor    \"\"\"\n",
    "    if task is not None:\n",
    "        processor = ICHIProcessor()  \n",
    "        if label_list is None:\n",
    "            label_list = processor.get_labels()\n",
    "            logger.info(\"Using label list %s for task %s\" % (label_list, task))\n",
    "        if output_mode is None:\n",
    "            output_mode = \"classification\"\n",
    "            logger.info(\"Using output mode %s for task %s\" % (output_mode, task))\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    \"\"\"      Processing the examples    \"\"\"\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logger.info(\"Writing example %d/%d\" % (ex_index, len(examples)))\n",
    "        if is_tf_dataset:\n",
    "            example = processor.get_example_from_tensor_dict(example)\n",
    "            example = processor.tfds_map(example)\n",
    "        \"\"\"      Assuming that each aspect consist of max 4 tokens hence making sure that aspects total tokens coming from aspects are not greater than max sequence length    \"\"\"\n",
    "        aspect_present = bool(example.aspects is not None)\n",
    "        if aspect_present:\n",
    "            if 4 * len(example.aspects) > max_length:\n",
    "                example.aspects = random.sample(example.aspects, k = int(max_length/4))\n",
    "                \n",
    "        \"\"\"      Using tokenizer.encode_plus_lcf the global features are encoded as [<special token> + text_tokens + <special token> + heading_tokens + <special token> + aspect1_token + <special token> + .... + aspectn_token + <special token>] \"\"\"\n",
    "        inputs_global = tokenizer.encode_plus_lcf(example.text, example.heading, example.aspects, add_special_tokens=True, max_length=max_length,) ######## edittttttt\n",
    "        \"\"\"      Using tokenizer.encode_plus_lcf the local features(medical module) are encoded as [<special token> + text_tokens + <special token>] \"\"\"\n",
    "        inputs_local = tokenizer.encode_plus_lcf(example.text, add_special_tokens=True, max_length=max_length,)\n",
    "        \"\"\" Generating tokens for the clean_text i.e. tokenising text based on glove or BioASQ \"\"\"\n",
    "        text_clean_indices = tokenizer_cleantext.text_to_sequence(example.clean_text)\n",
    "        \n",
    "        \"\"\"      Using tokenizer.encode_plus_lcf the aspect_indices are encoded as [<special token> + aspect1_token + <special token> + .... + aspectn_token + <special token>] \"\"\"\n",
    "        if len(example.aspects) > 2:\n",
    "            aspect_indices = tokenizer.encode_plus_lcf(example.aspects[0], example.aspects[1], example.aspects[2:], add_special_tokens=False, max_length=max_length,)\n",
    "        elif len(example.aspects) == 2:\n",
    "            aspect_indices = tokenizer.encode_plus_lcf(example.aspects[0], example.aspects[1], add_special_tokens=False, max_length=max_length,)\n",
    "        else:\n",
    "            aspect_indices = tokenizer.encode_plus_lcf(example.aspects[0], add_special_tokens=False, max_length=max_length,)\n",
    "        \n",
    "        input_global_ids, token_global_type_ids = inputs_global[\"input_ids\"], inputs_global[\"token_type_ids\"]\n",
    "        input_local_ids, token_local_type_ids = inputs_local[\"input_ids\"], inputs_local[\"token_type_ids\"]\n",
    "        \n",
    "        aspect_indices = aspect_indices[\"input_ids\"]\n",
    "        padding_length_aspect = max_length - len(aspect_indices)\n",
    "        aspect_indices = aspect_indices + ([pad_token] * padding_length_aspect)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        attention_mask_global = [1 if mask_padding_with_zero else 0] * len(input_global_ids)\n",
    "        attention_mask_local = [1 if mask_padding_with_zero else 0] * len(input_local_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length_global = max_length - len(input_global_ids)\n",
    "        padding_length_local = max_length - len(input_local_ids)\n",
    "        if pad_on_left:\n",
    "            input_global_ids = ([pad_token] * padding_length_global) + input_global_ids\n",
    "            attention_mask_global = ([0 if mask_padding_with_zero else 1] * padding_length_global) + attention_mask_global\n",
    "            token_global_type_ids = ([pad_token_segment_id] * padding_length_global) + token_global_type_ids\n",
    "            input_local_ids = ([pad_token] * padding_length_local) + input_local_ids\n",
    "            attention_mask_local = ([0 if mask_padding_with_zero else 1] * padding_length_local) + attention_mask_local\n",
    "            token_local_type_ids = ([pad_token_segment_id] * padding_length_local) + token_local_type_ids\n",
    "        else:\n",
    "            input_global_ids = input_global_ids + ([pad_token] * padding_length_global)\n",
    "            attention_mask_global = attention_mask_global + ([0 if mask_padding_with_zero else 1] * padding_length_global)\n",
    "            token_global_type_ids = token_global_type_ids + ([pad_token_segment_id] * padding_length_global)\n",
    "            input_local_ids = input_local_ids + ([pad_token] * padding_length_local)\n",
    "            attention_mask_local = attention_mask_local + ([0 if mask_padding_with_zero else 1] * padding_length_local)\n",
    "            token_local_type_ids = token_local_type_ids + ([pad_token_segment_id] * padding_length_local)\n",
    "\n",
    "        assert len(input_global_ids) == max_length, \"Error with input length {} vs {}\".format(len(input_global_ids), max_length)\n",
    "        assert len(input_local_ids) == max_length, \"Error with input length {} vs {}\".format(len(input_local_ids), max_length)\n",
    "        assert len(attention_mask_global) == max_length, \"Error with input length {} vs {}\".format(\n",
    "            len(attention_mask_global), max_length\n",
    "        )\n",
    "        assert len(attention_mask_local) == max_length, \"Error with input length {} vs {}\".format(\n",
    "            len(attention_mask_local), max_length\n",
    "        )\n",
    "        assert len(token_global_type_ids) == max_length, \"Error with input length {} vs {}\".format(\n",
    "            len(token_global_type_ids), max_length\n",
    "        )\n",
    "        assert len(token_local_type_ids) == max_length, \"Error with input length {} vs {}\".format(\n",
    "            len(token_local_type_ids), max_length\n",
    "        )\n",
    "\n",
    "        if output_mode == \"classification\":\n",
    "            label = label_map[example.label]\n",
    "        elif output_mode == \"regression\":\n",
    "            label = float(example.label)\n",
    "        else:\n",
    "            raise KeyError(output_mode)\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"input_global_ids: %s\" % \" \".join([str(x) for x in input_global_ids]))\n",
    "            logger.info(\"attention_mask_global: %s\" % \" \".join([str(x) for x in attention_mask_global]))\n",
    "            logger.info(\"token_global_type_ids: %s\" % \" \".join([str(x) for x in token_global_type_ids]))\n",
    "            logger.info(\"input_local_ids: %s\" % \" \".join([str(x) for x in input_local_ids]))\n",
    "            logger.info(\"attention_mask_local: %s\" % \" \".join([str(x) for x in attention_mask_local]))\n",
    "            logger.info(\"token_local_type_ids: %s\" % \" \".join([str(x) for x in token_local_type_ids]))\n",
    "            logger.info(\"text_clean_indices: %s\" % \" \".join([str(x) for x in text_clean_indices]))\n",
    "            logger.info(\"aspect_indices: %s\" % \" \".join([str(x) for x in aspect_indices]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label))\n",
    "\n",
    "        features.append(\n",
    "            ICHI_InputFeatures(\n",
    "                input_global_ids=input_global_ids, input_local_ids=input_local_ids, attention_mask_global=attention_mask_global, attention_mask_local=attention_mask_local,\n",
    "                token_global_type_ids=token_global_type_ids, token_local_type_ids=token_local_type_ids, text_clean_indices=text_clean_indices, aspect_indices=aspect_indices, label=label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if is_tf_available() and is_tf_dataset:\n",
    "\n",
    "        def gen():\n",
    "            for ex in features:\n",
    "                yield (\n",
    "                    {\n",
    "                        \"input_global_ids\": ex.input_global_ids,\n",
    "                        \"input_local_ids\": ex.input_local_ids,\n",
    "                        \"attention_mask_global\": ex.attention_mask_global,\n",
    "                        \"attention_mask_local\": ex.attention_mask_local,\n",
    "                        \"token_global_type_ids\": ex.token_global_type_ids,\n",
    "                        \"token_local_type_ids\": ex.token_local_type_ids,\n",
    "                        \"text_clean_indices\": ex.text_clean_indices,\n",
    "                        \"aspect_indices\": ex.aspect_indices,\n",
    "                    },\n",
    "                    ex.label,\n",
    "                )\n",
    "\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            gen,\n",
    "            ({\"input_global_ids\": tf.int32, \"input_local_ids\": tf.int32, \"attention_mask_global\": tf.int32, \"attention_mask_local\": tf.int32, \"token_global_type_ids\": tf.int32, \"token_local_type_ids\": tf.int32, \"text_clean_indices\": tf.int32, \"aspect_indices\": tf.int32}, tf.int64),\n",
    "            (\n",
    "                {\n",
    "                    \"input_global_ids\": tf.TensorShape([None]),\n",
    "                    \"input_local_ids\": tf.TensorShape([None]),\n",
    "                    \"attention_mask_global\": tf.TensorShape([None]),\n",
    "                    \"attention_mask_local\": tf.TensorShape([None]),\n",
    "                    \"token_global_type_ids\": tf.TensorShape([None]),\n",
    "                    \"token_local_type_ids\": tf.TensorShape([None]),\n",
    "                    \"text_clean_indices\": tf.TensorShape([None]),\n",
    "                    \"aspect_indices\": tf.TensorShape([None]),\n",
    "                },\n",
    "                tf.TensorShape([]),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return features\n",
    "\n",
    "class ICHIProcessor(object):\n",
    "    \"\"\"Processor for the ICHI data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return ICHI_InputExample(tensor_dict['idx'].numpy(),\n",
    "                            tensor_dict['heading'].numpy().decode('utf-8'),\n",
    "                            tensor_dict['text'].numpy().decode('utf-8'),\n",
    "                            tensor_dict['clean_text'].numpy().decode('utf-8'),\n",
    "                            tensor_dict['aspects'].numpy().decode('utf-8'),\n",
    "                            str(tensor_dict['label'].numpy()))\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"final_train_result.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"final_test_result.tsv\")),\n",
    "            \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"DEMO\", \"DISE\", \"TRMT\", \"GOAL\", \"PREG\", \"FAML\", \"SOCL\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            label = line[0]\n",
    "            heading = None\n",
    "            text = line[1] + ' ' + line[2]\n",
    "            try:\n",
    "                aspects = [' ' + x for x in line[3].split('|')]\n",
    "                temp_clean_text = \" \".join(line[3].split('|'))\n",
    "            except:\n",
    "                aspects = None\n",
    "                temp_clean_text = \"\"\n",
    "                print(\"Sed\")\n",
    "            \n",
    "            \"\"\"    IMPORTANT    IMPORTANT    IMPORTANT    IMPORTANT    IMPORTANT   \"\"\"\n",
    "            clean_text = self.clean_str( text, lemmatizer) ######## for using glove embeddings over sentence\n",
    "            # clean_text = self.clean_str( temp_clean_text, lemmatizer) ##### for using BioASQ embeddings in aspects\n",
    "            examples.append(\n",
    "                ICHI_InputExample(guid=guid, heading=heading, text=text, clean_text=clean_text, aspects=aspects, label=label))\n",
    "        return examples\n",
    "\n",
    "    def tfds_map(self, example):\n",
    "        \"\"\"Some tensorflow_datasets datasets are not formatted the same way the GLUE datasets are.\n",
    "        This method converts examples to the correct format.\"\"\"\n",
    "        if len(self.get_labels()) > 1:\n",
    "            example.label = self.get_labels()[int(example.label)]\n",
    "        return example\n",
    "\n",
    "    def clean_str(self, string1, lemmatizer):\n",
    "        \"\"\"\n",
    "        Tokenization/string cleaning for dataset\n",
    "        Every dataset is lower cased except\n",
    "        \"\"\"\n",
    "        str_stop = \"\"\n",
    "        string1 = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '',string1)\n",
    "        string1 = re.sub(r\"\\\\\", \" \", string1)    \n",
    "        string1 = re.sub(r\"\\'\", \" \", string1)    \n",
    "        string1 = re.sub(r\"\\\"\", \" \", string1)   \n",
    "        string1 = re.sub(r'(\\W)\\1+', r'\\1', string1)\n",
    "        word_list=string1.split(\" \")\n",
    "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "        for kj in filtered_words:\n",
    "            new=lemmatizer.lemmatize(str(kj)) \n",
    "            str_stop=str_stop +\" \"+new\n",
    "            str_stop.encode('utf-8')\n",
    "        return str_stop.strip().lower()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))\n",
    "\n",
    "class ICHI_InputExample(object):\n",
    "    \"\"\"\n",
    "    A single training/test example for simple sequence classification.\n",
    "\n",
    "    Args:\n",
    "        guid: Unique id for the example.\n",
    "        heading: string. The untokenized heading of the sequence\n",
    "        text: string. The untokenized text part of the sequence.\n",
    "        clean_text: string. The untokenized text part of the sequence used for medical module(as tokenization will be different for glove and BioASQ than BERT).\n",
    "        aspects: list of string. The untokenized aspects for the sequence as a list of strings\n",
    "        Only must be specified for sequence pair tasks.\n",
    "        label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, guid, heading, text, clean_text, aspects, label=None):\n",
    "        self.guid = guid\n",
    "        self.heading = heading\n",
    "        self.text = text\n",
    "        self.clean_text = clean_text\n",
    "        self.aspects = aspects\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "\n",
    "class ICHI_InputFeatures(object):\n",
    "    \"\"\"\n",
    "    A single set of features of data.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Indices of input sequence tokens in the vocabulary.\n",
    "        attention_mask: Mask to avoid performing attention on padding token indices.\n",
    "            Mask values selected in ``[0, 1]``:\n",
    "            Usually  ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded) tokens.\n",
    "        token_type_ids: Segment token indices to indicate first and second portions of the inputs.\n",
    "        label: Label corresponding to the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_global_ids, input_local_ids, attention_mask_global=None, attention_mask_local=None, token_global_type_ids=None, token_local_type_ids=None, text_clean_indices=None, aspect_indices=None, label=None):\n",
    "        self.input_global_ids = input_global_ids\n",
    "        self.attention_mask_global = attention_mask_global\n",
    "        self.token_global_type_ids = token_global_type_ids\n",
    "        self.input_local_ids = input_local_ids\n",
    "        self.attention_mask_local = attention_mask_local\n",
    "        self.token_local_type_ids = token_local_type_ids\n",
    "        self.text_clean_indices = text_clean_indices\n",
    "        self.aspect_indices = aspect_indices\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "\n",
    "def load_and_cache_examples(args, task, tokenizer, tokenizer_cleantext, evaluate=False):\n",
    "    if args.local_rank not in [-1, 0] and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    processor = ICHIProcessor()  \n",
    "    output_mode = \"classification\"\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cached_{}_{}_{}_{}\".format(\n",
    "            \"dev\" if evaluate else \"train\",\n",
    "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
    "            str(args.max_seq_length),\n",
    "            str(task),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    \"\"\" Load saved tokenizer for the clean_text\"\"\"\n",
    "    cached_tokenizer_cleantext_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cachedtokenizer_{}_{}_{}\".format(\n",
    "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
    "            str(args.embedding_type),\n",
    "            str(task),\n",
    "        ),\n",
    "    )\n",
    "    \"\"\" if overwrite cache is disabled and saved feature and tokenizer file exists then load from the saved file else process them \"\"\"\n",
    "    if os.path.exists(cached_features_file) and os.path.exists(cached_tokenizer_cleantext_file) and not args.overwrite_cache:\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "        print('loading tokenizer from cache:', cached_tokenizer_cleantext_file)\n",
    "        tokenizer_cleantext = pickle.load(open(cached_tokenizer_cleantext_file, 'rb'))\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "        label_list = processor.get_labels()\n",
    "        if task in [\"mnli\", \"mnli-mm\"] and args.model_type in [\"roberta\", \"xlmroberta\"]:\n",
    "            # HACK(label indices are swapped in RoBERTa pretrained model)\n",
    "            label_list[1], label_list[2] = label_list[2], label_list[1]\n",
    "        examples = (\n",
    "            processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n",
    "        )\n",
    "        if (not evaluate):\n",
    "            tokenizer_cleantext.fit_on_examples(examples)\n",
    "            tokenizer_cleantext.update_tokenizer()\n",
    "        \"\"\" features are created using this function which is defined above\"\"\"\n",
    "        features = convert_examples_to_features(\n",
    "            examples,\n",
    "            tokenizer,\n",
    "            tokenizer_cleantext,\n",
    "            label_list=label_list,\n",
    "            max_length=args.max_seq_length,\n",
    "            output_mode=output_mode,\n",
    "            pad_on_left=bool(args.model_type in [\"xlnet\"]),  # pad on the left for xlnet\n",
    "            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "            pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n",
    "        )\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            torch.save(features, cached_features_file)\n",
    "            pickle.dump(tokenizer_cleantext, open(cached_tokenizer_cleantext_file, 'wb'))\n",
    "\n",
    "    if args.local_rank == 0 and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_global_ids = torch.tensor([f.input_global_ids for f in features], dtype=torch.long)\n",
    "    all_input_local_ids = torch.tensor([f.input_local_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask_global = torch.tensor([f.attention_mask_global for f in features], dtype=torch.long)\n",
    "    all_attention_mask_local = torch.tensor([f.attention_mask_local for f in features], dtype=torch.long)\n",
    "    all_token_global_type_ids = torch.tensor([f.token_global_type_ids for f in features], dtype=torch.long)\n",
    "    all_token_local_type_ids = torch.tensor([f.token_local_type_ids for f in features], dtype=torch.long)\n",
    "    all_text_clean_indices = torch.tensor([f.text_clean_indices for f in features], dtype=torch.long)\n",
    "    all_aspect_indices = torch.tensor([f.aspect_indices for f in features], dtype=torch.long)\n",
    "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "    \n",
    "    dataset = TensorDataset(all_input_global_ids, all_input_local_ids, all_attention_mask_global, all_attention_mask_local, all_token_global_type_ids, all_token_local_type_ids, all_text_clean_indices, all_aspect_indices, all_labels)\n",
    "    \n",
    "    return dataset, tokenizer_cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wO6DCMAO5uWV"
   },
   "source": [
    "###### creating lcf_ichi code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyy-AEpoS8Lx",
    "outputId": "4a8aea7d-cfa8-4cff-8475-5bb48cf88730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./examples/ichi/lcf_ichi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./examples/ichi/lcf_ichi.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import numpy as np\n",
    "from transformers.modeling_bert import BertPooler, BertSelfAttention, BertPreTrainedModel, BertModel\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "def compute_average_with_padding(tensor, padding):\n",
    "    \"\"\"\n",
    "    :param tensor: dimension batch_size, seq_length, hidden_size\n",
    "    :param padding: dimension batch_size, seq_length\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    batch_size, seq_length, emb_size = tensor.shape\n",
    "    entry_sizes = torch.sum(padding, axis=1)\n",
    "    return torch.sum(tensor, axis=1) / entry_sizes\n",
    "\n",
    "\n",
    "class BertMaxPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states, mask):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = compute_average_with_padding(hidden_states, mask)\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config, args):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.args = args\n",
    "        self.SA = BertSelfAttention(config)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        zero_tensor = torch.tensor(np.zeros((inputs.size(0), 1, 1, self.args.max_seq_length),\n",
    "                                            dtype=np.float32), dtype=torch.float32).to(self.args.device)\n",
    "        SA_out = self.SA(inputs, zero_tensor)\n",
    "        return self.tanh(SA_out[0])\n",
    "\n",
    "\n",
    "class lcf_BERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, args):\n",
    "        super(lcf_BERT, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.args = args\n",
    "        self.config =config\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.bert_SA = SelfAttention(config, args) \n",
    "        self.linear_double_cdm_or_cdw = nn.Linear(config.hidden_size * 2,config.hidden_size)\n",
    "        self.linear_triple_lcf_global = nn.Linear(config.hidden_size * 3, config.hidden_size)\n",
    "        self.bert_pooler_org = BertPooler(config)\n",
    "        self.bert_pooler = BertMaxPooler(config)\n",
    "        self.dense = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.gelu(conv(x)).squeeze(3) # (n, Co, W)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def feature_dynamic_mask(self, text_local_indices, aspect_indices):\n",
    "        texts = text_local_indices\n",
    "        # mask_len = self.args.SRD\n",
    "        masked_text_raw_indices = torch.zeros((text_local_indices.size(0), self.args.max_seq_length, self.config.hidden_size),\n",
    "                                          dtype=torch.float)\n",
    "        \n",
    "        masked_text_raw_indices[:, 0, :] = torch.ones((text_local_indices.size(0), self.config.hidden_size), dtype=torch.float) \n",
    "        zero_tensor = torch.tensor(0).to(self.args.device)\n",
    "        for i in range(aspect_indices.shape[0]):\n",
    "            for j in range(aspect_indices[i].shape[0]):\n",
    "                if aspect_indices[i][j] == zero_tensor:\n",
    "                    break\n",
    "                else:\n",
    "                    indices = (text_local_indices[i] == aspect_indices[i][j]).nonzero()\n",
    "                    for k in indices:\n",
    "                        masked_text_raw_indices[i][k] = torch.ones(self.config.hidden_size, dtype=torch.float)\n",
    "        return masked_text_raw_indices.to(self.args.device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_global_ids=None,\n",
    "        attention_mask_global=None,\n",
    "        token_global_type_ids=None,\n",
    "        input_local_ids=None,\n",
    "        attention_mask_local=None,\n",
    "        token_local_type_ids=None,\n",
    "        text_clean_indices=None,\n",
    "        aspect_indices=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "\n",
    "        global_outputs, _ = self.bert(\n",
    "            input_global_ids,\n",
    "            attention_mask=attention_mask_global,\n",
    "            token_type_ids=token_global_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        local_outputs, _ = self.bert(\n",
    "            input_local_ids,\n",
    "            attention_mask=attention_mask_local,\n",
    "            token_type_ids=token_local_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        \n",
    "        if self.args.local_context_focus == 'cdm':\n",
    "            masked_local_text_vec = self.feature_dynamic_mask(input_local_ids, aspect_indices)\n",
    "            local_outputs = torch.mul(local_outputs, masked_local_text_vec)\n",
    "\n",
    "        elif self.args.local_context_focus == 'cdw':\n",
    "            weighted_text_local_features = self.feature_dynamic_weighted(input_local_ids, aspect_indices)\n",
    "            local_outputs = torch.mul(local_outputs, weighted_text_local_features)\n",
    "            out_cat = torch.cat((local_outputs, global_outputs), dim=-1)\n",
    "            out_cat = self.linear_double_cdm_or_cdw(out_cat)\n",
    "\n",
    "        elif self.args.local_context_focus == 'lcf_fusion':\n",
    "            masked_local_text_vec = self.feature_dynamic_mask(text_local_indices, aspect_indices)\n",
    "            masked_local_out = torch.mul(local_outputs, masked_local_text_vec)\n",
    "            weighted_text_local_features = self.feature_dynamic_weighted(text_local_indices, aspect_indices)\n",
    "            weighted_local_out = torch.mul(local_outputs, weighted_text_local_features)\n",
    "            out_cat = torch.cat((masked_local_out, global_outputs, weighted_local_out), dim=-1)\n",
    "            out_cat = self.linear_triple_lcf_global(out_cat)\n",
    "\n",
    "        self_attention_out = self.dropout(local_outputs)\n",
    "        local_pooled_out = self.bert_pooler(self_attention_out, masked_local_text_vec)\n",
    "        self_attention_out = self.dropout(global_outputs)\n",
    "        global_pooled_out = self.bert_pooler_org(self_attention_out)\n",
    "        pooled_out = torch.cat((local_pooled_out, global_pooled_out), dim=-1)\n",
    "        \n",
    "        mean_pool = self.linear_double_cdm_or_cdw(pooled_out)\n",
    "        logits = self.dense(mean_pool)\n",
    "        outputs = (logits,)\n",
    "\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEqO6Hng5zkE"
   },
   "source": [
    "##### creating run_ichi.py main driving code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZWhlcZgHS8a9",
    "outputId": "91cc70bc-5741-4de3-c8a4-3b42775bb519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./examples/ichi/run_ichi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./examples/ichi/run_ichi.py\n",
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AlbertConfig,\n",
    "    AlbertForSequenceClassification,\n",
    "    AlbertTokenizer,\n",
    "    BertConfig,\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaTokenizer,\n",
    "    XLMConfig,\n",
    "    XLMForSequenceClassification,\n",
    "    XLMRobertaConfig,\n",
    "    XLMRobertaForSequenceClassification,\n",
    "    XLMRobertaTokenizer,\n",
    "    XLMTokenizer,\n",
    "    XLNetConfig,\n",
    "    XLNetForSequenceClassification,\n",
    "    XLNetTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from lcf_ichi import lcf_BERT#, lcf_XLNET, lcf_XLM, lcf_Roberta, lcf_DistilBert, lcf_Albert, lcf_XLMRoberta\n",
    "from utils_ichi import convert_examples_to_features, ICHIProcessor, compute_metrics, load_and_cache_examples, Tokenizer, pad_and_truncate, build_embedding_matrix_glove, build_embedding_matrix_BioASQ\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ALL_MODELS = sum(\n",
    "    (\n",
    "        tuple(conf.pretrained_config_archive_map.keys())\n",
    "        for conf in (\n",
    "            BertConfig,\n",
    "            XLNetConfig,\n",
    "            XLMConfig,\n",
    "            RobertaConfig,\n",
    "            DistilBertConfig,\n",
    "            AlbertConfig,\n",
    "            XLMRobertaConfig,\n",
    "        )\n",
    "    ),\n",
    "    (),\n",
    ")\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"bert\": (BertConfig, BertForSequenceClassification, BertTokenizer, lcf_BERT),\n",
    "}\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def train(args, train_dataset, model, tokenizer, tokenizer_cleantext):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
    "        os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True,\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if os.path.exists(args.model_name_or_path):\n",
    "        # set global_step to gobal_step of last saved checkpoint from model path\n",
    "        global_step = int(args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n",
    "        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "        logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "        logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "        logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "        logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0],\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproductibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            inputs = {\"input_global_ids\": batch[0], \"attention_mask_global\": batch[2], \"input_local_ids\": batch[1], \"attention_mask_local\": batch[3], \"text_clean_indices\": batch[6], \"aspect_indices\": batch[7], \"labels\": batch[8]}\n",
    "            if args.model_type != \"distilbert\":\n",
    "                inputs[\"token_global_type_ids\"] = (\n",
    "                    batch[4] if args.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n",
    "                )\n",
    "                inputs[\"token_local_type_ids\"] = (\n",
    "                    batch[5] if args.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n",
    "                )\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    logs = {}\n",
    "                    if (\n",
    "                        args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer, tokenizer_cleantext)\n",
    "                        for key, value in results.items():\n",
    "                            eval_key = \"eval_{}\".format(key)\n",
    "                            logs[eval_key] = value\n",
    "\n",
    "                    loss_scalar = (tr_loss - logging_loss) / args.logging_steps\n",
    "                    learning_rate_scalar = scheduler.get_lr()[0]\n",
    "                    logs[\"learning_rate\"] = learning_rate_scalar\n",
    "                    logs[\"loss\"] = loss_scalar\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                    for key, value in logs.items():\n",
    "                        tb_writer.add_scalar(key, value, global_step)\n",
    "                    print(json.dumps({**logs, **{\"step\": global_step}}))\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "\n",
    "def evaluate(args, model, tokenizer, tokenizer_cleantext, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_task_names = (\"ichi\",)\n",
    "    eval_outputs_dirs = (args.output_dir,)\n",
    "\n",
    "    results = {}\n",
    "    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n",
    "        eval_dataset, tokenizer_cleantext = load_and_cache_examples(args, eval_task, tokenizer, tokenizer_cleantext, evaluate=True)\n",
    "\n",
    "        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
    "            os.makedirs(eval_output_dir)\n",
    "\n",
    "        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "        # Note that DistributedSampler samples randomly\n",
    "        eval_sampler = SequentialSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "        # multi-gpu eval\n",
    "        if args.n_gpu > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "        logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        preds = None\n",
    "        preds_original = None\n",
    "        out_label_ids = None\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            model.eval()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = {\"input_global_ids\": batch[0], \"attention_mask_global\": batch[2], \"input_local_ids\": batch[1], \"attention_mask_local\": batch[3], \"text_clean_indices\": batch[6], \"aspect_indices\": batch[7], \"labels\": batch[8]}\n",
    "                if args.model_type != \"distilbert\":\n",
    "                    inputs[\"token_global_type_ids\"] = (\n",
    "                        batch[4] if args.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n",
    "                    )\n",
    "                    inputs[\"token_local_type_ids\"] = (\n",
    "                        batch[5] if args.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n",
    "                    )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n",
    "                outputs = model(**inputs)\n",
    "                tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "                eval_loss += tmp_eval_loss.mean().item()\n",
    "            nb_eval_steps += 1\n",
    "            if preds is None:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "                preds_original = logits.detach().cpu().numpy()\n",
    "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                preds_original = np.append(preds_original, logits.detach().cpu().numpy(), axis=0)\n",
    "                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        result = compute_metrics(preds, out_label_ids)\n",
    "        results.update(result)\n",
    "\n",
    "        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "            writer.write('[')\n",
    "            for a in preds_original:\n",
    "                writer.write('[')\n",
    "                for c in range(len(a)):\n",
    "                    b = a[c]\n",
    "                    if c!=6:\n",
    "                        writer.write(str(b)+',')\n",
    "                    else:\n",
    "                        writer.write(str(b))\n",
    "                writer.write(']')\n",
    "                writer.write('\\n')\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Required parameters\n",
    "    parser.add_argument(\n",
    "        \"--data_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_type\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "\n",
    "    # Other parameters\n",
    "    parser.add_argument(\n",
    "        \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_name\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cache_dir\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_seq_length\",\n",
    "        default=128,\n",
    "        type=int,\n",
    "        help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "        \"than this will be truncated, sequences shorter will be padded.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--embedding_dim_word2vec\",\n",
    "        default=300,\n",
    "        type=int,\n",
    "        help=\"embedding dimension for the word vectors in the medical module\",\n",
    "    )\n",
    "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
    "    parser.add_argument(\n",
    "        \"--evaluate_during_training\", action=\"store_true\", help=\"Rul evaluation during training at each logging step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\n",
    "        \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_steps\",\n",
    "        default=-1,\n",
    "        type=int,\n",
    "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "\n",
    "    parser.add_argument(\"--logging_steps\", type=int, default=50, help=\"Log every X updates steps.\")\n",
    "    parser.add_argument(\"--save_steps\", type=int, default=10000, help=\"Save checkpoint every X updates steps.\")\n",
    "    parser.add_argument(\n",
    "        \"--eval_all_checkpoints\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
    "    )\n",
    "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "    parser.add_argument(\n",
    "        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\",\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--fp16\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fp16_opt_level\",\n",
    "        type=str,\n",
    "        default=\"O1\",\n",
    "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "        \"See details at https://nvidia.github.io/apex/amp.html\",\n",
    "    )\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if (\n",
    "        os.path.exists(args.output_dir)\n",
    "        and os.listdir(args.output_dir)\n",
    "        and args.do_train\n",
    "        and not args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Setup distant debugging if needed\n",
    "    if args.server_ip and args.server_port:\n",
    "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "        import ptvsd\n",
    "\n",
    "        print(\"Waiting for debugger attach\")\n",
    "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "        ptvsd.wait_for_attach()\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        args.n_gpu = torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        torch.distributed.init_process_group(backend=\"nccl\")\n",
    "        args.n_gpu = 1\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        args.local_rank,\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "        bool(args.local_rank != -1),\n",
    "        args.fp16,\n",
    "    )\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "\n",
    "    # Prepare GLUE task\n",
    "    args.task_name = 'ichi'\n",
    "    processor = ICHIProcessor()\n",
    "    args.output_mode = \"classification\"\n",
    "    label_list = processor.get_labels()\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    if args.local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "    args.model_type = args.model_type.lower()\n",
    "    config_class, model_class, tokenizer_class, lcf_model = MODEL_CLASSES[args.model_type]\n",
    "    config = config_class.from_pretrained(\n",
    "        args.config_name if args.config_name else args.model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        finetuning_task=args.task_name,\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    )\n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "        do_lower_case=args.do_lower_case,\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    )\n",
    "    tokenizer_cleantext = Tokenizer(max_seq_len = 1600, max_num_words=20000,lower=True)\n",
    "    args.use_single_bert = False\n",
    "    args.local_context_focus = 'cdm'\n",
    "    args.embedding_type = 'glove'\n",
    "\n",
    "    cached_embeddingmatrix_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cachedword2vec_{}_{}\".format(\n",
    "            str(args.task_name),\n",
    "            str(\"glove\"),\n",
    "        ),\n",
    "    )\n",
    "    cached_embeddingmatrix_path = \".\"\n",
    "    \n",
    "    train_dataset, tokenizer_cleantext = load_and_cache_examples(args, args.task_name, tokenizer, tokenizer_cleantext, evaluate=False)\n",
    "    model = lcf_model.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "        config=config,\n",
    "        args=args,\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    )\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "    model.to(args.device)\n",
    "\n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "    \n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer, tokenizer_cleantext)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    result = evaluate(args, model, tokenizer, tokenizer_cleantext)\n",
    "    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "        # Create output directory if needed\n",
    "        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "            os.makedirs(args.output_dir)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        # model_to_save = (\n",
    "        #     model.module if hasattr(model, \"module\") else model\n",
    "        # )  # Take care of distributed/parallel training\n",
    "        # model_to_save.save_pretrained(args.output_dir)\n",
    "        model_name = \"{}.pt\".format(args.model_type)\n",
    "        torch.save(model.state_dict(), os.path.join(args.output_dir,model_name))\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        # model = model_class.from_pretrained(args.output_dir)\n",
    "        model = lcf_model(config, args)\n",
    "        model.load_state_dict(torch.load(os.path.join(args.output_dir,model_name)))\n",
    "        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "        cached_tokenizer_cleantext_file = os.path.join(\n",
    "            args.data_dir,\n",
    "            \"cachedtokenizer_{}_{}_{}\".format(\n",
    "                list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
    "                str(args.embedding_type),\n",
    "                str(args.task_name),\n",
    "            ),\n",
    "        )\n",
    "        if os.path.exists(cached_tokenizer_cleantext_file):\n",
    "            print('loading tokenizer from cache:', cached_tokenizer_cleantext_file)\n",
    "            tokenizer_cleantext = pickle.load(open(cached_tokenizer_cleantext_file, 'rb'))\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = lcf_model(config, args)\n",
    "            model_name = \"{}.pt\".format(args.model_type)\n",
    "            model.load_state_dict(torch.load(os.path.join(args.output_dir,model_name)))\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, tokenizer_cleantext, prefix=prefix)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KY1yJ15B57TD"
   },
   "source": [
    "### model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnTJ0jJDZxZG",
    "outputId": "61b053b8-26f7-45ae-950c-3b6d32fab3a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/19/2021 14:07:57 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "08/19/2021 14:07:57 - INFO - filelock -   Lock 140092790003984 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "08/19/2021 14:07:57 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpn_0ralz2\n",
      "Downloading: 100% 433/433 [00:00<00:00, 498kB/s]\n",
      "08/19/2021 14:07:57 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "08/19/2021 14:07:57 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "08/19/2021 14:07:57 - INFO - filelock -   Lock 140092790003984 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "08/19/2021 14:07:57 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "08/19/2021 14:07:57 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"ichi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 7,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "08/19/2021 14:07:57 - INFO - filelock -   Lock 140092789979024 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "08/19/2021 14:07:57 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpcvk__8bx\n",
      "Downloading: 100% 232k/232k [00:00<00:00, 944kB/s]\n",
      "08/19/2021 14:07:58 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "08/19/2021 14:07:58 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "08/19/2021 14:07:58 - INFO - filelock -   Lock 140092789979024 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "08/19/2021 14:07:58 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "08/19/2021 14:07:58 - INFO - utils_ichi -   Creating features from dataset file at ./data/ichi_dataset\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   Writing example 0/8000\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   *** Example ***\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   guid: train-1\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   input_global_ids: 101 15116 2006 2026 7223 1045 2031 1037 15116 2006 2026 7223 1010 2157 2917 1996 2157 7639 1012 5683 2066 1037 18521 9113 1010 2049 3255 2440 2069 2043 1045 2693 2026 7223 1037 3056 2126 1012 2151 2028 2031 2151 4784 2055 2054 1045 2064 2079 2000 8081 2023 1012 2009 3310 1998 2009 3632 1012 102 2157 7639 102 2157 102 7223 102 3255 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   input_local_ids: 101 15116 2006 2026 7223 1045 2031 1037 15116 2006 2026 7223 1010 2157 2917 1996 2157 7639 1012 5683 2066 1037 18521 9113 1010 2049 3255 2440 2069 2043 1045 2693 2026 7223 1037 3056 2126 1012 2151 2028 2031 2151 4784 2055 2054 1045 2064 2079 2000 8081 2023 1012 2009 3310 1998 2009 3632 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 551 2776 1 551 13437 43 43 11255 21 4 5313 8580 18 407 406 2776 747 1002 11 208 1 1039 292 31 70 3521\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   aspect_indices: 2157 7639 2157 7223 3255 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   label: SOCL (id = 6)\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   *** Example ***\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   guid: train-2\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   input_global_ids: 101 2260 1059 5705 18720 2007 8178 1998 2383 5976 1006 2512 1011 9145 1007 14855 5910 1999 2026 13878 1011 2064 2009 2022 1012 1012 1012 1045 2572 2260 2860 2487 2094 18720 2007 8178 1998 2005 2055 1996 2627 2733 1010 1045 1005 2310 2018 2000 5976 5519 8449 1999 2026 3356 13878 1012 2009 1005 1055 2062 2105 2026 7579 6462 1998 3553 2000 2026 10335 2061 1045 1005 1049 3492 3056 2009 1005 1055 2025 1996 10834 2138 1045 2123 1005 1056 2228 2026 21183 21608 2052 2022 2039 2008 2152 2664 1012 4606 2009 2052 2022 2205 2220 2157 2085 1012 1045 2196 2018 2122 2007 2026 16233 2040 2003 2471 1018 1012 2027 2123 1005 1056 3480 1010 2021 2027 2272 2041 1997 7880 1998 2514 2074 2066 3336 14590 1012 1045 1005 1049 2469 2009 1005 1055 2074 6740 12403 19230 1010 2021 1045 2001 2074 8025 2000 2113 2065 3087 2842 2038 2018 2505 2066 2023 1012 2028 2001 3492 2844 2157 1999 2026 7579 6462 1012 2027 2069 2197 1037 2117 1998 2059 2175 2185 1012 4283 2005 2115 10960 1012 102 3356 13878 102 10335 102 2041 102 12403 19230 102 21183 21608 102 2157 102 7579 6462 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   input_local_ids: 101 2260 1059 5705 18720 2007 8178 1998 2383 5976 1006 2512 1011 9145 1007 14855 5910 1999 2026 13878 1011 2064 2009 2022 1012 1012 1012 1045 2572 2260 2860 2487 2094 18720 2007 8178 1998 2005 2055 1996 2627 2733 1010 1045 1005 2310 2018 2000 5976 5519 8449 1999 2026 3356 13878 1012 2009 1005 1055 2062 2105 2026 7579 6462 1998 3553 2000 2026 10335 2061 1045 1005 1049 3492 3056 2009 1005 1055 2025 1996 10834 2138 1045 2123 1005 1056 2228 2026 21183 21608 2052 2022 2039 2008 2152 2664 1012 4606 2009 2052 2022 2205 2220 2157 2085 1012 1045 2196 2018 2122 2007 2026 16233 2040 2003 2471 1018 1012 2027 2123 1005 1056 3480 1010 2021 2027 2272 2041 1997 7880 1998 2514 2074 2066 3336 14590 1012 1045 1005 1049 2469 2009 1005 1055 2074 6740 12403 19230 1010 2021 1045 2001 2074 8025 2000 2113 2065 3087 2842 2038 2018 2505 2066 2023 1012 2028 2001 3492 2844 2157 1999 2026 7579 6462 1012 2027 2069 2197 1037 2117 1998 2059 2175 2185 1012 4283 2005 2115 10960 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 418 1831 3522 1546 1230 20002 11256 1016 51 193 91 1 20002 3522 1546 128 748 1 1230 11257 491 2867 31 84 1074 3106 2627 1547 1 258 747 111 1 33 749 7 197 978 919 7 270 43 262 1 54 966 109 3292 145 2456 70 3107 21 4 111 20002 1 113 440 11258 1 1081 5 50 158 86 4 292 11 258 760 43 1074 7723 145 27 241 16 20002 7724\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   aspect_indices: 3356 13878 10335 2041 12403 19230 21183 21608 2157 7579 6462 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   label: PREG (id = 4)\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   *** Example ***\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   guid: train-3\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   input_global_ids: 101 5729 3255 1999 2187 3239 3531 1045 2342 2393 2613 4248 1045 2031 2589 2019 27011 1998 14931 13594 1011 2053 8254 2271 2483 1010 2053 10722 20360 2015 1010 2498 1012 1045 2031 2904 2026 20422 1018 2335 1999 1996 2197 1022 2706 1010 2053 4335 1012 2664 1045 2031 2023 5729 3255 1999 2026 2187 3239 1012 2053 14978 2015 1011 2074 3255 1999 1996 3239 2004 2295 1996 3239 6650 2024 5410 1998 3810 1996 3239 2105 5320 7367 6299 3255 1012 2174 1010 2130 2043 1045 3844 2026 2159 2096 8300 1010 2061 2146 2004 1045 2693 1996 3239 2055 5819 3844 1010 2043 1045 2330 2026 3239 1045 2514 6387 3255 1012 2044 1037 2096 1010 2009 2089 2175 2021 2059 5651 1012 2009 2788 2358 8609 2015 2044 1037 2146 2558 1997 2717 1010 1998 2089 4942 7363 2044 1037 2146 2558 1997 2108 5697 1010 2021 2823 4152 4788 1996 3902 3771 1045 2131 1012 1996 3255 15804 2869 1045 2202 2123 2102 2393 1998 2031 16222 2819 7068 2094 1999 2026 2291 2000 3426 17964 1998 6911 1999 2026 4167 1012 2054 2064 1045 2079 3531 1012 102 3239 6650 102 7367 6299 102 13594 102 5410 102 2159 102 3255 1999 3239 102 3255 102 17964 102 4167 102 2187 3239 102 3239 102 14978 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   input_local_ids: 101 5729 3255 1999 2187 3239 3531 1045 2342 2393 2613 4248 1045 2031 2589 2019 27011 1998 14931 13594 1011 2053 8254 2271 2483 1010 2053 10722 20360 2015 1010 2498 1012 1045 2031 2904 2026 20422 1018 2335 1999 1996 2197 1022 2706 1010 2053 4335 1012 2664 1045 2031 2023 5729 3255 1999 2026 2187 3239 1012 2053 14978 2015 1011 2074 3255 1999 1996 3239 2004 2295 1996 3239 6650 2024 5410 1998 3810 1996 3239 2105 5320 7367 6299 3255 1012 2174 1010 2130 2043 1045 3844 2026 2159 2096 8300 1010 2061 2146 2004 1045 2693 1996 3239 2055 5819 3844 1010 2043 1045 2330 2026 3239 1045 2514 6387 3255 1012 2044 1037 2096 1010 2009 2089 2175 2021 2059 5651 1012 2009 2788 2358 8609 2015 2044 1037 2146 2558 1997 2717 1010 1998 2089 4942 7363 2044 1037 2146 2558 1997 2108 5697 1010 2021 2823 4152 4788 1996 3902 3771 1045 2131 1012 1996 3255 15804 2869 1045 2202 2123 2102 2393 1998 2031 16222 2819 7068 2094 1999 2026 2291 2000 3426 17964 1998 6911 1999 2026 4167 1012 2054 2064 1045 2079 3531 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 285 18 64 32 61 1 46 19 523 1277 1 110 494 812 545 51 20002 20002 1202 1 702 889 71 9 27 195 909 3293 366 1 285 18 64 538 321 444 51 415 18 32 234 32 440 1496 1432 32 84 125 2868 497 334 39 1 2136 32 13438 98 406 32 3645 11259 1 552 32 1 21 1548 497 232 2278 90 16 9742 31 235 20002 98 38 6081 90 4769 98 38 13439 140 3 328 20002 1 6532 12 18 11260 34 80 19 20002 910 125 1246 592 4548 73 1 1803\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   aspect_indices: 3239 6650 7367 6299 13594 5410 2159 3255 1999 3239 3255 17964 4167 2187 3239 3239 14978 2015 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   label: GOAL (id = 3)\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   *** Example ***\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   guid: train-4\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   input_global_ids: 101 2158 5344 5571 2005 3752 2564 1005 1055 1041 1011 5653 1000 8299 1024 1013 1013 7479 1012 5796 28957 1012 5796 2078 1012 4012 1013 8909 1013 2871 2620 11387 2620 2683 2475 1013 24978 1013 2974 1035 1998 1035 2671 1011 3036 1013 2260 1013 2676 1013 2184 9825 12733 2109 2010 2564 1005 1055 20786 2000 3229 2014 20917 4014 1999 8758 3217 25322 4564 1010 23025 2232 1012 1037 29645 1574 1524 1037 4174 2158 2040 2758 2002 4342 1997 2010 2564 1005 1055 6771 2011 3752 2014 1041 1011 5653 2006 2037 3274 5344 3979 13114 1012 1021 2006 24648 3274 28616 8557 5571 1012 4228 1011 2093 1011 2095 1011 2214 6506 5232 2109 2010 2564 1005 1055 20786 2000 2131 2046 2014 20917 4014 4070 1012 10254 5232 6406 2005 1037 8179 1010 2029 2001 4379 2023 3204 1012 6506 5232 4136 1996 9182 2811 1997 24538 2002 2001 2667 2000 4047 1996 3232 1005 1055 2336 2013 19046 1998 4455 1996 2553 1037 1000 1000 28616 21539 1997 3425 1012 1000 1000 9182 2221 3353 12478 3994 6769 2758 1996 3715 2003 15123 1012 9394 2375 3213 5406 4644 4136 1996 5626 2489 2811 1996 2375 4050 2003 2109 2000 12388 26869 4767 11933 1998 11065 3119 7800 1012 2002 2758 2002 3980 2065 1037 2564 2064 5987 9394 2006 1037 3274 2016 6661 2007 2014 3129 1012 9385 2230 1996 3378 2811 102 1000 2916 102 3979 102 5344 102 1000 1000 28616 21539 1000 102\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   input_local_ids: 101 2158 5344 5571 2005 3752 2564 1005 1055 1041 1011 5653 1000 8299 1024 1013 1013 7479 1012 5796 28957 1012 5796 2078 1012 4012 1013 8909 1013 2871 2620 11387 2620 2683 2475 1013 24978 1013 2974 1035 1998 1035 2671 1011 3036 1013 2260 1013 2676 1013 2184 9825 12733 2109 2010 2564 1005 1055 20786 2000 3229 2014 20917 4014 1999 8758 3217 25322 4564 1010 23025 2232 1012 1037 29645 1574 1524 1037 4174 2158 2040 2758 2002 4342 1997 2010 2564 1005 1055 6771 2011 3752 2014 1041 1011 5653 2006 2037 3274 5344 3979 13114 1012 1021 2006 24648 3274 28616 8557 5571 1012 4228 1011 2093 1011 2095 1011 2214 6506 5232 2109 2010 2564 1005 1055 20786 2000 2131 2046 2014 20917 4014 4070 1012 10254 5232 6406 2005 1037 8179 1010 2029 2001 4379 2023 3204 1012 6506 5232 4136 1996 9182 2811 1997 24538 2002 2001 2667 2000 4047 1996 3232 1005 1055 2336 2013 19046 1998 4455 1996 2553 1037 1000 1000 28616 21539 1997 3425 1012 1000 1000 9182 2221 3353 12478 3994 6769 2758 1996 3715 2003 15123 1012 9394 2375 3213 5406 4644 4136 1996 5626 2489 2811 1996 2375 4050 2003 2109 2000 12388 26869 4767 11933 1998 11065 3119 7800 1012 2002 2758 2002 3980 2065 1037 2564 2064 5987 9394 2006 1037 3274 2016 6661 2007 2014 3129 1012 9385 2230 1996 3378 2811 1012 2035 2916 9235 1012 2023 3430 2089 2025 2022 2405 1010 3743 102\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 519 376 1549 430 428 4135 20002 155 428 8581 2517 17277 20002 20002 17278 157 26 707 148 3810 519 48 1597 428 3973 430 4135 1031 376 2177 5011 180 17279 1031 17280 20002 20002 4770 155 428 8581 3 17277 13440 20002 4770 3023 6533 8582 20002 4770 74 12 7725 1497 20002 124 2566 200 67 8583 267 476 608 13441 7725 3202 3294 5314 13442 13443 48 1549 20002 636 4325 8584 7726 74 7042 615 1497 636 3386 155 20002 4136 5012 3295 3108 17281 10 48 108 428 951 5315 1031 844 20002 1052 12 1373 9743 421 43 8585 82 2567 90 11261 11262 11263 20002\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   aspect_indices: 1000 2916 3979 5344 1000 1000 28616 21539 1000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   label: SOCL (id = 6)\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   *** Example ***\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   guid: train-5\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   input_global_ids: 101 2054 1005 1055 3154 15333 7174 4931 3071 1010 1024 1007 1045 1005 1049 2205 5697 2000 3524 2105 2005 2026 7435 2436 2000 2655 2033 2044 1045 2681 2026 4471 1045 2342 2000 2831 2007 2068 1012 1045 2031 2000 2022 2006 1037 6381 8738 1996 2154 2077 5970 1012 2054 3599 2515 3154 15333 7174 2812 1029 1029 1029 1045 1005 2310 2657 2477 2055 6595 2185 2013 18554 1005 1055 1998 3056 3924 1012 1045 1005 2310 2042 2409 2045 2024 18554 2489 15333 7174 1005 1055 2021 2027 2031 1996 26389 1010 2021 4033 1005 1056 2179 2151 2664 1012 2036 1010 1045 2001 2409 1045 2064 4392 13675 2319 9766 10869 1998 14722 10869 1010 2021 2664 2001 2409 2000 2994 2185 2013 2417 1998 6379 15333 7174 1998 2035 1012 3246 2070 1997 2017 2064 2393 1012 1045 1005 1049 2061 5697 2007 5970 2746 2039 2279 5958 1012 14743 2039 3467 4933 1998 4895 23947 2075 1996 2621 1998 2893 2068 2035 8871 1998 3201 1012 9344 1996 2878 2160 2613 2204 1010 7079 1996 8236 1010 4518 2075 2039 2006 26298 1010 2893 4253 1045 1005 2222 2022 2583 2000 4929 2044 5970 1012 4385 1012 1012 1012 1012 3246 2017 2035 2024 2725 2092 1012 2667 2000 2562 2039 2007 3071 2205 1012 1045 2428 9120 2035 1997 2017 1012 4067 2017 2005 2673 1012 1012 1012 1012 1012 1012 1012 1012 102 18554 102 18554 102 6381 8738 102 4895 23947 2075 102 2035 102\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   input_local_ids: 101 2054 1005 1055 3154 15333 7174 4931 3071 1010 1024 1007 1045 1005 1049 2205 5697 2000 3524 2105 2005 2026 7435 2436 2000 2655 2033 2044 1045 2681 2026 4471 1045 2342 2000 2831 2007 2068 1012 1045 2031 2000 2022 2006 1037 6381 8738 1996 2154 2077 5970 1012 2054 3599 2515 3154 15333 7174 2812 1029 1029 1029 1045 1005 2310 2657 2477 2055 6595 2185 2013 18554 1005 1055 1998 3056 3924 1012 1045 1005 2310 2042 2409 2045 2024 18554 2489 15333 7174 1005 1055 2021 2027 2031 1996 26389 1010 2021 4033 1005 1056 2179 2151 2664 1012 2036 1010 1045 2001 2409 1045 2064 4392 13675 2319 9766 10869 1998 14722 10869 1010 2021 2664 2001 2409 2000 2994 2185 2013 2417 1998 6379 15333 7174 1998 2035 1012 3246 2070 1997 2017 2064 2393 1012 1045 1005 1049 2061 5697 2007 5970 2746 2039 2279 5958 1012 14743 2039 3467 4933 1998 4895 23947 2075 1996 2621 1998 2893 2068 2035 8871 1998 3201 1012 9344 1996 2878 2160 2613 2204 1010 7079 1996 8236 1010 4518 2075 2039 2006 26298 1010 2893 4253 1045 1005 2222 2022 2583 2000 4929 2044 5970 1012 4385 1012 1012 1012 1012 3246 2017 2035 2024 2725 2092 1012 2667 2000 2562 2039 2007 3071 2205 1012 1045 2428 9120 2035 1997 2017 1012 4067 2017 2005 2673 1012 1012 1012 1012 1012 1012 1012 1012 1012 1012 1012 1012 1012 1012 1024 1007 102 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 73 471 8586 1144 2091 9744 2232 359 84 29 787 267 1 540 1875 1 46 206 458 1 1625 463 6 721 73 754 471 8586 2137 1 309 47 1832 152 4549 747 3811 1 35 4549 615 8586 20002 129 978 710 1 35 1 532 6534 2000 6082 9745 366 35 297 152 227 4550 8586 17282 274 1 2232 132 316 121 3109 17283 3024 716 20002 1679 65 3296 7043 1967 284 289 523 1708 2457 9746 17284 20002 65 1126 1 183 533 721 20002 353 421 711 5013 498 124 138 387 795 1 25 735 421 522 555 256 353 246 1341 1065\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   aspect_indices: 18554 18554 6381 8738 4895 23947 2075 2035 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:09:49 - INFO - utils_ichi -   label: TRMT (id = 2)\n",
      "08/19/2021 14:10:40 - INFO - utils_ichi -   Saving features into cached file ./data/ichi_dataset/cached_train_bert-base-uncased_256_ichi\n",
      "08/19/2021 14:10:47 - INFO - filelock -   Lock 140092790004112 acquired on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "08/19/2021 14:10:47 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpuanb0eu4\n",
      "Downloading: 100% 440M/440M [00:11<00:00, 38.8MB/s]\n",
      "08/19/2021 14:10:59 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "08/19/2021 14:10:59 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "08/19/2021 14:10:59 - INFO - filelock -   Lock 140092790004112 released on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "08/19/2021 14:10:59 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "08/19/2021 14:11:01 - INFO - transformers.modeling_utils -   Weights of lcf_BERT not initialized from pretrained model: ['bert_SA.SA.query.weight', 'bert_SA.SA.query.bias', 'bert_SA.SA.key.weight', 'bert_SA.SA.key.bias', 'bert_SA.SA.value.weight', 'bert_SA.SA.value.bias', 'linear_double_cdm_or_cdw.weight', 'linear_double_cdm_or_cdw.bias', 'linear_triple_lcf_global.weight', 'linear_triple_lcf_global.bias', 'bert_pooler_org.dense.weight', 'bert_pooler_org.dense.bias', 'bert_pooler.dense.weight', 'bert_pooler.dense.bias', 'dense.weight', 'dense.bias']\n",
      "08/19/2021 14:11:01 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in lcf_BERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "08/19/2021 14:11:13 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='./data/ichi_dataset', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, embedding_dim_word2vec=300, embedding_type='glove', eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=0.0001, local_context_focus='cdm', local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=256, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=2.0, output_dir='./tmp/ichi_bert_base_new', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=16, per_gpu_train_batch_size=16, save_steps=10000, seed=42, server_ip='', server_port='', task_name='ichi', tokenizer_name='', use_single_bert=False, warmup_steps=0, weight_decay=0.0)\n",
      "08/19/2021 14:11:13 - INFO - __main__ -   ***** Running training *****\n",
      "08/19/2021 14:11:13 - INFO - __main__ -     Num examples = 8000\n",
      "08/19/2021 14:11:13 - INFO - __main__ -     Num Epochs = 2\n",
      "08/19/2021 14:11:13 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
      "08/19/2021 14:11:13 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "08/19/2021 14:11:13 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "08/19/2021 14:11:13 - INFO - __main__ -     Total optimization steps = 1000\n",
      "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
      "Iteration:   0% 0/500 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "\n",
      "Iteration:   0% 1/500 [00:01<14:12,  1.71s/it]\u001b[A\n",
      "Iteration:   0% 2/500 [00:03<12:55,  1.56s/it]\u001b[A\n",
      "Iteration:   1% 3/500 [00:04<12:28,  1.51s/it]\u001b[A\n",
      "Iteration:   1% 4/500 [00:06<12:14,  1.48s/it]\u001b[A\n",
      "Iteration:   1% 5/500 [00:07<12:10,  1.48s/it]\u001b[A\n",
      "Iteration:   1% 6/500 [00:08<12:07,  1.47s/it]\u001b[A\n",
      "Iteration:   1% 7/500 [00:10<12:03,  1.47s/it]\u001b[A\n",
      "Iteration:   2% 8/500 [00:11<12:02,  1.47s/it]\u001b[A\n",
      "Iteration:   2% 9/500 [00:13<11:58,  1.46s/it]\u001b[A\n",
      "Iteration:   2% 10/500 [00:14<12:00,  1.47s/it]\u001b[A\n",
      "Iteration:   2% 11/500 [00:16<11:56,  1.47s/it]\u001b[A\n",
      "Iteration:   2% 12/500 [00:17<11:56,  1.47s/it]\u001b[A\n",
      "Iteration:   3% 13/500 [00:19<11:55,  1.47s/it]\u001b[A\n",
      "Iteration:   3% 14/500 [00:20<11:57,  1.48s/it]\u001b[A\n",
      "Iteration:   3% 15/500 [00:22<11:58,  1.48s/it]\u001b[A\n",
      "Iteration:   3% 16/500 [00:23<11:58,  1.48s/it]\u001b[A\n",
      "Iteration:   3% 17/500 [00:25<11:53,  1.48s/it]\u001b[A\n",
      "Iteration:   4% 18/500 [00:26<11:54,  1.48s/it]\u001b[A\n",
      "Iteration:   4% 19/500 [00:28<11:53,  1.48s/it]\u001b[A\n",
      "Iteration:   4% 20/500 [00:29<11:55,  1.49s/it]\u001b[A\n",
      "Iteration:   4% 21/500 [00:31<11:56,  1.50s/it]\u001b[A\n",
      "Iteration:   4% 22/500 [00:32<11:53,  1.49s/it]\u001b[A\n",
      "Iteration:   5% 23/500 [00:34<11:56,  1.50s/it]\u001b[A\n",
      "Iteration:   5% 24/500 [00:35<11:56,  1.51s/it]\u001b[A\n",
      "Iteration:   5% 25/500 [00:37<11:57,  1.51s/it]\u001b[A\n",
      "Iteration:   5% 26/500 [00:38<11:58,  1.52s/it]\u001b[A\n",
      "Iteration:   5% 27/500 [00:40<11:56,  1.51s/it]\u001b[A\n",
      "Iteration:   6% 28/500 [00:41<11:59,  1.52s/it]\u001b[A\n",
      "Iteration:   6% 29/500 [00:43<12:01,  1.53s/it]\u001b[A\n",
      "Iteration:   6% 30/500 [00:44<12:03,  1.54s/it]\u001b[A\n",
      "Iteration:   6% 31/500 [00:46<11:58,  1.53s/it]\u001b[A\n",
      "Iteration:   6% 32/500 [00:48<12:02,  1.54s/it]\u001b[A\n",
      "Iteration:   7% 33/500 [00:49<12:02,  1.55s/it]\u001b[A\n",
      "Iteration:   7% 34/500 [00:51<12:01,  1.55s/it]\u001b[A\n",
      "Iteration:   7% 35/500 [00:52<12:01,  1.55s/it]\u001b[A\n",
      "Iteration:   7% 36/500 [00:54<12:02,  1.56s/it]\u001b[A\n",
      "Iteration:   7% 37/500 [00:55<11:58,  1.55s/it]\u001b[A\n",
      "Iteration:   8% 38/500 [00:57<11:58,  1.56s/it]\u001b[A\n",
      "Iteration:   8% 39/500 [00:58<11:56,  1.55s/it]\u001b[A\n",
      "Iteration:   8% 40/500 [01:00<11:57,  1.56s/it]\u001b[A\n",
      "Iteration:   8% 41/500 [01:02<11:56,  1.56s/it]\u001b[A\n",
      "Iteration:   8% 42/500 [01:03<12:06,  1.59s/it]\u001b[A\n",
      "Iteration:   9% 43/500 [01:05<12:04,  1.58s/it]\u001b[A\n",
      "Iteration:   9% 44/500 [01:06<12:01,  1.58s/it]\u001b[A\n",
      "Iteration:   9% 45/500 [01:08<12:03,  1.59s/it]\u001b[A\n",
      "Iteration:   9% 46/500 [01:10<12:07,  1.60s/it]\u001b[A\n",
      "Iteration:   9% 47/500 [01:11<12:04,  1.60s/it]\u001b[A\n",
      "Iteration:  10% 48/500 [01:13<12:03,  1.60s/it]\u001b[A\n",
      "Iteration:  10% 49/500 [01:14<12:05,  1.61s/it]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"learning_rate\": 9.5e-05, \"loss\": 1.5865396165847778, \"step\": 50}\n",
      "\n",
      "Iteration:  10% 50/500 [01:16<12:07,  1.62s/it]\u001b[A\n",
      "Iteration:  10% 51/500 [01:18<12:04,  1.61s/it]\u001b[A\n",
      "Iteration:  10% 52/500 [01:19<12:05,  1.62s/it]\u001b[A\n",
      "Iteration:  11% 53/500 [01:21<12:06,  1.63s/it]\u001b[A\n",
      "Iteration:  11% 54/500 [01:23<12:06,  1.63s/it]\u001b[A\n",
      "Iteration:  11% 55/500 [01:24<12:05,  1.63s/it]\u001b[A\n",
      "Iteration:  11% 56/500 [01:26<12:17,  1.66s/it]\u001b[A\n",
      "Iteration:  11% 57/500 [01:28<12:14,  1.66s/it]\u001b[A\n",
      "Iteration:  12% 58/500 [01:29<12:14,  1.66s/it]\u001b[A\n",
      "Iteration:  12% 59/500 [01:31<12:16,  1.67s/it]\u001b[A\n",
      "Iteration:  12% 60/500 [01:33<12:17,  1.68s/it]\u001b[A\n",
      "Iteration:  12% 61/500 [01:34<12:19,  1.69s/it]\u001b[A\n",
      "Iteration:  12% 62/500 [01:36<12:21,  1.69s/it]\u001b[A\n",
      "Iteration:  13% 63/500 [01:38<12:22,  1.70s/it]\u001b[A\n",
      "Iteration:  13% 64/500 [01:39<12:25,  1.71s/it]\u001b[A\n",
      "Iteration:  13% 65/500 [01:41<12:25,  1.71s/it]\u001b[A\n",
      "Iteration:  13% 66/500 [01:43<12:23,  1.71s/it]\u001b[A\n",
      "Iteration:  13% 67/500 [01:45<12:26,  1.72s/it]\u001b[A\n",
      "Iteration:  14% 68/500 [01:46<12:30,  1.74s/it]\u001b[A\n",
      "Iteration:  14% 69/500 [01:48<12:28,  1.74s/it]\u001b[A\n",
      "Iteration:  14% 70/500 [01:50<12:29,  1.74s/it]\u001b[A\n",
      "Iteration:  14% 71/500 [01:52<12:36,  1.76s/it]\u001b[A\n",
      "Iteration:  14% 72/500 [01:54<12:40,  1.78s/it]\u001b[A\n",
      "Iteration:  15% 73/500 [01:55<12:42,  1.78s/it]\u001b[A\n",
      "Iteration:  15% 74/500 [01:57<12:44,  1.79s/it]\u001b[A\n",
      "Iteration:  15% 75/500 [01:59<12:43,  1.80s/it]\u001b[A\n",
      "Iteration:  15% 76/500 [02:01<12:45,  1.80s/it]\u001b[A\n",
      "Iteration:  15% 77/500 [02:03<12:47,  1.81s/it]\u001b[A\n",
      "Iteration:  16% 78/500 [02:04<12:46,  1.82s/it]\u001b[A\n",
      "Iteration:  16% 79/500 [02:06<12:48,  1.82s/it]\u001b[A\n",
      "Iteration:  16% 80/500 [02:08<12:40,  1.81s/it]\u001b[A\n",
      "Iteration:  16% 81/500 [02:10<12:33,  1.80s/it]\u001b[A\n",
      "Iteration:  16% 82/500 [02:12<12:30,  1.80s/it]\u001b[A\n",
      "Iteration:  17% 83/500 [02:13<12:29,  1.80s/it]\u001b[A\n",
      "Iteration:  17% 84/500 [02:15<12:25,  1.79s/it]\u001b[A\n",
      "Iteration:  17% 85/500 [02:17<12:21,  1.79s/it]\u001b[A\n",
      "Iteration:  17% 86/500 [02:19<12:14,  1.77s/it]\u001b[A\n",
      "Iteration:  17% 87/500 [02:20<12:07,  1.76s/it]\u001b[A\n",
      "Iteration:  18% 88/500 [02:22<12:02,  1.75s/it]\u001b[A\n",
      "Iteration:  18% 89/500 [02:24<12:00,  1.75s/it]\u001b[A\n",
      "Iteration:  18% 90/500 [02:26<11:56,  1.75s/it]\u001b[A\n",
      "Iteration:  18% 91/500 [02:27<11:55,  1.75s/it]\u001b[A\n",
      "Iteration:  18% 92/500 [02:29<11:55,  1.75s/it]\u001b[A\n",
      "Iteration:  19% 93/500 [02:31<11:47,  1.74s/it]\u001b[A\n",
      "Iteration:  19% 94/500 [02:33<11:38,  1.72s/it]\u001b[A\n",
      "Iteration:  19% 95/500 [02:34<11:38,  1.72s/it]\u001b[A\n",
      "Iteration:  19% 96/500 [02:36<11:39,  1.73s/it]\u001b[A\n",
      "Iteration:  19% 97/500 [02:38<11:34,  1.72s/it]\u001b[A\n",
      "Iteration:  20% 98/500 [02:39<11:32,  1.72s/it]\u001b[A\n",
      "Iteration:  20% 99/500 [02:41<11:31,  1.73s/it]\u001b[A{\"learning_rate\": 9e-05, \"loss\": 1.1918856084346772, \"step\": 100}\n",
      "\n",
      "Iteration:  20% 100/500 [02:43<11:34,  1.74s/it]\u001b[A\n",
      "Iteration:  20% 101/500 [02:45<11:33,  1.74s/it]\u001b[A\n",
      "Iteration:  20% 102/500 [02:46<11:24,  1.72s/it]\u001b[A\n",
      "Iteration:  21% 103/500 [02:48<11:29,  1.74s/it]\u001b[A\n",
      "Iteration:  21% 104/500 [02:50<11:23,  1.73s/it]\u001b[A\n",
      "Iteration:  21% 105/500 [02:52<11:18,  1.72s/it]\u001b[A\n",
      "Iteration:  21% 106/500 [02:53<11:13,  1.71s/it]\u001b[A\n",
      "Iteration:  21% 107/500 [02:55<11:13,  1.71s/it]\u001b[A\n",
      "Iteration:  22% 108/500 [02:57<11:11,  1.71s/it]\u001b[A\n",
      "Iteration:  22% 109/500 [02:58<11:14,  1.72s/it]\u001b[A\n",
      "Iteration:  22% 110/500 [03:00<11:20,  1.74s/it]\u001b[A\n",
      "Iteration:  22% 111/500 [03:02<11:12,  1.73s/it]\u001b[A\n",
      "Iteration:  22% 112/500 [03:04<11:09,  1.73s/it]\u001b[A\n",
      "Iteration:  23% 113/500 [03:05<11:14,  1.74s/it]\u001b[A\n",
      "Iteration:  23% 114/500 [03:07<11:10,  1.74s/it]\u001b[A\n",
      "Iteration:  23% 115/500 [03:09<11:08,  1.74s/it]\u001b[A\n",
      "Iteration:  23% 116/500 [03:11<11:06,  1.74s/it]\u001b[A\n",
      "Iteration:  23% 117/500 [03:12<11:06,  1.74s/it]\u001b[A\n",
      "Iteration:  24% 118/500 [03:14<11:05,  1.74s/it]\u001b[A\n",
      "Iteration:  24% 119/500 [03:16<10:59,  1.73s/it]\u001b[A\n",
      "Iteration:  24% 120/500 [03:18<11:01,  1.74s/it]\u001b[A\n",
      "Iteration:  24% 121/500 [03:19<11:01,  1.75s/it]\u001b[A\n",
      "Iteration:  24% 122/500 [03:21<11:05,  1.76s/it]\u001b[A\n",
      "Iteration:  25% 123/500 [03:23<11:06,  1.77s/it]\u001b[A\n",
      "Iteration:  25% 124/500 [03:25<11:02,  1.76s/it]\u001b[A\n",
      "Iteration:  25% 125/500 [03:26<10:58,  1.76s/it]\u001b[A\n",
      "Iteration:  25% 126/500 [03:28<10:58,  1.76s/it]\u001b[A\n",
      "Iteration:  25% 127/500 [03:30<10:59,  1.77s/it]\u001b[A\n",
      "Iteration:  26% 128/500 [03:32<10:56,  1.77s/it]\u001b[A\n",
      "Iteration:  26% 129/500 [03:34<10:56,  1.77s/it]\u001b[A\n",
      "Iteration:  26% 130/500 [03:35<10:54,  1.77s/it]\u001b[A\n",
      "Iteration:  26% 131/500 [03:37<10:52,  1.77s/it]\u001b[A\n",
      "Iteration:  26% 132/500 [03:39<10:49,  1.77s/it]\u001b[A\n",
      "Iteration:  27% 133/500 [03:41<10:48,  1.77s/it]\u001b[A\n",
      "Iteration:  27% 134/500 [03:42<10:44,  1.76s/it]\u001b[A\n",
      "Iteration:  27% 135/500 [03:44<10:43,  1.76s/it]\u001b[A\n",
      "Iteration:  27% 136/500 [03:46<10:41,  1.76s/it]\u001b[A\n",
      "Iteration:  27% 137/500 [03:48<10:35,  1.75s/it]\u001b[A\n",
      "Iteration:  28% 138/500 [03:49<10:33,  1.75s/it]\u001b[A\n",
      "Iteration:  28% 139/500 [03:51<10:38,  1.77s/it]\u001b[A\n",
      "Iteration:  28% 140/500 [03:53<10:32,  1.76s/it]\u001b[A\n",
      "Iteration:  28% 141/500 [03:55<10:28,  1.75s/it]\u001b[A\n",
      "Iteration:  28% 142/500 [03:56<10:28,  1.76s/it]\u001b[A\n",
      "Iteration:  29% 143/500 [03:58<10:24,  1.75s/it]\u001b[A\n",
      "Iteration:  29% 144/500 [04:00<10:20,  1.74s/it]\u001b[A\n",
      "Iteration:  29% 145/500 [04:02<10:14,  1.73s/it]\u001b[A\n",
      "Iteration:  29% 146/500 [04:03<10:09,  1.72s/it]\u001b[A\n",
      "Iteration:  29% 147/500 [04:05<10:12,  1.73s/it]\u001b[A\n",
      "Iteration:  30% 148/500 [04:07<10:11,  1.74s/it]\u001b[A\n",
      "Iteration:  30% 149/500 [04:08<10:09,  1.74s/it]\u001b[A{\"learning_rate\": 8.5e-05, \"loss\": 1.111846069097519, \"step\": 150}\n",
      "\n",
      "Iteration:  30% 150/500 [04:10<10:08,  1.74s/it]\u001b[A\n",
      "Iteration:  30% 151/500 [04:12<10:09,  1.75s/it]\u001b[A\n",
      "Iteration:  30% 152/500 [04:14<10:02,  1.73s/it]\u001b[A\n",
      "Iteration:  31% 153/500 [04:15<09:59,  1.73s/it]\u001b[A\n",
      "Iteration:  31% 154/500 [04:17<09:58,  1.73s/it]\u001b[A\n",
      "Iteration:  31% 155/500 [04:19<09:57,  1.73s/it]\u001b[A\n",
      "Iteration:  31% 156/500 [04:21<09:57,  1.74s/it]\u001b[A\n",
      "Iteration:  31% 157/500 [04:22<09:59,  1.75s/it]\u001b[A\n",
      "Iteration:  32% 158/500 [04:24<09:55,  1.74s/it]\u001b[A\n",
      "Iteration:  32% 159/500 [04:26<09:49,  1.73s/it]\u001b[A\n",
      "Iteration:  32% 160/500 [04:28<09:48,  1.73s/it]\u001b[A\n",
      "Iteration:  32% 161/500 [04:29<09:47,  1.73s/it]\u001b[A\n",
      "Iteration:  32% 162/500 [04:31<09:52,  1.75s/it]\u001b[A\n",
      "Iteration:  33% 163/500 [04:33<09:48,  1.75s/it]\u001b[A\n",
      "Iteration:  33% 164/500 [04:35<09:46,  1.74s/it]\u001b[A\n",
      "Iteration:  33% 165/500 [04:36<09:43,  1.74s/it]\u001b[A\n",
      "Iteration:  33% 166/500 [04:38<09:42,  1.75s/it]\u001b[A\n",
      "Iteration:  33% 167/500 [04:40<09:40,  1.74s/it]\u001b[A\n",
      "Iteration:  34% 168/500 [04:42<09:39,  1.75s/it]\u001b[A\n",
      "Iteration:  34% 169/500 [04:43<09:39,  1.75s/it]\u001b[A\n",
      "Iteration:  34% 170/500 [04:45<09:34,  1.74s/it]\u001b[A\n",
      "Iteration:  34% 171/500 [04:47<09:34,  1.74s/it]\u001b[A\n",
      "Iteration:  34% 172/500 [04:49<09:34,  1.75s/it]\u001b[A\n",
      "Iteration:  35% 173/500 [04:50<09:32,  1.75s/it]\u001b[A\n",
      "Iteration:  35% 174/500 [04:52<09:26,  1.74s/it]\u001b[A\n",
      "Iteration:  35% 175/500 [04:54<09:22,  1.73s/it]\u001b[A\n",
      "Iteration:  35% 176/500 [04:55<09:24,  1.74s/it]\u001b[A\n",
      "Iteration:  35% 177/500 [04:57<09:25,  1.75s/it]\u001b[A\n",
      "Iteration:  36% 178/500 [04:59<09:21,  1.74s/it]\u001b[A\n",
      "Iteration:  36% 179/500 [05:01<09:22,  1.75s/it]\u001b[A\n",
      "Iteration:  36% 180/500 [05:02<09:19,  1.75s/it]\u001b[A\n",
      "Iteration:  36% 181/500 [05:04<09:16,  1.75s/it]\u001b[A\n",
      "Iteration:  36% 182/500 [05:06<09:20,  1.76s/it]\u001b[A\n",
      "Iteration:  37% 183/500 [05:08<09:20,  1.77s/it]\u001b[A\n",
      "Iteration:  37% 184/500 [05:10<09:19,  1.77s/it]\u001b[A\n",
      "Iteration:  37% 185/500 [05:11<09:12,  1.76s/it]\u001b[A\n",
      "Iteration:  37% 186/500 [05:13<09:10,  1.75s/it]\u001b[A\n",
      "Iteration:  37% 187/500 [05:15<09:10,  1.76s/it]\u001b[A\n",
      "Iteration:  38% 188/500 [05:17<09:05,  1.75s/it]\u001b[A\n",
      "Iteration:  38% 189/500 [05:18<09:05,  1.75s/it]\u001b[A\n",
      "Iteration:  38% 190/500 [05:20<09:02,  1.75s/it]\u001b[A\n",
      "Iteration:  38% 191/500 [05:22<08:57,  1.74s/it]\u001b[A\n",
      "Iteration:  38% 192/500 [05:24<08:56,  1.74s/it]\u001b[A\n",
      "Iteration:  39% 193/500 [05:25<08:57,  1.75s/it]\u001b[A\n",
      "Iteration:  39% 194/500 [05:27<08:58,  1.76s/it]\u001b[A\n",
      "Iteration:  39% 195/500 [05:29<08:57,  1.76s/it]\u001b[A\n",
      "Iteration:  39% 196/500 [05:31<08:49,  1.74s/it]\u001b[A\n",
      "Iteration:  39% 197/500 [05:32<08:54,  1.76s/it]\u001b[A\n",
      "Iteration:  40% 198/500 [05:34<08:50,  1.76s/it]\u001b[A\n",
      "Iteration:  40% 199/500 [05:36<08:50,  1.76s/it]\u001b[A{\"learning_rate\": 8e-05, \"loss\": 1.078092645406723, \"step\": 200}\n",
      "\n",
      "Iteration:  40% 200/500 [05:38<08:50,  1.77s/it]\u001b[A\n",
      "Iteration:  40% 201/500 [05:39<08:46,  1.76s/it]\u001b[A\n",
      "Iteration:  40% 202/500 [05:41<08:45,  1.76s/it]\u001b[A\n",
      "Iteration:  41% 203/500 [05:43<08:41,  1.76s/it]\u001b[A\n",
      "Iteration:  41% 204/500 [05:45<08:37,  1.75s/it]\u001b[A\n",
      "Iteration:  41% 205/500 [05:46<08:35,  1.75s/it]\u001b[A\n",
      "Iteration:  41% 206/500 [05:48<08:38,  1.77s/it]\u001b[A\n",
      "Iteration:  41% 207/500 [05:50<08:36,  1.76s/it]\u001b[A\n",
      "Iteration:  42% 208/500 [05:52<08:35,  1.77s/it]\u001b[A\n",
      "Iteration:  42% 209/500 [05:54<08:37,  1.78s/it]\u001b[A\n",
      "Iteration:  42% 210/500 [05:55<08:35,  1.78s/it]\u001b[A\n",
      "Iteration:  42% 211/500 [05:57<08:31,  1.77s/it]\u001b[A\n",
      "Iteration:  42% 212/500 [05:59<08:27,  1.76s/it]\u001b[A\n",
      "Iteration:  43% 213/500 [06:01<08:27,  1.77s/it]\u001b[A\n",
      "Iteration:  43% 214/500 [06:02<08:24,  1.76s/it]\u001b[A\n",
      "Iteration:  43% 215/500 [06:04<08:22,  1.76s/it]\u001b[A\n",
      "Iteration:  43% 216/500 [06:06<08:21,  1.77s/it]\u001b[A\n",
      "Iteration:  43% 217/500 [06:08<08:17,  1.76s/it]\u001b[A\n",
      "Iteration:  44% 218/500 [06:09<08:14,  1.75s/it]\u001b[A\n",
      "Iteration:  44% 219/500 [06:11<08:14,  1.76s/it]\u001b[A\n",
      "Iteration:  44% 220/500 [06:13<08:11,  1.75s/it]\u001b[A\n",
      "Iteration:  44% 221/500 [06:15<08:11,  1.76s/it]\u001b[A\n",
      "Iteration:  44% 222/500 [06:16<08:08,  1.76s/it]\u001b[A\n",
      "Iteration:  45% 223/500 [06:18<08:04,  1.75s/it]\u001b[A\n",
      "Iteration:  45% 224/500 [06:20<08:02,  1.75s/it]\u001b[A\n",
      "Iteration:  45% 225/500 [06:22<08:00,  1.75s/it]\u001b[A\n",
      "Iteration:  45% 226/500 [06:23<07:58,  1.74s/it]\u001b[A\n",
      "Iteration:  45% 227/500 [06:25<07:57,  1.75s/it]\u001b[A\n",
      "Iteration:  46% 228/500 [06:27<07:58,  1.76s/it]\u001b[A\n",
      "Iteration:  46% 229/500 [06:29<07:52,  1.74s/it]\u001b[A\n",
      "Iteration:  46% 230/500 [06:30<07:52,  1.75s/it]\u001b[A\n",
      "Iteration:  46% 231/500 [06:32<07:51,  1.75s/it]\u001b[A\n",
      "Iteration:  46% 232/500 [06:34<07:48,  1.75s/it]\u001b[A\n",
      "Iteration:  47% 233/500 [06:36<07:50,  1.76s/it]\u001b[A\n",
      "Iteration:  47% 234/500 [06:37<07:44,  1.75s/it]\u001b[A\n",
      "Iteration:  47% 235/500 [06:39<07:46,  1.76s/it]\u001b[A\n",
      "Iteration:  47% 236/500 [06:41<07:45,  1.77s/it]\u001b[A\n",
      "Iteration:  47% 237/500 [06:43<07:43,  1.76s/it]\u001b[A\n",
      "Iteration:  48% 238/500 [06:44<07:42,  1.77s/it]\u001b[A\n",
      "Iteration:  48% 239/500 [06:46<07:40,  1.77s/it]\u001b[A\n",
      "Iteration:  48% 240/500 [06:48<07:39,  1.77s/it]\u001b[A\n",
      "Iteration:  48% 241/500 [06:50<07:38,  1.77s/it]\u001b[A\n",
      "Iteration:  48% 242/500 [06:52<07:34,  1.76s/it]\u001b[A\n",
      "Iteration:  49% 243/500 [06:53<07:31,  1.75s/it]\u001b[A\n",
      "Iteration:  49% 244/500 [06:55<07:27,  1.75s/it]\u001b[A\n",
      "Iteration:  49% 245/500 [06:57<07:26,  1.75s/it]\u001b[A\n",
      "Iteration:  49% 246/500 [06:59<07:27,  1.76s/it]\u001b[A\n",
      "Iteration:  49% 247/500 [07:00<07:23,  1.75s/it]\u001b[A\n",
      "Iteration:  50% 248/500 [07:02<07:20,  1.75s/it]\u001b[A\n",
      "Iteration:  50% 249/500 [07:04<07:18,  1.75s/it]\u001b[A{\"learning_rate\": 7.500000000000001e-05, \"loss\": 1.0301872402429582, \"step\": 250}\n",
      "\n",
      "Iteration:  50% 250/500 [07:05<07:15,  1.74s/it]\u001b[A\n",
      "Iteration:  50% 251/500 [07:07<07:15,  1.75s/it]\u001b[A\n",
      "Iteration:  50% 252/500 [07:09<07:13,  1.75s/it]\u001b[A\n",
      "Iteration:  51% 253/500 [07:11<07:09,  1.74s/it]\u001b[A\n",
      "Iteration:  51% 254/500 [07:12<07:07,  1.74s/it]\u001b[A\n",
      "Iteration:  51% 255/500 [07:14<07:07,  1.74s/it]\u001b[A\n",
      "Iteration:  51% 256/500 [07:16<07:04,  1.74s/it]\u001b[A\n",
      "Iteration:  51% 257/500 [07:18<07:05,  1.75s/it]\u001b[A\n",
      "Iteration:  52% 258/500 [07:19<07:05,  1.76s/it]\u001b[A\n",
      "Iteration:  52% 259/500 [07:21<07:03,  1.76s/it]\u001b[A\n",
      "Iteration:  52% 260/500 [07:23<07:01,  1.75s/it]\u001b[A\n",
      "Iteration:  52% 261/500 [07:25<07:03,  1.77s/it]\u001b[A\n",
      "Iteration:  52% 262/500 [07:27<07:01,  1.77s/it]\u001b[A\n",
      "Iteration:  53% 263/500 [07:28<06:59,  1.77s/it]\u001b[A\n",
      "Iteration:  53% 264/500 [07:30<06:56,  1.76s/it]\u001b[A\n",
      "Iteration:  53% 265/500 [07:32<06:55,  1.77s/it]\u001b[A\n",
      "Iteration:  53% 266/500 [07:34<06:53,  1.77s/it]\u001b[A\n",
      "Iteration:  53% 267/500 [07:35<06:50,  1.76s/it]\u001b[A\n",
      "Iteration:  54% 268/500 [07:37<06:47,  1.76s/it]\u001b[A\n",
      "Iteration:  54% 269/500 [07:39<06:46,  1.76s/it]\u001b[A\n",
      "Iteration:  54% 270/500 [07:41<06:45,  1.76s/it]\u001b[A\n",
      "Iteration:  54% 271/500 [07:42<06:43,  1.76s/it]\u001b[A\n",
      "Iteration:  54% 272/500 [07:44<06:42,  1.76s/it]\u001b[A\n",
      "Iteration:  55% 273/500 [07:46<06:40,  1.77s/it]\u001b[A\n",
      "Iteration:  55% 274/500 [07:48<06:38,  1.76s/it]\u001b[A\n",
      "Iteration:  55% 275/500 [07:49<06:37,  1.77s/it]\u001b[A\n",
      "Iteration:  55% 276/500 [07:51<06:32,  1.75s/it]\u001b[A\n",
      "Iteration:  55% 277/500 [07:53<06:33,  1.76s/it]\u001b[A\n",
      "Iteration:  56% 278/500 [07:55<06:29,  1.76s/it]\u001b[A\n",
      "Iteration:  56% 279/500 [07:57<06:30,  1.76s/it]\u001b[A\n",
      "Iteration:  56% 280/500 [07:58<06:29,  1.77s/it]\u001b[A\n",
      "Iteration:  56% 281/500 [08:00<06:25,  1.76s/it]\u001b[A\n",
      "Iteration:  56% 282/500 [08:02<06:21,  1.75s/it]\u001b[A\n",
      "Iteration:  57% 283/500 [08:04<06:20,  1.75s/it]\u001b[A\n",
      "Iteration:  57% 284/500 [08:05<06:20,  1.76s/it]\u001b[A\n",
      "Iteration:  57% 285/500 [08:07<06:20,  1.77s/it]\u001b[A\n",
      "Iteration:  57% 286/500 [08:09<06:15,  1.76s/it]\u001b[A\n",
      "Iteration:  57% 287/500 [08:11<06:13,  1.75s/it]\u001b[A\n",
      "Iteration:  58% 288/500 [08:12<06:11,  1.75s/it]\u001b[A\n",
      "Iteration:  58% 289/500 [08:14<06:10,  1.76s/it]\u001b[A\n",
      "Iteration:  58% 290/500 [08:16<06:08,  1.75s/it]\u001b[A\n",
      "Iteration:  58% 291/500 [08:18<06:04,  1.74s/it]\u001b[A\n",
      "Iteration:  58% 292/500 [08:19<06:02,  1.74s/it]\u001b[A\n",
      "Iteration:  59% 293/500 [08:21<06:01,  1.74s/it]\u001b[A\n",
      "Iteration:  59% 294/500 [08:23<06:01,  1.76s/it]\u001b[A\n",
      "Iteration:  59% 295/500 [08:25<06:01,  1.76s/it]\u001b[A\n",
      "Iteration:  59% 296/500 [08:26<05:59,  1.76s/it]\u001b[A\n",
      "Iteration:  59% 297/500 [08:28<05:55,  1.75s/it]\u001b[A\n",
      "Iteration:  60% 298/500 [08:30<05:53,  1.75s/it]\u001b[A\n",
      "Iteration:  60% 299/500 [08:32<05:50,  1.74s/it]\u001b[A{\"learning_rate\": 7e-05, \"loss\": 1.0569886380434037, \"step\": 300}\n",
      "\n",
      "Iteration:  60% 300/500 [08:33<05:50,  1.75s/it]\u001b[A\n",
      "Iteration:  60% 301/500 [08:35<05:48,  1.75s/it]\u001b[A\n",
      "Iteration:  60% 302/500 [08:37<05:47,  1.75s/it]\u001b[A\n",
      "Iteration:  61% 303/500 [08:39<05:43,  1.74s/it]\u001b[A\n",
      "Iteration:  61% 304/500 [08:40<05:44,  1.76s/it]\u001b[A\n",
      "Iteration:  61% 305/500 [08:42<05:40,  1.75s/it]\u001b[A\n",
      "Iteration:  61% 306/500 [08:44<05:37,  1.74s/it]\u001b[A\n",
      "Iteration:  61% 307/500 [08:46<05:37,  1.75s/it]\u001b[A\n",
      "Iteration:  62% 308/500 [08:47<05:36,  1.75s/it]\u001b[A\n",
      "Iteration:  62% 309/500 [08:49<05:36,  1.76s/it]\u001b[A\n",
      "Iteration:  62% 310/500 [08:51<05:36,  1.77s/it]\u001b[A\n",
      "Iteration:  62% 311/500 [08:53<05:34,  1.77s/it]\u001b[A\n",
      "Iteration:  62% 312/500 [08:54<05:30,  1.76s/it]\u001b[A\n",
      "Iteration:  63% 313/500 [08:56<05:28,  1.76s/it]\u001b[A\n",
      "Iteration:  63% 314/500 [08:58<05:24,  1.75s/it]\u001b[A\n",
      "Iteration:  63% 315/500 [09:00<05:23,  1.75s/it]\u001b[A\n",
      "Iteration:  63% 316/500 [09:01<05:22,  1.75s/it]\u001b[A\n",
      "Iteration:  63% 317/500 [09:03<05:19,  1.75s/it]\u001b[A\n",
      "Iteration:  64% 318/500 [09:05<05:19,  1.75s/it]\u001b[A\n",
      "Iteration:  64% 319/500 [09:07<05:17,  1.75s/it]\u001b[A\n",
      "Iteration:  64% 320/500 [09:08<05:15,  1.75s/it]\u001b[A\n",
      "Iteration:  64% 321/500 [09:10<05:15,  1.76s/it]\u001b[A\n",
      "Iteration:  64% 322/500 [09:12<05:12,  1.76s/it]\u001b[A\n",
      "Iteration:  65% 323/500 [09:14<05:10,  1.75s/it]\u001b[A\n",
      "Iteration:  65% 324/500 [09:15<05:08,  1.75s/it]\u001b[A\n",
      "Iteration:  65% 325/500 [09:17<05:05,  1.74s/it]\u001b[A\n",
      "Iteration:  65% 326/500 [09:19<05:03,  1.74s/it]\u001b[A\n",
      "Iteration:  65% 327/500 [09:21<05:02,  1.75s/it]\u001b[A\n",
      "Iteration:  66% 328/500 [09:22<05:03,  1.76s/it]\u001b[A\n",
      "Iteration:  66% 329/500 [09:24<05:00,  1.76s/it]\u001b[A\n",
      "Iteration:  66% 330/500 [09:26<04:59,  1.76s/it]\u001b[A\n",
      "Iteration:  66% 331/500 [09:28<04:56,  1.75s/it]\u001b[A\n",
      "Iteration:  66% 332/500 [09:29<04:53,  1.75s/it]\u001b[A\n",
      "Iteration:  67% 333/500 [09:31<04:52,  1.75s/it]\u001b[A\n",
      "Iteration:  67% 334/500 [09:33<04:51,  1.76s/it]\u001b[A\n",
      "Iteration:  67% 335/500 [09:35<04:50,  1.76s/it]\u001b[A\n",
      "Iteration:  67% 336/500 [09:37<04:50,  1.77s/it]\u001b[A\n",
      "Iteration:  67% 337/500 [09:38<04:48,  1.77s/it]\u001b[A\n",
      "Iteration:  68% 338/500 [09:40<04:46,  1.77s/it]\u001b[A\n",
      "Iteration:  68% 339/500 [09:42<04:43,  1.76s/it]\u001b[A\n",
      "Iteration:  68% 340/500 [09:44<04:41,  1.76s/it]\u001b[A\n",
      "Iteration:  68% 341/500 [09:45<04:40,  1.76s/it]\u001b[A\n",
      "Iteration:  68% 342/500 [09:47<04:36,  1.75s/it]\u001b[A\n",
      "Iteration:  69% 343/500 [09:49<04:36,  1.76s/it]\u001b[A\n",
      "Iteration:  69% 344/500 [09:51<04:33,  1.75s/it]\u001b[A\n",
      "Iteration:  69% 345/500 [09:52<04:32,  1.76s/it]\u001b[A\n",
      "Iteration:  69% 346/500 [09:54<04:30,  1.76s/it]\u001b[A\n",
      "Iteration:  69% 347/500 [09:56<04:28,  1.75s/it]\u001b[A\n",
      "Iteration:  70% 348/500 [09:58<04:26,  1.75s/it]\u001b[A\n",
      "Iteration:  70% 349/500 [09:59<04:25,  1.76s/it]\u001b[A{\"learning_rate\": 6.500000000000001e-05, \"loss\": 1.0299853736162186, \"step\": 350}\n",
      "\n",
      "Iteration:  70% 350/500 [10:01<04:24,  1.76s/it]\u001b[A\n",
      "Iteration:  70% 351/500 [10:03<04:22,  1.76s/it]\u001b[A\n",
      "Iteration:  70% 352/500 [10:05<04:22,  1.77s/it]\u001b[A\n",
      "Iteration:  71% 353/500 [10:06<04:19,  1.77s/it]\u001b[A\n",
      "Iteration:  71% 354/500 [10:08<04:16,  1.76s/it]\u001b[A\n",
      "Iteration:  71% 355/500 [10:10<04:13,  1.75s/it]\u001b[A\n",
      "Iteration:  71% 356/500 [10:12<04:11,  1.75s/it]\u001b[A\n",
      "Iteration:  71% 357/500 [10:13<04:10,  1.75s/it]\u001b[A\n",
      "Iteration:  72% 358/500 [10:15<04:08,  1.75s/it]\u001b[A\n",
      "Iteration:  72% 359/500 [10:17<04:05,  1.74s/it]\u001b[A\n",
      "Iteration:  72% 360/500 [10:19<04:05,  1.75s/it]\u001b[A\n",
      "Iteration:  72% 361/500 [10:20<04:03,  1.75s/it]\u001b[A\n",
      "Iteration:  72% 362/500 [10:22<04:01,  1.75s/it]\u001b[A\n",
      "Iteration:  73% 363/500 [10:24<04:00,  1.75s/it]\u001b[A\n",
      "Iteration:  73% 364/500 [10:26<03:57,  1.75s/it]\u001b[A\n",
      "Iteration:  73% 365/500 [10:27<03:56,  1.75s/it]\u001b[A\n",
      "Iteration:  73% 366/500 [10:29<03:54,  1.75s/it]\u001b[A\n",
      "Iteration:  73% 367/500 [10:31<03:54,  1.76s/it]\u001b[A\n",
      "Iteration:  74% 368/500 [10:33<03:52,  1.76s/it]\u001b[A\n",
      "Iteration:  74% 369/500 [10:34<03:51,  1.76s/it]\u001b[A\n",
      "Iteration:  74% 370/500 [10:36<03:49,  1.76s/it]\u001b[A\n",
      "Iteration:  74% 371/500 [10:38<03:47,  1.76s/it]\u001b[A\n",
      "Iteration:  74% 372/500 [10:40<03:44,  1.76s/it]\u001b[A\n",
      "Iteration:  75% 373/500 [10:41<03:42,  1.75s/it]\u001b[A\n",
      "Iteration:  75% 374/500 [10:43<03:40,  1.75s/it]\u001b[A\n",
      "Iteration:  75% 375/500 [10:45<03:40,  1.76s/it]\u001b[A\n",
      "Iteration:  75% 376/500 [10:47<03:38,  1.76s/it]\u001b[A\n",
      "Iteration:  75% 377/500 [10:48<03:34,  1.75s/it]\u001b[A\n",
      "Iteration:  76% 378/500 [10:50<03:34,  1.76s/it]\u001b[A\n",
      "Iteration:  76% 379/500 [10:52<03:32,  1.76s/it]\u001b[A\n",
      "Iteration:  76% 380/500 [10:54<03:28,  1.74s/it]\u001b[A\n",
      "Iteration:  76% 381/500 [10:56<03:29,  1.76s/it]\u001b[A\n",
      "Iteration:  76% 382/500 [10:57<03:29,  1.77s/it]\u001b[A\n",
      "Iteration:  77% 383/500 [10:59<03:27,  1.77s/it]\u001b[A\n",
      "Iteration:  77% 384/500 [11:01<03:25,  1.77s/it]\u001b[A\n",
      "Iteration:  77% 385/500 [11:03<03:22,  1.76s/it]\u001b[A\n",
      "Iteration:  77% 386/500 [11:04<03:21,  1.77s/it]\u001b[A\n",
      "Iteration:  77% 387/500 [11:06<03:20,  1.77s/it]\u001b[A\n",
      "Iteration:  78% 388/500 [11:08<03:18,  1.77s/it]\u001b[A\n",
      "Iteration:  78% 389/500 [11:10<03:15,  1.76s/it]\u001b[A\n",
      "Iteration:  78% 390/500 [11:11<03:14,  1.77s/it]\u001b[A\n",
      "Iteration:  78% 391/500 [11:13<03:12,  1.77s/it]\u001b[A\n",
      "Iteration:  78% 392/500 [11:15<03:09,  1.76s/it]\u001b[A\n",
      "Iteration:  79% 393/500 [11:17<03:07,  1.75s/it]\u001b[A\n",
      "Iteration:  79% 394/500 [11:18<03:04,  1.75s/it]\u001b[A\n",
      "Iteration:  79% 395/500 [11:20<03:03,  1.75s/it]\u001b[A\n",
      "Iteration:  79% 396/500 [11:22<03:03,  1.76s/it]\u001b[A\n",
      "Iteration:  79% 397/500 [11:24<03:01,  1.76s/it]\u001b[A\n",
      "Iteration:  80% 398/500 [11:25<02:57,  1.74s/it]\u001b[A\n",
      "Iteration:  80% 399/500 [11:27<02:56,  1.74s/it]\u001b[A{\"learning_rate\": 6e-05, \"loss\": 0.9584501451253891, \"step\": 400}\n",
      "\n",
      "Iteration:  80% 400/500 [11:29<02:54,  1.75s/it]\u001b[A\n",
      "Iteration:  80% 401/500 [11:31<02:53,  1.75s/it]\u001b[A\n",
      "Iteration:  80% 402/500 [11:32<02:51,  1.75s/it]\u001b[A\n",
      "Iteration:  81% 403/500 [11:34<02:50,  1.76s/it]\u001b[A\n",
      "Iteration:  81% 404/500 [11:36<02:49,  1.76s/it]\u001b[A\n",
      "Iteration:  81% 405/500 [11:38<02:47,  1.76s/it]\u001b[A\n",
      "Iteration:  81% 406/500 [11:40<02:45,  1.76s/it]\u001b[A\n",
      "Iteration:  81% 407/500 [11:41<02:43,  1.76s/it]\u001b[A\n",
      "Iteration:  82% 408/500 [11:43<02:42,  1.76s/it]\u001b[A\n",
      "Iteration:  82% 409/500 [11:45<02:41,  1.77s/it]\u001b[A\n",
      "Iteration:  82% 410/500 [11:47<02:38,  1.77s/it]\u001b[A\n",
      "Iteration:  82% 411/500 [11:48<02:37,  1.77s/it]\u001b[A\n",
      "Iteration:  82% 412/500 [11:50<02:36,  1.78s/it]\u001b[A\n",
      "Iteration:  83% 413/500 [11:52<02:33,  1.76s/it]\u001b[A\n",
      "Iteration:  83% 414/500 [11:54<02:30,  1.76s/it]\u001b[A\n",
      "Iteration:  83% 415/500 [11:55<02:29,  1.76s/it]\u001b[A\n",
      "Iteration:  83% 416/500 [11:57<02:27,  1.76s/it]\u001b[A\n",
      "Iteration:  83% 417/500 [11:59<02:25,  1.76s/it]\u001b[A\n",
      "Iteration:  84% 418/500 [12:01<02:24,  1.76s/it]\u001b[A\n",
      "Iteration:  84% 419/500 [12:02<02:22,  1.75s/it]\u001b[A\n",
      "Iteration:  84% 420/500 [12:04<02:20,  1.75s/it]\u001b[A\n",
      "Iteration:  84% 421/500 [12:06<02:18,  1.75s/it]\u001b[A\n",
      "Iteration:  84% 422/500 [12:08<02:16,  1.75s/it]\u001b[A\n",
      "Iteration:  85% 423/500 [12:09<02:14,  1.74s/it]\u001b[A\n",
      "Iteration:  85% 424/500 [12:11<02:12,  1.74s/it]\u001b[A\n",
      "Iteration:  85% 425/500 [12:13<02:12,  1.76s/it]\u001b[A\n",
      "Iteration:  85% 426/500 [12:15<02:09,  1.75s/it]\u001b[A\n",
      "Iteration:  85% 427/500 [12:16<02:07,  1.75s/it]\u001b[A\n",
      "Iteration:  86% 428/500 [12:18<02:05,  1.75s/it]\u001b[A\n",
      "Iteration:  86% 429/500 [12:20<02:04,  1.75s/it]\u001b[A\n",
      "Iteration:  86% 430/500 [12:22<02:02,  1.75s/it]\u001b[A\n",
      "Iteration:  86% 431/500 [12:23<02:00,  1.74s/it]\u001b[A\n",
      "Iteration:  86% 432/500 [12:25<01:59,  1.75s/it]\u001b[A\n",
      "Iteration:  87% 433/500 [12:27<01:57,  1.75s/it]\u001b[A\n",
      "Iteration:  87% 434/500 [12:29<01:54,  1.74s/it]\u001b[A\n",
      "Iteration:  87% 435/500 [12:30<01:53,  1.75s/it]\u001b[A\n",
      "Iteration:  87% 436/500 [12:32<01:52,  1.76s/it]\u001b[A\n",
      "Iteration:  87% 437/500 [12:34<01:50,  1.75s/it]\u001b[A\n",
      "Iteration:  88% 438/500 [12:36<01:48,  1.76s/it]\u001b[A\n",
      "Iteration:  88% 439/500 [12:37<01:47,  1.76s/it]\u001b[A\n",
      "Iteration:  88% 440/500 [12:39<01:44,  1.75s/it]\u001b[A\n",
      "Iteration:  88% 441/500 [12:41<01:43,  1.75s/it]\u001b[A\n",
      "Iteration:  88% 442/500 [12:43<01:41,  1.75s/it]\u001b[A\n",
      "Iteration:  89% 443/500 [12:44<01:39,  1.75s/it]\u001b[A\n",
      "Iteration:  89% 444/500 [12:46<01:38,  1.76s/it]\u001b[A\n",
      "Iteration:  89% 445/500 [12:48<01:36,  1.76s/it]\u001b[A\n",
      "Iteration:  89% 446/500 [12:50<01:34,  1.76s/it]\u001b[A\n",
      "Iteration:  89% 447/500 [12:51<01:32,  1.75s/it]\u001b[A\n",
      "Iteration:  90% 448/500 [12:53<01:30,  1.74s/it]\u001b[A\n",
      "Iteration:  90% 449/500 [12:55<01:29,  1.75s/it]\u001b[A{\"learning_rate\": 5.500000000000001e-05, \"loss\": 0.9842779606580734, \"step\": 450}\n",
      "\n",
      "Iteration:  90% 450/500 [12:57<01:27,  1.75s/it]\u001b[A\n",
      "Iteration:  90% 451/500 [12:58<01:25,  1.75s/it]\u001b[A\n",
      "Iteration:  90% 452/500 [13:00<01:23,  1.75s/it]\u001b[A\n",
      "Iteration:  91% 453/500 [13:02<01:22,  1.75s/it]\u001b[A\n",
      "Iteration:  91% 454/500 [13:04<01:20,  1.75s/it]\u001b[A\n",
      "Iteration:  91% 455/500 [13:05<01:18,  1.75s/it]\u001b[A\n",
      "Iteration:  91% 456/500 [13:07<01:16,  1.75s/it]\u001b[A\n",
      "Iteration:  91% 457/500 [13:09<01:14,  1.74s/it]\u001b[A\n",
      "Iteration:  92% 458/500 [13:11<01:13,  1.74s/it]\u001b[A\n",
      "Iteration:  92% 459/500 [13:12<01:12,  1.76s/it]\u001b[A\n",
      "Iteration:  92% 460/500 [13:14<01:10,  1.76s/it]\u001b[A\n",
      "Iteration:  92% 461/500 [13:16<01:08,  1.76s/it]\u001b[A\n",
      "Iteration:  92% 462/500 [13:18<01:06,  1.76s/it]\u001b[A\n",
      "Iteration:  93% 463/500 [13:20<01:04,  1.76s/it]\u001b[A\n",
      "Iteration:  93% 464/500 [13:21<01:03,  1.76s/it]\u001b[A\n",
      "Iteration:  93% 465/500 [13:23<01:01,  1.76s/it]\u001b[A\n",
      "Iteration:  93% 466/500 [13:25<00:59,  1.75s/it]\u001b[A\n",
      "Iteration:  93% 467/500 [13:27<00:57,  1.75s/it]\u001b[A\n",
      "Iteration:  94% 468/500 [13:28<00:56,  1.76s/it]\u001b[A\n",
      "Iteration:  94% 469/500 [13:30<00:54,  1.76s/it]\u001b[A\n",
      "Iteration:  94% 470/500 [13:32<00:52,  1.76s/it]\u001b[A\n",
      "Iteration:  94% 471/500 [13:34<00:50,  1.76s/it]\u001b[A\n",
      "Iteration:  94% 472/500 [13:35<00:49,  1.76s/it]\u001b[A\n",
      "Iteration:  95% 473/500 [13:37<00:47,  1.76s/it]\u001b[A\n",
      "Iteration:  95% 474/500 [13:39<00:45,  1.76s/it]\u001b[A\n",
      "Iteration:  95% 475/500 [13:41<00:44,  1.76s/it]\u001b[A\n",
      "Iteration:  95% 476/500 [13:42<00:42,  1.76s/it]\u001b[A\n",
      "Iteration:  95% 477/500 [13:44<00:40,  1.75s/it]\u001b[A\n",
      "Iteration:  96% 478/500 [13:46<00:38,  1.75s/it]\u001b[A\n",
      "Iteration:  96% 479/500 [13:48<00:36,  1.74s/it]\u001b[A\n",
      "Iteration:  96% 480/500 [13:49<00:35,  1.75s/it]\u001b[A\n",
      "Iteration:  96% 481/500 [13:51<00:33,  1.75s/it]\u001b[A\n",
      "Iteration:  96% 482/500 [13:53<00:31,  1.74s/it]\u001b[A\n",
      "Iteration:  97% 483/500 [13:55<00:29,  1.75s/it]\u001b[A\n",
      "Iteration:  97% 484/500 [13:56<00:27,  1.75s/it]\u001b[A\n",
      "Iteration:  97% 485/500 [13:58<00:26,  1.75s/it]\u001b[A\n",
      "Iteration:  97% 486/500 [14:00<00:24,  1.74s/it]\u001b[A\n",
      "Iteration:  97% 487/500 [14:02<00:22,  1.74s/it]\u001b[A\n",
      "Iteration:  98% 488/500 [14:03<00:20,  1.74s/it]\u001b[A\n",
      "Iteration:  98% 489/500 [14:05<00:19,  1.74s/it]\u001b[A\n",
      "Iteration:  98% 490/500 [14:07<00:17,  1.75s/it]\u001b[A\n",
      "Iteration:  98% 491/500 [14:09<00:15,  1.75s/it]\u001b[A\n",
      "Iteration:  98% 492/500 [14:10<00:13,  1.74s/it]\u001b[A\n",
      "Iteration:  99% 493/500 [14:12<00:12,  1.74s/it]\u001b[A\n",
      "Iteration:  99% 494/500 [14:14<00:10,  1.75s/it]\u001b[A\n",
      "Iteration:  99% 495/500 [14:16<00:08,  1.74s/it]\u001b[A\n",
      "Iteration:  99% 496/500 [14:17<00:06,  1.74s/it]\u001b[A\n",
      "Iteration:  99% 497/500 [14:19<00:05,  1.74s/it]\u001b[A\n",
      "Iteration: 100% 498/500 [14:21<00:03,  1.74s/it]\u001b[A\n",
      "Iteration: 100% 499/500 [14:22<00:01,  1.74s/it]\u001b[A{\"learning_rate\": 5e-05, \"loss\": 0.9515641868114472, \"step\": 500}\n",
      "\n",
      "Iteration: 100% 500/500 [14:24<00:00,  1.73s/it]\n",
      "Epoch:  50% 1/2 [14:24<14:24, 864.73s/it]\n",
      "Iteration:   0% 0/500 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   0% 1/500 [00:01<14:20,  1.72s/it]\u001b[A\n",
      "Iteration:   0% 2/500 [00:03<14:30,  1.75s/it]\u001b[A\n",
      "Iteration:   1% 3/500 [00:05<14:32,  1.75s/it]\u001b[A\n",
      "Iteration:   1% 4/500 [00:06<14:28,  1.75s/it]\u001b[A\n",
      "Iteration:   1% 5/500 [00:08<14:30,  1.76s/it]\u001b[A\n",
      "Iteration:   1% 6/500 [00:10<14:29,  1.76s/it]\u001b[A\n",
      "Iteration:   1% 7/500 [00:12<14:20,  1.75s/it]\u001b[A\n",
      "Iteration:   2% 8/500 [00:14<14:25,  1.76s/it]\u001b[A\n",
      "Iteration:   2% 9/500 [00:15<14:21,  1.75s/it]\u001b[A\n",
      "Iteration:   2% 10/500 [00:17<14:15,  1.75s/it]\u001b[A\n",
      "Iteration:   2% 11/500 [00:19<14:17,  1.75s/it]\u001b[A\n",
      "Iteration:   2% 12/500 [00:21<14:11,  1.74s/it]\u001b[A\n",
      "Iteration:   3% 13/500 [00:22<14:11,  1.75s/it]\u001b[A\n",
      "Iteration:   3% 14/500 [00:24<14:09,  1.75s/it]\u001b[A\n",
      "Iteration:   3% 15/500 [00:26<14:07,  1.75s/it]\u001b[A\n",
      "Iteration:   3% 16/500 [00:28<14:13,  1.76s/it]\u001b[A\n",
      "Iteration:   3% 17/500 [00:29<14:13,  1.77s/it]\u001b[A\n",
      "Iteration:   4% 18/500 [00:31<14:09,  1.76s/it]\u001b[A\n",
      "Iteration:   4% 19/500 [00:33<14:09,  1.77s/it]\u001b[A\n",
      "Iteration:   4% 20/500 [00:35<14:02,  1.76s/it]\u001b[A\n",
      "Iteration:   4% 21/500 [00:36<14:04,  1.76s/it]\u001b[A\n",
      "Iteration:   4% 22/500 [00:38<14:02,  1.76s/it]\u001b[A\n",
      "Iteration:   5% 23/500 [00:40<13:58,  1.76s/it]\u001b[A\n",
      "Iteration:   5% 24/500 [00:42<13:57,  1.76s/it]\u001b[A\n",
      "Iteration:   5% 25/500 [00:43<13:54,  1.76s/it]\u001b[A\n",
      "Iteration:   5% 26/500 [00:45<13:52,  1.76s/it]\u001b[A\n",
      "Iteration:   5% 27/500 [00:47<13:51,  1.76s/it]\u001b[A\n",
      "Iteration:   6% 28/500 [00:49<13:53,  1.77s/it]\u001b[A\n",
      "Iteration:   6% 29/500 [00:50<13:44,  1.75s/it]\u001b[A\n",
      "Iteration:   6% 30/500 [00:52<13:43,  1.75s/it]\u001b[A\n",
      "Iteration:   6% 31/500 [00:54<13:49,  1.77s/it]\u001b[A\n",
      "Iteration:   6% 32/500 [00:56<13:42,  1.76s/it]\u001b[A\n",
      "Iteration:   7% 33/500 [00:58<13:47,  1.77s/it]\u001b[A\n",
      "Iteration:   7% 34/500 [00:59<13:40,  1.76s/it]\u001b[A\n",
      "Iteration:   7% 35/500 [01:01<13:36,  1.76s/it]\u001b[A\n",
      "Iteration:   7% 36/500 [01:03<13:30,  1.75s/it]\u001b[A\n",
      "Iteration:   7% 37/500 [01:04<13:26,  1.74s/it]\u001b[A\n",
      "Iteration:   8% 38/500 [01:06<13:30,  1.75s/it]\u001b[A\n",
      "Iteration:   8% 39/500 [01:08<13:25,  1.75s/it]\u001b[A\n",
      "Iteration:   8% 40/500 [01:10<13:27,  1.75s/it]\u001b[A\n",
      "Iteration:   8% 41/500 [01:11<13:24,  1.75s/it]\u001b[A\n",
      "Iteration:   8% 42/500 [01:13<13:19,  1.75s/it]\u001b[A\n",
      "Iteration:   9% 43/500 [01:15<13:21,  1.75s/it]\u001b[A\n",
      "Iteration:   9% 44/500 [01:17<13:22,  1.76s/it]\u001b[A\n",
      "Iteration:   9% 45/500 [01:18<13:15,  1.75s/it]\u001b[A\n",
      "Iteration:   9% 46/500 [01:20<13:15,  1.75s/it]\u001b[A\n",
      "Iteration:   9% 47/500 [01:22<13:17,  1.76s/it]\u001b[A\n",
      "Iteration:  10% 48/500 [01:24<13:16,  1.76s/it]\u001b[A\n",
      "Iteration:  10% 49/500 [01:26<13:13,  1.76s/it]\u001b[A{\"learning_rate\": 4.5e-05, \"loss\": 0.6863673350214958, \"step\": 550}\n",
      "\n",
      "Iteration:  10% 50/500 [01:27<13:13,  1.76s/it]\u001b[A\n",
      "Iteration:  10% 51/500 [01:29<13:16,  1.77s/it]\u001b[A\n",
      "Iteration:  10% 52/500 [01:31<13:16,  1.78s/it]\u001b[A\n",
      "Iteration:  11% 53/500 [01:33<13:07,  1.76s/it]\u001b[A\n",
      "Iteration:  11% 54/500 [01:34<13:03,  1.76s/it]\u001b[A\n",
      "Iteration:  11% 55/500 [01:36<13:10,  1.78s/it]\u001b[A\n",
      "Iteration:  11% 56/500 [01:38<13:05,  1.77s/it]\u001b[A\n",
      "Iteration:  11% 57/500 [01:40<13:00,  1.76s/it]\u001b[A\n",
      "Iteration:  12% 58/500 [01:41<12:53,  1.75s/it]\u001b[A\n",
      "Iteration:  12% 59/500 [01:43<12:56,  1.76s/it]\u001b[A\n",
      "Iteration:  12% 60/500 [01:45<12:58,  1.77s/it]\u001b[A\n",
      "Iteration:  12% 61/500 [01:47<12:56,  1.77s/it]\u001b[A\n",
      "Iteration:  12% 62/500 [01:48<12:51,  1.76s/it]\u001b[A\n",
      "Iteration:  13% 63/500 [01:50<12:49,  1.76s/it]\u001b[A\n",
      "Iteration:  13% 64/500 [01:52<12:52,  1.77s/it]\u001b[A\n",
      "Iteration:  13% 65/500 [01:54<12:46,  1.76s/it]\u001b[A\n",
      "Iteration:  13% 66/500 [01:56<12:46,  1.77s/it]\u001b[A\n",
      "Iteration:  13% 67/500 [01:57<12:47,  1.77s/it]\u001b[A\n",
      "Iteration:  14% 68/500 [01:59<12:42,  1.77s/it]\u001b[A\n",
      "Iteration:  14% 69/500 [02:01<12:40,  1.76s/it]\u001b[A\n",
      "Iteration:  14% 70/500 [02:03<12:42,  1.77s/it]\u001b[A\n",
      "Iteration:  14% 71/500 [02:04<12:35,  1.76s/it]\u001b[A\n",
      "Iteration:  14% 72/500 [02:06<12:34,  1.76s/it]\u001b[A\n",
      "Iteration:  15% 73/500 [02:08<12:27,  1.75s/it]\u001b[A\n",
      "Iteration:  15% 74/500 [02:10<12:30,  1.76s/it]\u001b[A\n",
      "Iteration:  15% 75/500 [02:11<12:32,  1.77s/it]\u001b[A\n",
      "Iteration:  15% 76/500 [02:13<12:26,  1.76s/it]\u001b[A\n",
      "Iteration:  15% 77/500 [02:15<12:22,  1.76s/it]\u001b[A\n",
      "Iteration:  16% 78/500 [02:17<12:22,  1.76s/it]\u001b[A\n",
      "Iteration:  16% 79/500 [02:18<12:19,  1.76s/it]\u001b[A\n",
      "Iteration:  16% 80/500 [02:20<12:12,  1.74s/it]\u001b[A\n",
      "Iteration:  16% 81/500 [02:22<12:12,  1.75s/it]\u001b[A\n",
      "Iteration:  16% 82/500 [02:24<12:10,  1.75s/it]\u001b[A\n",
      "Iteration:  17% 83/500 [02:25<12:11,  1.75s/it]\u001b[A\n",
      "Iteration:  17% 84/500 [02:27<12:12,  1.76s/it]\u001b[A\n",
      "Iteration:  17% 85/500 [02:29<12:08,  1.76s/it]\u001b[A\n",
      "Iteration:  17% 86/500 [02:31<12:11,  1.77s/it]\u001b[A\n",
      "Iteration:  17% 87/500 [02:33<12:11,  1.77s/it]\u001b[A\n",
      "Iteration:  18% 88/500 [02:34<12:10,  1.77s/it]\u001b[A\n",
      "Iteration:  18% 89/500 [02:36<12:08,  1.77s/it]\u001b[A\n",
      "Iteration:  18% 90/500 [02:38<12:07,  1.77s/it]\u001b[A\n",
      "Iteration:  18% 91/500 [02:40<12:03,  1.77s/it]\u001b[A\n",
      "Iteration:  18% 92/500 [02:41<12:03,  1.77s/it]\u001b[A\n",
      "Iteration:  19% 93/500 [02:43<11:57,  1.76s/it]\u001b[A\n",
      "Iteration:  19% 94/500 [02:45<11:55,  1.76s/it]\u001b[A\n",
      "Iteration:  19% 95/500 [02:47<11:53,  1.76s/it]\u001b[A\n",
      "Iteration:  19% 96/500 [02:48<11:52,  1.76s/it]\u001b[A\n",
      "Iteration:  19% 97/500 [02:50<11:51,  1.77s/it]\u001b[A\n",
      "Iteration:  20% 98/500 [02:52<11:54,  1.78s/it]\u001b[A\n",
      "Iteration:  20% 99/500 [02:54<11:46,  1.76s/it]\u001b[A{\"learning_rate\": 4e-05, \"loss\": 0.712733610868454, \"step\": 600}\n",
      "\n",
      "Iteration:  20% 100/500 [02:55<11:44,  1.76s/it]\u001b[A\n",
      "Iteration:  20% 101/500 [02:57<11:43,  1.76s/it]\u001b[A\n",
      "Iteration:  20% 102/500 [02:59<11:40,  1.76s/it]\u001b[A\n",
      "Iteration:  21% 103/500 [03:01<11:38,  1.76s/it]\u001b[A\n",
      "Iteration:  21% 104/500 [03:02<11:32,  1.75s/it]\u001b[A\n",
      "Iteration:  21% 105/500 [03:04<11:33,  1.76s/it]\u001b[A\n",
      "Iteration:  21% 106/500 [03:06<11:30,  1.75s/it]\u001b[A\n",
      "Iteration:  21% 107/500 [03:08<11:26,  1.75s/it]\u001b[A\n",
      "Iteration:  22% 108/500 [03:10<11:28,  1.76s/it]\u001b[A\n",
      "Iteration:  22% 109/500 [03:11<11:28,  1.76s/it]\u001b[A\n",
      "Iteration:  22% 110/500 [03:13<11:27,  1.76s/it]\u001b[A\n",
      "Iteration:  22% 111/500 [03:15<11:26,  1.76s/it]\u001b[A\n",
      "Iteration:  22% 112/500 [03:17<11:22,  1.76s/it]\u001b[A\n",
      "Iteration:  23% 113/500 [03:18<11:22,  1.76s/it]\u001b[A\n",
      "Iteration:  23% 114/500 [03:20<11:21,  1.77s/it]\u001b[A\n",
      "Iteration:  23% 115/500 [03:22<11:12,  1.75s/it]\u001b[A\n",
      "Iteration:  23% 116/500 [03:24<11:11,  1.75s/it]\u001b[A\n",
      "Iteration:  23% 117/500 [03:25<11:11,  1.75s/it]\u001b[A\n",
      "Iteration:  24% 118/500 [03:27<11:12,  1.76s/it]\u001b[A\n",
      "Iteration:  24% 119/500 [03:29<11:11,  1.76s/it]\u001b[A\n",
      "Iteration:  24% 120/500 [03:31<11:07,  1.76s/it]\u001b[A\n",
      "Iteration:  24% 121/500 [03:32<11:01,  1.75s/it]\u001b[A\n",
      "Iteration:  24% 122/500 [03:34<11:00,  1.75s/it]\u001b[A\n",
      "Iteration:  25% 123/500 [03:36<10:59,  1.75s/it]\u001b[A\n",
      "Iteration:  25% 124/500 [03:38<10:55,  1.74s/it]\u001b[A\n",
      "Iteration:  25% 125/500 [03:39<10:53,  1.74s/it]\u001b[A\n",
      "Iteration:  25% 126/500 [03:41<10:52,  1.75s/it]\u001b[A\n",
      "Iteration:  25% 127/500 [03:43<10:49,  1.74s/it]\u001b[A\n",
      "Iteration:  26% 128/500 [03:45<10:49,  1.75s/it]\u001b[A\n",
      "Iteration:  26% 129/500 [03:46<10:48,  1.75s/it]\u001b[A\n",
      "Iteration:  26% 130/500 [03:48<10:47,  1.75s/it]\u001b[A\n",
      "Iteration:  26% 131/500 [03:50<10:43,  1.74s/it]\u001b[A\n",
      "Iteration:  26% 132/500 [03:52<10:42,  1.75s/it]\u001b[A\n",
      "Iteration:  27% 133/500 [03:53<10:40,  1.74s/it]\u001b[A\n",
      "Iteration:  27% 134/500 [03:55<10:41,  1.75s/it]\u001b[A\n",
      "Iteration:  27% 135/500 [03:57<10:40,  1.75s/it]\u001b[A\n",
      "Iteration:  27% 136/500 [03:59<10:35,  1.75s/it]\u001b[A\n",
      "Iteration:  27% 137/500 [04:00<10:31,  1.74s/it]\u001b[A\n",
      "Iteration:  28% 138/500 [04:02<10:32,  1.75s/it]\u001b[A\n",
      "Iteration:  28% 139/500 [04:04<10:30,  1.75s/it]\u001b[A\n",
      "Iteration:  28% 140/500 [04:06<10:27,  1.74s/it]\u001b[A\n",
      "Iteration:  28% 141/500 [04:07<10:29,  1.75s/it]\u001b[A\n",
      "Iteration:  28% 142/500 [04:09<10:30,  1.76s/it]\u001b[A\n",
      "Iteration:  29% 143/500 [04:11<10:24,  1.75s/it]\u001b[A\n",
      "Iteration:  29% 144/500 [04:13<10:23,  1.75s/it]\u001b[A\n",
      "Iteration:  29% 145/500 [04:14<10:19,  1.74s/it]\u001b[A\n",
      "Iteration:  29% 146/500 [04:16<10:17,  1.74s/it]\u001b[A\n",
      "Iteration:  29% 147/500 [04:18<10:13,  1.74s/it]\u001b[A\n",
      "Iteration:  30% 148/500 [04:19<10:10,  1.73s/it]\u001b[A\n",
      "Iteration:  30% 149/500 [04:21<10:11,  1.74s/it]\u001b[A{\"learning_rate\": 3.5e-05, \"loss\": 0.644409139752388, \"step\": 650}\n",
      "\n",
      "Iteration:  30% 150/500 [04:23<10:09,  1.74s/it]\u001b[A\n",
      "Iteration:  30% 151/500 [04:25<10:11,  1.75s/it]\u001b[A\n",
      "Iteration:  30% 152/500 [04:26<10:09,  1.75s/it]\u001b[A\n",
      "Iteration:  31% 153/500 [04:28<10:09,  1.76s/it]\u001b[A\n",
      "Iteration:  31% 154/500 [04:30<10:05,  1.75s/it]\u001b[A\n",
      "Iteration:  31% 155/500 [04:32<10:04,  1.75s/it]\u001b[A\n",
      "Iteration:  31% 156/500 [04:33<10:02,  1.75s/it]\u001b[A\n",
      "Iteration:  31% 157/500 [04:35<09:58,  1.74s/it]\u001b[A\n",
      "Iteration:  32% 158/500 [04:37<09:58,  1.75s/it]\u001b[A\n",
      "Iteration:  32% 159/500 [04:39<09:59,  1.76s/it]\u001b[A\n",
      "Iteration:  32% 160/500 [04:41<10:00,  1.77s/it]\u001b[A\n",
      "Iteration:  32% 161/500 [04:42<09:58,  1.77s/it]\u001b[A\n",
      "Iteration:  32% 162/500 [04:44<09:55,  1.76s/it]\u001b[A\n",
      "Iteration:  33% 163/500 [04:46<09:54,  1.76s/it]\u001b[A\n",
      "Iteration:  33% 164/500 [04:48<09:47,  1.75s/it]\u001b[A\n",
      "Iteration:  33% 165/500 [04:49<09:47,  1.75s/it]\u001b[A\n",
      "Iteration:  33% 166/500 [04:51<09:44,  1.75s/it]\u001b[A\n",
      "Iteration:  33% 167/500 [04:53<09:46,  1.76s/it]\u001b[A\n",
      "Iteration:  34% 168/500 [04:55<09:44,  1.76s/it]\u001b[A\n",
      "Iteration:  34% 169/500 [04:56<09:45,  1.77s/it]\u001b[A\n",
      "Iteration:  34% 170/500 [04:58<09:39,  1.76s/it]\u001b[A\n",
      "Iteration:  34% 171/500 [05:00<09:36,  1.75s/it]\u001b[A\n",
      "Iteration:  34% 172/500 [05:02<09:36,  1.76s/it]\u001b[A\n",
      "Iteration:  35% 173/500 [05:03<09:34,  1.76s/it]\u001b[A\n",
      "Iteration:  35% 174/500 [05:05<09:28,  1.74s/it]\u001b[A\n",
      "Iteration:  35% 175/500 [05:07<09:26,  1.74s/it]\u001b[A\n",
      "Iteration:  35% 176/500 [05:09<09:25,  1.75s/it]\u001b[A\n",
      "Iteration:  35% 177/500 [05:10<09:26,  1.75s/it]\u001b[A\n",
      "Iteration:  36% 178/500 [05:12<09:22,  1.75s/it]\u001b[A\n",
      "Iteration:  36% 179/500 [05:14<09:23,  1.76s/it]\u001b[A\n",
      "Iteration:  36% 180/500 [05:16<09:24,  1.76s/it]\u001b[A\n",
      "Iteration:  36% 181/500 [05:17<09:19,  1.75s/it]\u001b[A\n",
      "Iteration:  36% 182/500 [05:19<09:16,  1.75s/it]\u001b[A\n",
      "Iteration:  37% 183/500 [05:21<09:14,  1.75s/it]\u001b[A\n",
      "Iteration:  37% 184/500 [05:23<09:12,  1.75s/it]\u001b[A\n",
      "Iteration:  37% 185/500 [05:24<09:15,  1.76s/it]\u001b[A\n",
      "Iteration:  37% 186/500 [05:26<09:11,  1.76s/it]\u001b[A\n",
      "Iteration:  37% 187/500 [05:28<09:12,  1.77s/it]\u001b[A\n",
      "Iteration:  38% 188/500 [05:30<09:09,  1.76s/it]\u001b[A\n",
      "Iteration:  38% 189/500 [05:31<09:09,  1.77s/it]\u001b[A\n",
      "Iteration:  38% 190/500 [05:33<09:08,  1.77s/it]\u001b[A\n",
      "Iteration:  38% 191/500 [05:35<09:01,  1.75s/it]\u001b[A\n",
      "Iteration:  38% 192/500 [05:37<08:59,  1.75s/it]\u001b[A\n",
      "Iteration:  39% 193/500 [05:39<09:01,  1.76s/it]\u001b[A\n",
      "Iteration:  39% 194/500 [05:40<08:58,  1.76s/it]\u001b[A\n",
      "Iteration:  39% 195/500 [05:42<08:58,  1.76s/it]\u001b[A\n",
      "Iteration:  39% 196/500 [05:44<08:53,  1.75s/it]\u001b[A\n",
      "Iteration:  39% 197/500 [05:45<08:48,  1.74s/it]\u001b[A\n",
      "Iteration:  40% 198/500 [05:47<08:48,  1.75s/it]\u001b[A\n",
      "Iteration:  40% 199/500 [05:49<08:43,  1.74s/it]\u001b[A{\"learning_rate\": 3e-05, \"loss\": 0.6304161177575588, \"step\": 700}\n",
      "\n",
      "Iteration:  40% 200/500 [05:51<08:39,  1.73s/it]\u001b[A\n",
      "Iteration:  40% 201/500 [05:52<08:40,  1.74s/it]\u001b[A\n",
      "Iteration:  40% 202/500 [05:54<08:42,  1.75s/it]\u001b[A\n",
      "Iteration:  41% 203/500 [05:56<08:39,  1.75s/it]\u001b[A\n",
      "Iteration:  41% 204/500 [05:58<08:39,  1.75s/it]\u001b[A\n",
      "Iteration:  41% 205/500 [05:59<08:37,  1.76s/it]\u001b[A\n",
      "Iteration:  41% 206/500 [06:01<08:38,  1.76s/it]\u001b[A\n",
      "Iteration:  41% 207/500 [06:03<08:37,  1.77s/it]\u001b[A\n",
      "Iteration:  42% 208/500 [06:05<08:36,  1.77s/it]\u001b[A\n",
      "Iteration:  42% 209/500 [06:07<08:31,  1.76s/it]\u001b[A\n",
      "Iteration:  42% 210/500 [06:08<08:31,  1.76s/it]\u001b[A\n",
      "Iteration:  42% 211/500 [06:10<08:29,  1.76s/it]\u001b[A\n",
      "Iteration:  42% 212/500 [06:12<08:28,  1.76s/it]\u001b[A\n",
      "Iteration:  43% 213/500 [06:14<08:24,  1.76s/it]\u001b[A\n",
      "Iteration:  43% 214/500 [06:15<08:23,  1.76s/it]\u001b[A\n",
      "Iteration:  43% 215/500 [06:17<08:23,  1.77s/it]\u001b[A\n",
      "Iteration:  43% 216/500 [06:19<08:19,  1.76s/it]\u001b[A\n",
      "Iteration:  43% 217/500 [06:21<08:18,  1.76s/it]\u001b[A\n",
      "Iteration:  44% 218/500 [06:22<08:17,  1.76s/it]\u001b[A\n",
      "Iteration:  44% 219/500 [06:24<08:13,  1.76s/it]\u001b[A\n",
      "Iteration:  44% 220/500 [06:26<08:10,  1.75s/it]\u001b[A\n",
      "Iteration:  44% 221/500 [06:28<08:10,  1.76s/it]\u001b[A\n",
      "Iteration:  44% 222/500 [06:29<08:06,  1.75s/it]\u001b[A\n",
      "Iteration:  45% 223/500 [06:31<08:02,  1.74s/it]\u001b[A\n",
      "Iteration:  45% 224/500 [06:33<08:01,  1.75s/it]\u001b[A\n",
      "Iteration:  45% 225/500 [06:35<08:02,  1.75s/it]\u001b[A\n",
      "Iteration:  45% 226/500 [06:36<07:59,  1.75s/it]\u001b[A\n",
      "Iteration:  45% 227/500 [06:38<07:57,  1.75s/it]\u001b[A\n",
      "Iteration:  46% 228/500 [06:40<07:53,  1.74s/it]\u001b[A\n",
      "Iteration:  46% 229/500 [06:42<07:53,  1.75s/it]\u001b[A\n",
      "Iteration:  46% 230/500 [06:43<07:50,  1.74s/it]\u001b[A\n",
      "Iteration:  46% 231/500 [06:45<07:47,  1.74s/it]\u001b[A\n",
      "Iteration:  46% 232/500 [06:47<07:44,  1.73s/it]\u001b[A\n",
      "Iteration:  47% 233/500 [06:49<07:43,  1.74s/it]\u001b[A\n",
      "Iteration:  47% 234/500 [06:50<07:44,  1.75s/it]\u001b[A\n",
      "Iteration:  47% 235/500 [06:52<07:43,  1.75s/it]\u001b[A\n",
      "Iteration:  47% 236/500 [06:54<07:41,  1.75s/it]\u001b[A\n",
      "Iteration:  47% 237/500 [06:56<07:41,  1.75s/it]\u001b[A\n",
      "Iteration:  48% 238/500 [06:57<07:40,  1.76s/it]\u001b[A\n",
      "Iteration:  48% 239/500 [06:59<07:37,  1.75s/it]\u001b[A\n",
      "Iteration:  48% 240/500 [07:01<07:38,  1.76s/it]\u001b[A\n",
      "Iteration:  48% 241/500 [07:03<07:37,  1.77s/it]\u001b[A\n",
      "Iteration:  48% 242/500 [07:04<07:34,  1.76s/it]\u001b[A\n",
      "Iteration:  49% 243/500 [07:06<07:35,  1.77s/it]\u001b[A\n",
      "Iteration:  49% 244/500 [07:08<07:31,  1.76s/it]\u001b[A\n",
      "Iteration:  49% 245/500 [07:10<07:26,  1.75s/it]\u001b[A\n",
      "Iteration:  49% 246/500 [07:11<07:26,  1.76s/it]\u001b[A\n",
      "Iteration:  49% 247/500 [07:13<07:24,  1.76s/it]\u001b[A\n",
      "Iteration:  50% 248/500 [07:15<07:22,  1.76s/it]\u001b[A\n",
      "Iteration:  50% 249/500 [07:17<07:23,  1.77s/it]\u001b[A{\"learning_rate\": 2.5e-05, \"loss\": 0.6454505825042725, \"step\": 750}\n",
      "\n",
      "Iteration:  50% 250/500 [07:19<07:21,  1.76s/it]\u001b[A\n",
      "Iteration:  50% 251/500 [07:20<07:14,  1.75s/it]\u001b[A\n",
      "Iteration:  50% 252/500 [07:22<07:11,  1.74s/it]\u001b[A\n",
      "Iteration:  51% 253/500 [07:24<07:09,  1.74s/it]\u001b[A\n",
      "Iteration:  51% 254/500 [07:25<07:06,  1.73s/it]\u001b[A\n",
      "Iteration:  51% 255/500 [07:27<07:06,  1.74s/it]\u001b[A\n",
      "Iteration:  51% 256/500 [07:29<07:07,  1.75s/it]\u001b[A\n",
      "Iteration:  51% 257/500 [07:31<07:07,  1.76s/it]\u001b[A\n",
      "Iteration:  52% 258/500 [07:32<07:05,  1.76s/it]\u001b[A\n",
      "Iteration:  52% 259/500 [07:34<07:03,  1.76s/it]\u001b[A\n",
      "Iteration:  52% 260/500 [07:36<07:01,  1.75s/it]\u001b[A\n",
      "Iteration:  52% 261/500 [07:38<06:58,  1.75s/it]\u001b[A\n",
      "Iteration:  52% 262/500 [07:39<06:58,  1.76s/it]\u001b[A\n",
      "Iteration:  53% 263/500 [07:41<06:58,  1.77s/it]\u001b[A\n",
      "Iteration:  53% 264/500 [07:43<06:56,  1.77s/it]\u001b[A\n",
      "Iteration:  53% 265/500 [07:45<06:55,  1.77s/it]\u001b[A\n",
      "Iteration:  53% 266/500 [07:47<06:53,  1.77s/it]\u001b[A\n",
      "Iteration:  53% 267/500 [07:48<06:50,  1.76s/it]\u001b[A\n",
      "Iteration:  54% 268/500 [07:50<06:48,  1.76s/it]\u001b[A\n",
      "Iteration:  54% 269/500 [07:52<06:47,  1.77s/it]\u001b[A\n",
      "Iteration:  54% 270/500 [07:54<06:45,  1.76s/it]\u001b[A\n",
      "Iteration:  54% 271/500 [07:55<06:43,  1.76s/it]\u001b[A\n",
      "Iteration:  54% 272/500 [07:57<06:38,  1.75s/it]\u001b[A\n",
      "Iteration:  55% 273/500 [07:59<06:37,  1.75s/it]\u001b[A\n",
      "Iteration:  55% 274/500 [08:01<06:33,  1.74s/it]\u001b[A\n",
      "Iteration:  55% 275/500 [08:02<06:35,  1.76s/it]\u001b[A\n",
      "Iteration:  55% 276/500 [08:04<06:34,  1.76s/it]\u001b[A\n",
      "Iteration:  55% 277/500 [08:06<06:33,  1.77s/it]\u001b[A\n",
      "Iteration:  56% 278/500 [08:08<06:29,  1.76s/it]\u001b[A\n",
      "Iteration:  56% 279/500 [08:09<06:27,  1.76s/it]\u001b[A\n",
      "Iteration:  56% 280/500 [08:11<06:27,  1.76s/it]\u001b[A\n",
      "Iteration:  56% 281/500 [08:13<06:26,  1.77s/it]\u001b[A\n",
      "Iteration:  56% 282/500 [08:15<06:25,  1.77s/it]\u001b[A\n",
      "Iteration:  57% 283/500 [08:16<06:22,  1.76s/it]\u001b[A\n",
      "Iteration:  57% 284/500 [08:18<06:20,  1.76s/it]\u001b[A\n",
      "Iteration:  57% 285/500 [08:20<06:17,  1.75s/it]\u001b[A\n",
      "Iteration:  57% 286/500 [08:22<06:15,  1.76s/it]\u001b[A\n",
      "Iteration:  57% 287/500 [08:23<06:13,  1.75s/it]\u001b[A\n",
      "Iteration:  58% 288/500 [08:25<06:13,  1.76s/it]\u001b[A\n",
      "Iteration:  58% 289/500 [08:27<06:12,  1.77s/it]\u001b[A\n",
      "Iteration:  58% 290/500 [08:29<06:11,  1.77s/it]\u001b[A\n",
      "Iteration:  58% 291/500 [08:31<06:08,  1.76s/it]\u001b[A\n",
      "Iteration:  58% 292/500 [08:32<06:07,  1.77s/it]\u001b[A\n",
      "Iteration:  59% 293/500 [08:34<06:05,  1.76s/it]\u001b[A\n",
      "Iteration:  59% 294/500 [08:36<06:02,  1.76s/it]\u001b[A\n",
      "Iteration:  59% 295/500 [08:38<06:01,  1.76s/it]\u001b[A\n",
      "Iteration:  59% 296/500 [08:39<05:58,  1.76s/it]\u001b[A\n",
      "Iteration:  59% 297/500 [08:41<05:57,  1.76s/it]\u001b[A\n",
      "Iteration:  60% 298/500 [08:43<05:55,  1.76s/it]\u001b[A\n",
      "Iteration:  60% 299/500 [08:45<05:55,  1.77s/it]\u001b[A{\"learning_rate\": 2e-05, \"loss\": 0.692234605550766, \"step\": 800}\n",
      "\n",
      "Iteration:  60% 300/500 [08:46<05:53,  1.77s/it]\u001b[A\n",
      "Iteration:  60% 301/500 [08:48<05:49,  1.76s/it]\u001b[A\n",
      "Iteration:  60% 302/500 [08:50<05:48,  1.76s/it]\u001b[A\n",
      "Iteration:  61% 303/500 [08:52<05:44,  1.75s/it]\u001b[A\n",
      "Iteration:  61% 304/500 [08:53<05:44,  1.76s/it]\u001b[A\n",
      "Iteration:  61% 305/500 [08:55<05:44,  1.77s/it]\u001b[A\n",
      "Iteration:  61% 306/500 [08:57<05:38,  1.75s/it]\u001b[A\n",
      "Iteration:  61% 307/500 [08:59<05:38,  1.75s/it]\u001b[A\n",
      "Iteration:  62% 308/500 [09:00<05:35,  1.75s/it]\u001b[A\n",
      "Iteration:  62% 309/500 [09:02<05:34,  1.75s/it]\u001b[A\n",
      "Iteration:  62% 310/500 [09:04<05:35,  1.77s/it]\u001b[A\n",
      "Iteration:  62% 311/500 [09:06<05:31,  1.75s/it]\u001b[A\n",
      "Iteration:  62% 312/500 [09:08<05:32,  1.77s/it]\u001b[A\n",
      "Iteration:  63% 313/500 [09:09<05:29,  1.76s/it]\u001b[A\n",
      "Iteration:  63% 314/500 [09:11<05:29,  1.77s/it]\u001b[A\n",
      "Iteration:  63% 315/500 [09:13<05:29,  1.78s/it]\u001b[A\n",
      "Iteration:  63% 316/500 [09:15<05:27,  1.78s/it]\u001b[A\n",
      "Iteration:  63% 317/500 [09:16<05:22,  1.76s/it]\u001b[A\n",
      "Iteration:  64% 318/500 [09:18<05:20,  1.76s/it]\u001b[A\n",
      "Iteration:  64% 319/500 [09:20<05:19,  1.77s/it]\u001b[A\n",
      "Iteration:  64% 320/500 [09:22<05:17,  1.76s/it]\u001b[A\n",
      "Iteration:  64% 321/500 [09:23<05:14,  1.76s/it]\u001b[A\n",
      "Iteration:  64% 322/500 [09:25<05:14,  1.77s/it]\u001b[A\n",
      "Iteration:  65% 323/500 [09:27<05:11,  1.76s/it]\u001b[A\n",
      "Iteration:  65% 324/500 [09:29<05:09,  1.76s/it]\u001b[A\n",
      "Iteration:  65% 325/500 [09:30<05:08,  1.77s/it]\u001b[A\n",
      "Iteration:  65% 326/500 [09:32<05:08,  1.77s/it]\u001b[A\n",
      "Iteration:  65% 327/500 [09:34<05:09,  1.79s/it]\u001b[A\n",
      "Iteration:  66% 328/500 [09:36<05:06,  1.78s/it]\u001b[A\n",
      "Iteration:  66% 329/500 [09:38<05:02,  1.77s/it]\u001b[A\n",
      "Iteration:  66% 330/500 [09:39<05:00,  1.77s/it]\u001b[A\n",
      "Iteration:  66% 331/500 [09:41<04:59,  1.77s/it]\u001b[A\n",
      "Iteration:  66% 332/500 [09:43<04:57,  1.77s/it]\u001b[A\n",
      "Iteration:  67% 333/500 [09:45<04:54,  1.76s/it]\u001b[A\n",
      "Iteration:  67% 334/500 [09:46<04:52,  1.76s/it]\u001b[A\n",
      "Iteration:  67% 335/500 [09:48<04:50,  1.76s/it]\u001b[A\n",
      "Iteration:  67% 336/500 [09:50<04:48,  1.76s/it]\u001b[A\n",
      "Iteration:  67% 337/500 [09:52<04:47,  1.76s/it]\u001b[A\n",
      "Iteration:  68% 338/500 [09:53<04:44,  1.75s/it]\u001b[A\n",
      "Iteration:  68% 339/500 [09:55<04:43,  1.76s/it]\u001b[A\n",
      "Iteration:  68% 340/500 [09:57<04:40,  1.76s/it]\u001b[A\n",
      "Iteration:  68% 341/500 [09:59<04:39,  1.76s/it]\u001b[A\n",
      "Iteration:  68% 342/500 [10:00<04:39,  1.77s/it]\u001b[A\n",
      "Iteration:  69% 343/500 [10:02<04:37,  1.77s/it]\u001b[A\n",
      "Iteration:  69% 344/500 [10:04<04:37,  1.78s/it]\u001b[A\n",
      "Iteration:  69% 345/500 [10:06<04:34,  1.77s/it]\u001b[A\n",
      "Iteration:  69% 346/500 [10:08<04:32,  1.77s/it]\u001b[A\n",
      "Iteration:  69% 347/500 [10:09<04:31,  1.77s/it]\u001b[A\n",
      "Iteration:  70% 348/500 [10:11<04:28,  1.76s/it]\u001b[A\n",
      "Iteration:  70% 349/500 [10:13<04:24,  1.75s/it]\u001b[A{\"learning_rate\": 1.5e-05, \"loss\": 0.6490297174453735, \"step\": 850}\n",
      "\n",
      "Iteration:  70% 350/500 [10:15<04:23,  1.76s/it]\u001b[A\n",
      "Iteration:  70% 351/500 [10:16<04:21,  1.75s/it]\u001b[A\n",
      "Iteration:  70% 352/500 [10:18<04:19,  1.75s/it]\u001b[A\n",
      "Iteration:  71% 353/500 [10:20<04:16,  1.75s/it]\u001b[A\n",
      "Iteration:  71% 354/500 [10:22<04:17,  1.76s/it]\u001b[A\n",
      "Iteration:  71% 355/500 [10:23<04:14,  1.76s/it]\u001b[A\n",
      "Iteration:  71% 356/500 [10:25<04:12,  1.75s/it]\u001b[A\n",
      "Iteration:  71% 357/500 [10:27<04:09,  1.75s/it]\u001b[A\n",
      "Iteration:  72% 358/500 [10:29<04:08,  1.75s/it]\u001b[A\n",
      "Iteration:  72% 359/500 [10:30<04:06,  1.75s/it]\u001b[A\n",
      "Iteration:  72% 360/500 [10:32<04:05,  1.75s/it]\u001b[A\n",
      "Iteration:  72% 361/500 [10:34<04:02,  1.75s/it]\u001b[A\n",
      "Iteration:  72% 362/500 [10:36<04:00,  1.74s/it]\u001b[A\n",
      "Iteration:  73% 363/500 [10:37<03:58,  1.74s/it]\u001b[A\n",
      "Iteration:  73% 364/500 [10:39<03:56,  1.74s/it]\u001b[A\n",
      "Iteration:  73% 365/500 [10:41<03:55,  1.75s/it]\u001b[A\n",
      "Iteration:  73% 366/500 [10:43<03:54,  1.75s/it]\u001b[A\n",
      "Iteration:  73% 367/500 [10:44<03:52,  1.75s/it]\u001b[A\n",
      "Iteration:  74% 368/500 [10:46<03:50,  1.74s/it]\u001b[A\n",
      "Iteration:  74% 369/500 [10:48<03:48,  1.75s/it]\u001b[A\n",
      "Iteration:  74% 370/500 [10:50<03:47,  1.75s/it]\u001b[A\n",
      "Iteration:  74% 371/500 [10:51<03:46,  1.76s/it]\u001b[A\n",
      "Iteration:  74% 372/500 [10:53<03:44,  1.76s/it]\u001b[A\n",
      "Iteration:  75% 373/500 [10:55<03:44,  1.76s/it]\u001b[A\n",
      "Iteration:  75% 374/500 [10:57<03:41,  1.75s/it]\u001b[A\n",
      "Iteration:  75% 375/500 [10:58<03:38,  1.75s/it]\u001b[A\n",
      "Iteration:  75% 376/500 [11:00<03:37,  1.76s/it]\u001b[A\n",
      "Iteration:  75% 377/500 [11:02<03:36,  1.76s/it]\u001b[A\n",
      "Iteration:  76% 378/500 [11:04<03:34,  1.76s/it]\u001b[A\n",
      "Iteration:  76% 379/500 [11:05<03:32,  1.76s/it]\u001b[A\n",
      "Iteration:  76% 380/500 [11:07<03:30,  1.76s/it]\u001b[A\n",
      "Iteration:  76% 381/500 [11:09<03:28,  1.75s/it]\u001b[A\n",
      "Iteration:  76% 382/500 [11:11<03:28,  1.76s/it]\u001b[A\n",
      "Iteration:  77% 383/500 [11:12<03:25,  1.76s/it]\u001b[A\n",
      "Iteration:  77% 384/500 [11:14<03:22,  1.74s/it]\u001b[A\n",
      "Iteration:  77% 385/500 [11:16<03:21,  1.75s/it]\u001b[A\n",
      "Iteration:  77% 386/500 [11:18<03:19,  1.75s/it]\u001b[A\n",
      "Iteration:  77% 387/500 [11:19<03:19,  1.77s/it]\u001b[A\n",
      "Iteration:  78% 388/500 [11:21<03:17,  1.76s/it]\u001b[A\n",
      "Iteration:  78% 389/500 [11:23<03:15,  1.76s/it]\u001b[A\n",
      "Iteration:  78% 390/500 [11:25<03:14,  1.77s/it]\u001b[A\n",
      "Iteration:  78% 391/500 [11:27<03:12,  1.77s/it]\u001b[A\n",
      "Iteration:  78% 392/500 [11:28<03:10,  1.77s/it]\u001b[A\n",
      "Iteration:  79% 393/500 [11:30<03:07,  1.76s/it]\u001b[A\n",
      "Iteration:  79% 394/500 [11:32<03:05,  1.75s/it]\u001b[A\n",
      "Iteration:  79% 395/500 [11:34<03:03,  1.75s/it]\u001b[A\n",
      "Iteration:  79% 396/500 [11:35<03:02,  1.75s/it]\u001b[A\n",
      "Iteration:  79% 397/500 [11:37<03:00,  1.75s/it]\u001b[A\n",
      "Iteration:  80% 398/500 [11:39<02:58,  1.75s/it]\u001b[A\n",
      "Iteration:  80% 399/500 [11:40<02:56,  1.75s/it]\u001b[A{\"learning_rate\": 1e-05, \"loss\": 0.5968187394738197, \"step\": 900}\n",
      "\n",
      "Iteration:  80% 400/500 [11:42<02:56,  1.77s/it]\u001b[A\n",
      "Iteration:  80% 401/500 [11:44<02:53,  1.76s/it]\u001b[A\n",
      "Iteration:  80% 402/500 [11:46<02:51,  1.75s/it]\u001b[A\n",
      "Iteration:  81% 403/500 [11:48<02:49,  1.75s/it]\u001b[A\n",
      "Iteration:  81% 404/500 [11:49<02:47,  1.74s/it]\u001b[A\n",
      "Iteration:  81% 405/500 [11:51<02:46,  1.75s/it]\u001b[A\n",
      "Iteration:  81% 406/500 [11:53<02:45,  1.76s/it]\u001b[A\n",
      "Iteration:  81% 407/500 [11:55<02:43,  1.76s/it]\u001b[A\n",
      "Iteration:  82% 408/500 [11:56<02:40,  1.75s/it]\u001b[A\n",
      "Iteration:  82% 409/500 [11:58<02:38,  1.74s/it]\u001b[A\n",
      "Iteration:  82% 410/500 [12:00<02:37,  1.74s/it]\u001b[A\n",
      "Iteration:  82% 411/500 [12:02<02:35,  1.74s/it]\u001b[A\n",
      "Iteration:  82% 412/500 [12:03<02:33,  1.75s/it]\u001b[A\n",
      "Iteration:  83% 413/500 [12:05<02:31,  1.74s/it]\u001b[A\n",
      "Iteration:  83% 414/500 [12:07<02:30,  1.76s/it]\u001b[A\n",
      "Iteration:  83% 415/500 [12:09<02:29,  1.75s/it]\u001b[A\n",
      "Iteration:  83% 416/500 [12:10<02:27,  1.76s/it]\u001b[A\n",
      "Iteration:  83% 417/500 [12:12<02:25,  1.76s/it]\u001b[A\n",
      "Iteration:  84% 418/500 [12:14<02:24,  1.76s/it]\u001b[A\n",
      "Iteration:  84% 419/500 [12:16<02:22,  1.76s/it]\u001b[A\n",
      "Iteration:  84% 420/500 [12:17<02:20,  1.75s/it]\u001b[A\n",
      "Iteration:  84% 421/500 [12:19<02:17,  1.74s/it]\u001b[A\n",
      "Iteration:  84% 422/500 [12:21<02:15,  1.74s/it]\u001b[A\n",
      "Iteration:  85% 423/500 [12:22<02:13,  1.74s/it]\u001b[A\n",
      "Iteration:  85% 424/500 [12:24<02:11,  1.73s/it]\u001b[A\n",
      "Iteration:  85% 425/500 [12:26<02:09,  1.73s/it]\u001b[A\n",
      "Iteration:  85% 426/500 [12:28<02:08,  1.74s/it]\u001b[A\n",
      "Iteration:  85% 427/500 [12:29<02:08,  1.75s/it]\u001b[A\n",
      "Iteration:  86% 428/500 [12:31<02:06,  1.75s/it]\u001b[A\n",
      "Iteration:  86% 429/500 [12:33<02:04,  1.75s/it]\u001b[A\n",
      "Iteration:  86% 430/500 [12:35<02:03,  1.76s/it]\u001b[A\n",
      "Iteration:  86% 431/500 [12:37<02:01,  1.77s/it]\u001b[A\n",
      "Iteration:  86% 432/500 [12:38<02:00,  1.77s/it]\u001b[A\n",
      "Iteration:  87% 433/500 [12:40<01:57,  1.76s/it]\u001b[A\n",
      "Iteration:  87% 434/500 [12:42<01:57,  1.77s/it]\u001b[A\n",
      "Iteration:  87% 435/500 [12:44<01:54,  1.77s/it]\u001b[A\n",
      "Iteration:  87% 436/500 [12:45<01:53,  1.77s/it]\u001b[A\n",
      "Iteration:  87% 437/500 [12:47<01:51,  1.77s/it]\u001b[A\n",
      "Iteration:  88% 438/500 [12:49<01:50,  1.78s/it]\u001b[A\n",
      "Iteration:  88% 439/500 [12:51<01:49,  1.79s/it]\u001b[A\n",
      "Iteration:  88% 440/500 [12:53<01:46,  1.78s/it]\u001b[A\n",
      "Iteration:  88% 441/500 [12:54<01:44,  1.77s/it]\u001b[A\n",
      "Iteration:  88% 442/500 [12:56<01:42,  1.77s/it]\u001b[A\n",
      "Iteration:  89% 443/500 [12:58<01:40,  1.77s/it]\u001b[A\n",
      "Iteration:  89% 444/500 [13:00<01:38,  1.77s/it]\u001b[A\n",
      "Iteration:  89% 445/500 [13:01<01:37,  1.77s/it]\u001b[A\n",
      "Iteration:  89% 446/500 [13:03<01:35,  1.76s/it]\u001b[A\n",
      "Iteration:  89% 447/500 [13:05<01:33,  1.76s/it]\u001b[A\n",
      "Iteration:  90% 448/500 [13:07<01:30,  1.75s/it]\u001b[A\n",
      "Iteration:  90% 449/500 [13:08<01:29,  1.75s/it]\u001b[A{\"learning_rate\": 5e-06, \"loss\": 0.6483889979124069, \"step\": 950}\n",
      "\n",
      "Iteration:  90% 450/500 [13:10<01:27,  1.75s/it]\u001b[A\n",
      "Iteration:  90% 451/500 [13:12<01:26,  1.76s/it]\u001b[A\n",
      "Iteration:  90% 452/500 [13:14<01:24,  1.76s/it]\u001b[A\n",
      "Iteration:  91% 453/500 [13:15<01:22,  1.75s/it]\u001b[A\n",
      "Iteration:  91% 454/500 [13:17<01:20,  1.75s/it]\u001b[A\n",
      "Iteration:  91% 455/500 [13:19<01:18,  1.75s/it]\u001b[A\n",
      "Iteration:  91% 456/500 [13:21<01:17,  1.75s/it]\u001b[A\n",
      "Iteration:  91% 457/500 [13:22<01:15,  1.75s/it]\u001b[A\n",
      "Iteration:  92% 458/500 [13:24<01:13,  1.76s/it]\u001b[A\n",
      "Iteration:  92% 459/500 [13:26<01:11,  1.75s/it]\u001b[A\n",
      "Iteration:  92% 460/500 [13:28<01:09,  1.74s/it]\u001b[A\n",
      "Iteration:  92% 461/500 [13:29<01:08,  1.75s/it]\u001b[A\n",
      "Iteration:  92% 462/500 [13:31<01:06,  1.76s/it]\u001b[A\n",
      "Iteration:  93% 463/500 [13:33<01:05,  1.76s/it]\u001b[A\n",
      "Iteration:  93% 464/500 [13:35<01:03,  1.76s/it]\u001b[A\n",
      "Iteration:  93% 465/500 [13:36<01:01,  1.76s/it]\u001b[A\n",
      "Iteration:  93% 466/500 [13:38<00:59,  1.76s/it]\u001b[A\n",
      "Iteration:  93% 467/500 [13:40<00:58,  1.76s/it]\u001b[A\n",
      "Iteration:  94% 468/500 [13:42<00:56,  1.77s/it]\u001b[A\n",
      "Iteration:  94% 469/500 [13:43<00:54,  1.75s/it]\u001b[A\n",
      "Iteration:  94% 470/500 [13:45<00:52,  1.75s/it]\u001b[A\n",
      "Iteration:  94% 471/500 [13:47<00:50,  1.75s/it]\u001b[A\n",
      "Iteration:  94% 472/500 [13:49<00:48,  1.74s/it]\u001b[A\n",
      "Iteration:  95% 473/500 [13:50<00:47,  1.74s/it]\u001b[A\n",
      "Iteration:  95% 474/500 [13:52<00:45,  1.75s/it]\u001b[A\n",
      "Iteration:  95% 475/500 [13:54<00:43,  1.75s/it]\u001b[A\n",
      "Iteration:  95% 476/500 [13:56<00:41,  1.74s/it]\u001b[A\n",
      "Iteration:  95% 477/500 [13:57<00:40,  1.75s/it]\u001b[A\n",
      "Iteration:  96% 478/500 [13:59<00:38,  1.75s/it]\u001b[A\n",
      "Iteration:  96% 479/500 [14:01<00:36,  1.74s/it]\u001b[A\n",
      "Iteration:  96% 480/500 [14:03<00:34,  1.74s/it]\u001b[A\n",
      "Iteration:  96% 481/500 [14:04<00:33,  1.74s/it]\u001b[A\n",
      "Iteration:  96% 482/500 [14:06<00:31,  1.74s/it]\u001b[A\n",
      "Iteration:  97% 483/500 [14:08<00:29,  1.74s/it]\u001b[A\n",
      "Iteration:  97% 484/500 [14:10<00:27,  1.74s/it]\u001b[A\n",
      "Iteration:  97% 485/500 [14:11<00:26,  1.75s/it]\u001b[A\n",
      "Iteration:  97% 486/500 [14:13<00:24,  1.75s/it]\u001b[A\n",
      "Iteration:  97% 487/500 [14:15<00:22,  1.74s/it]\u001b[A\n",
      "Iteration:  98% 488/500 [14:17<00:20,  1.74s/it]\u001b[A\n",
      "Iteration:  98% 489/500 [14:18<00:19,  1.75s/it]\u001b[A\n",
      "Iteration:  98% 490/500 [14:20<00:17,  1.76s/it]\u001b[A\n",
      "Iteration:  98% 491/500 [14:22<00:15,  1.77s/it]\u001b[A\n",
      "Iteration:  98% 492/500 [14:24<00:14,  1.77s/it]\u001b[A\n",
      "Iteration:  99% 493/500 [14:25<00:12,  1.77s/it]\u001b[A\n",
      "Iteration:  99% 494/500 [14:27<00:10,  1.76s/it]\u001b[A\n",
      "Iteration:  99% 495/500 [14:29<00:08,  1.78s/it]\u001b[A\n",
      "Iteration:  99% 496/500 [14:31<00:07,  1.77s/it]\u001b[A\n",
      "Iteration:  99% 497/500 [14:32<00:05,  1.76s/it]\u001b[A\n",
      "Iteration: 100% 498/500 [14:34<00:03,  1.76s/it]\u001b[A\n",
      "Iteration: 100% 499/500 [14:36<00:01,  1.75s/it]\u001b[A{\"learning_rate\": 0.0, \"loss\": 0.5807230797410011, \"step\": 1000}\n",
      "\n",
      "Iteration: 100% 500/500 [14:38<00:00,  1.76s/it]\n",
      "Epoch: 100% 2/2 [29:02<00:00, 871.48s/it]\n",
      "08/19/2021 14:40:16 - INFO - __main__ -    global_step = 1000, average loss = 0.8733194705024362\n",
      "08/19/2021 14:40:16 - INFO - utils_ichi -   Creating features from dataset file at ./data/ichi_dataset\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   Writing example 0/3000\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   *** Example ***\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   guid: dev-1\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   input_global_ids: 101 17938 19067 1029 1029 1029 7632 2035 999 1045 2572 2047 2182 2021 2031 2042 24261 2005 2070 2051 1012 2026 26319 2024 1024 3287 3429 1061 2869 1010 1044 25465 2828 2028 1010 10372 2220 3770 102 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   input_local_ids: 101 17938 19067 1029 1029 1029 7632 2035 999 1045 2572 2047 2182 2021 2031 2042 24261 2005 2070 2051 1012 2026 26319 2024 1024 3287 3429 1061 2869 1010 1044 25465 2828 2028 1010 10372 2220 3770 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 11144 3070 295 187 2402 1 83 20002 229 8 9752 8302 770 1385 5147 20002 314 1475 1947 270 3149\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   aspect_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   label: DISE (id = 1)\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   *** Example ***\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   guid: dev-2\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   input_global_ids: 101 13408 12436 20876 17962 2026 6513 1998 1045 2074 2288 2083 2383 3348 1998 2014 17962 2024 2200 13408 2054 2071 2023 2022 2013 1029 2054 2064 2057 2079 2000 2131 1996 18348 2091 1029 102 18348 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   input_local_ids: 101 13408 12436 20876 17962 2026 6513 1998 1045 2074 2288 2083 2383 3348 1998 2014 17962 2024 2200 13408 2054 2071 2023 2022 2013 1029 2054 2064 2057 2079 2000 2131 1996 18348 2091 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 382 448 8108 8 825 44 66 8108 382 14 6346 3 581 9411\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   aspect_indices: 18348 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   label: SOCL (id = 6)\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   *** Example ***\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   guid: dev-3\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   input_global_ids: 101 5255 3756 2159 2852 1012 1045 2031 6530 3756 21122 2075 2159 2144 2026 9458 2287 1012 2021 2049 2893 2524 1998 2524 1012 2004 1045 1044 3726 9015 2919 6322 1997 2166 2066 2192 3218 1010 28144 21939 2050 1012 1012 1012 1012 1012 2000 2172 2021 1045 2123 1005 1056 2031 2023 5292 10322 4183 2085 1998 28144 21939 2050 2003 2036 2031 2908 1012 2021 2026 2159 2024 3756 1012 1998 1045 2371 2009 2062 2919 4650 2012 2026 2305 2991 2051 1012 2043 1045 2131 2305 2991 2059 2023 3663 2131 2172 2062 2021 2044 1016 2420 2043 1045 2202 2300 2005 5948 2260 2000 2403 3221 2566 2154 2059 2009 2089 2022 5547 1012 1012 1012 1012 1012 1012 1012 1012 1012 20228 2480 12367 2033 2055 2023 4078 19500 5685 3949 1012 2097 2022 2428 18836 2000 2017 102 2159 102 4650 102 21122 2075 2159 102 2287 102 2192 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   input_local_ids: 101 5255 3756 2159 2852 1012 1045 2031 6530 3756 21122 2075 2159 2144 2026 9458 2287 1012 2021 2049 2893 2524 1998 2524 1012 2004 1045 1044 3726 9015 2919 6322 1997 2166 2066 2192 3218 1010 28144 21939 2050 1012 1012 1012 1012 1012 2000 2172 2021 1045 2123 1005 1056 2031 2023 5292 10322 4183 2085 1998 28144 21939 2050 2003 2036 2031 2908 1012 2021 2026 2159 2024 3756 1012 1998 1045 2371 2009 2062 2919 4650 2012 2026 2305 2991 2051 1012 2043 1045 2131 2305 2991 2059 2023 3663 2131 2172 2062 2021 2044 1016 2420 2043 1045 2202 2300 2005 5948 2260 2000 2403 3221 2566 2154 2059 2009 2089 2022 5547 1012 1012 1012 1012 1012 1012 1012 1012 1012 20228 2480 12367 2033 2055 2023 4078 19500 5685 3949 1012 2097 2022 2428 18836 2000 2017 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 642 1620 32 378 3473 1620 20002 32 36 1994 1845 65 149 20002 20002 1032 102 371 159 4 272 20002 57 20002 20002 17 2337 32 20002 175 102 767 8 120 650 20002 3 120 650 643 3 57 28 6 34 517 865 418 686 736 713 6 90 20002 1536 5366 20002 20002 25 4198\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   aspect_indices: 2159 4650 21122 2075 2159 2287 2192 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   label: GOAL (id = 3)\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   *** Example ***\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   guid: dev-4\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   input_global_ids: 101 4319 3231 1998 1059 2015 7632 1010 1037 2261 6385 3283 1045 2253 2000 1037 5637 3348 20464 12083 1998 5117 1999 5949 2869 25378 1012 1045 10749 21392 2013 1037 3232 1997 4364 1012 1999 1037 2261 2420 1045 2707 1037 2047 3105 1012 2065 1045 2131 1037 4319 3231 1010 2003 2009 2825 2009 2097 2265 2039 3893 2065 1996 2060 4364 2018 2579 1999 5850 1012 1045 2123 1005 1056 2079 5850 1010 2196 2031 1012 102 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   input_local_ids: 101 4319 3231 1998 1059 2015 7632 1010 1037 2261 6385 3283 1045 2253 2000 1037 5637 3348 20464 12083 1998 5117 1999 5949 2869 25378 1012 1045 10749 21392 2013 1037 3232 1997 4364 1012 1999 1037 2261 2420 1045 2707 1037 2047 3105 1012 2065 1045 2131 1037 4319 3231 1010 2003 2009 2825 2009 2097 2265 2039 3893 2065 1996 2060 4364 2018 2579 1999 5850 1012 1045 2123 1005 1056 2079 5850 1010 2196 2031 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 431 56 2874 561 120 81 1 49 1322 20002 4791 20002 1 2658 759 200 9027 165 6 1 116 83 3720 137 1 3 431 1830 215 264 419 361 212 4064 1 3960 54 2102\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   aspect_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   label: SOCL (id = 6)\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   *** Example ***\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   guid: dev-5\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   input_global_ids: 101 1018 2095 2214 2003 2041 1997 2491 2026 1018 2095 2214 2003 1037 10103 1012 2033 1998 2026 3129 2147 2440 2051 1010 2061 1999 1996 2851 2002 2038 2000 2175 2000 2082 1012 2002 2196 4122 2000 2131 5102 1010 2057 2954 2007 2032 2296 2851 1998 2009 2003 2061 2524 2000 2131 2000 2147 2006 2051 2296 2154 999 1996 2851 4627 2007 7491 16142 1998 2049 9643 1012 2002 2097 2707 15209 4303 1010 6886 2477 1012 2002 3504 2005 2477 2000 5466 1012 2002 7807 2673 999 2057 2031 2000 26470 2032 2007 10899 2000 2175 2000 2082 1998 2008 4510 2573 4902 1012 2002 2003 2074 2061 14205 1012 2002 2074 6732 2002 2003 1037 2332 1998 3071 2323 6812 2000 2032 1012 2002 2003 2066 1037 28561 2051 5968 1012 2002 2064 2022 2204 2059 2074 10245 1999 2019 6013 1012 1045 2228 2002 2038 5177 3314 1012 2002 4152 2061 2919 2008 2026 3129 2038 2000 2907 2032 2091 2096 2002 2003 6886 2010 9092 24456 2015 1012 2023 2038 2042 2183 2006 2005 2058 1037 2095 2085 1012 2002 2003 2074 1037 3697 2775 1010 2200 9694 1012 3071 2758 2002 2003 1037 3671 1018 2095 2214 2021 1045 11693 2000 11234 1012 3531 2393 999 2130 2026 3129 2758 2002 2003 3671 2005 2010 2287 1012 2057 2024 2061 13233 2035 1996 2051 2008 2057 2064 2102 3524 2005 2032 2000 102 7491 102 2035 102 9694 102 16142 102 10103 102 2287 102 2214 102\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   input_local_ids: 101 1018 2095 2214 2003 2041 1997 2491 2026 1018 2095 2214 2003 1037 10103 1012 2033 1998 2026 3129 2147 2440 2051 1010 2061 1999 1996 2851 2002 2038 2000 2175 2000 2082 1012 2002 2196 4122 2000 2131 5102 1010 2057 2954 2007 2032 2296 2851 1998 2009 2003 2061 2524 2000 2131 2000 2147 2006 2051 2296 2154 999 1996 2851 4627 2007 7491 16142 1998 2049 9643 1012 2002 2097 2707 15209 4303 1010 6886 2477 1012 2002 3504 2005 2477 2000 5466 1012 2002 7807 2673 999 2057 2031 2000 26470 2032 2007 10899 2000 2175 2000 2082 1998 2008 4510 2573 4902 1012 2002 2003 2074 2061 14205 1012 2002 2074 6732 2002 2003 1037 2332 1998 3071 2323 6812 2000 2032 1012 2002 2003 2066 1037 28561 2051 5968 1012 2002 2064 2022 2204 2059 2074 10245 1999 2019 6013 1012 1045 2228 2002 2038 5177 3314 1012 2002 4152 2061 2919 2008 2026 3129 2038 2000 2907 2032 2091 2096 2002 2003 6886 2010 9092 24456 2015 1012 2023 2038 2042 2183 2006 2005 2058 1037 2095 2085 1012 2002 2003 2074 1037 3697 2775 1010 2200 9694 1012 3071 2758 2002 2003 1037 3671 1018 2095 2214 2021 1045 11693 2000 11234 1012 3531 2393 999 2130 2026 3129 2758 2002 2003 3671 2005 2010 2287 1012 2057 2024 2061 13233 2035 1996 2051 2008 2057 2064 2102 3524 2005 2032 2000 4982 2039 999 999 999 999 102 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 71 2 13 247 71 2 13 10162 135 92 407 342 203 16 644 54 20 3 20002 1128 78 203 149 3 92 9 78 5814 203 116 1203 959 9500 116 9578 11421 1218 1047 106 47 20002 622 5100 20002 917 16 115 1252 92 941 16013 33 7012 387 12183 236 4 20002 9 20002 76 5850 20002 33 1090 1298 3 102 135 684 1218 8172 37 2 262 693 1564 20002 387 48 72 71 2 13 10461 20002 61 345 39 135 48 72 1845 1356 9 255 359 898 5478\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   aspect_indices: 7491 2035 9694 16142 10103 2287 2214 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/19/2021 14:40:56 - INFO - utils_ichi -   label: FAML (id = 5)\n",
      "08/19/2021 14:41:15 - INFO - utils_ichi -   Saving features into cached file ./data/ichi_dataset/cached_dev_bert-base-uncased_256_ichi\n",
      "08/19/2021 14:41:18 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "08/19/2021 14:41:18 - INFO - __main__ -     Num examples = 3000\n",
      "08/19/2021 14:41:18 - INFO - __main__ -     Batch size = 16\n",
      "Evaluating: 100% 188/188 [01:58<00:00,  1.59it/s]\n",
      "08/19/2021 14:43:16 - INFO - __main__ -   ***** Eval results  *****\n",
      "08/19/2021 14:43:16 - INFO - __main__ -     acc = 0.705\n",
      "08/19/2021 14:43:16 - INFO - __main__ -     f1 = 0.7024932870624285\n",
      "08/19/2021 14:43:16 - INFO - __main__ -   Saving model checkpoint to ./tmp/ichi_bert_base_new\n",
      "08/19/2021 14:43:20 - INFO - transformers.tokenization_utils -   Model name './tmp/ichi_bert_base_new' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming './tmp/ichi_bert_base_new' is a path or url to a directory containing tokenizer files.\n",
      "08/19/2021 14:43:20 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/vocab.txt\n",
      "08/19/2021 14:43:20 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/added_tokens.json\n",
      "08/19/2021 14:43:20 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/special_tokens_map.json\n",
      "08/19/2021 14:43:20 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/tokenizer_config.json\n",
      "08/19/2021 14:43:20 - INFO - transformers.tokenization_utils -   Model name './tmp/ichi_bert_base_new' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming './tmp/ichi_bert_base_new' is a path or url to a directory containing tokenizer files.\n",
      "08/19/2021 14:43:20 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/vocab.txt\n",
      "08/19/2021 14:43:20 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/added_tokens.json\n",
      "08/19/2021 14:43:20 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/special_tokens_map.json\n",
      "08/19/2021 14:43:20 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/tokenizer_config.json\n",
      "loading tokenizer from cache: ./data/ichi_dataset/cachedtokenizer_bert-base-uncased_glove_ichi\n",
      "08/19/2021 14:43:20 - INFO - __main__ -   Evaluate the following checkpoints: ['./tmp/ichi_bert_base_new']\n",
      "08/19/2021 14:43:23 - INFO - utils_ichi -   Loading features from cached file ./data/ichi_dataset/cached_dev_bert-base-uncased_256_ichi\n",
      "loading tokenizer from cache: ./data/ichi_dataset/cachedtokenizer_bert-base-uncased_glove_ichi\n",
      "08/19/2021 14:43:24 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "08/19/2021 14:43:24 - INFO - __main__ -     Num examples = 3000\n",
      "08/19/2021 14:43:24 - INFO - __main__ -     Batch size = 16\n",
      "Evaluating: 100% 188/188 [01:57<00:00,  1.60it/s]\n",
      "08/19/2021 14:45:22 - INFO - __main__ -   ***** Eval results  *****\n",
      "08/19/2021 14:45:22 - INFO - __main__ -     acc = 0.705\n",
      "08/19/2021 14:45:22 - INFO - __main__ -     f1 = 0.7024932870624285\n"
     ]
    }
   ],
   "source": [
    "!python ./examples/ichi/run_ichi.py --model_type bert --model_name_or_path bert-base-uncased --do_lower_case --do_train --do_eval --data_dir ./data/ichi_dataset --max_seq_length 256 --per_gpu_eval_batch_size=16 --per_gpu_train_batch_size=16 --learning_rate 1e-4 --num_train_epochs 2 --output_dir ./tmp/ichi_bert_base_new --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Hp_tPyIKVYw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of ICHI Ablation best Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
