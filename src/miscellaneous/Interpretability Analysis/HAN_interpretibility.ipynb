{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAN_interpretibility.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMr2U03wHJTe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "143828e5-11d0-4746-ea4f-01acae041746"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "#No need to install you can just specify version\n",
        "#pip install tensorflow==1.15"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rhliNKrI_jC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/project/ml_models')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxHqf1tjJMYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "df_train = pd.read_csv('train.tsv',sep='\\t')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNz4vLQaJPZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_csv('dev.tsv',sep='\\t')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOUgNZ4RJUcd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "5ac71820-8be0-428e-9111-d93682ae8fa4"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ids</th>\n",
              "      <th>body</th>\n",
              "      <th>Uncertainity of Post_Diagnosis</th>\n",
              "      <th>Results and Side-Effects Observed</th>\n",
              "      <th>Medical Assistance</th>\n",
              "      <th>Diet and Maintenance</th>\n",
              "      <th>Information Source</th>\n",
              "      <th>Concepts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>LIPITOR.449.txt</td>\n",
              "      <td>Extreme tiredness and flatulence. Not sure whe...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>flatulence|exhaustion|tired|Not sure|Extreme|t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>LIPITOR.188.txt</td>\n",
              "      <td>1/7/05-continued. not all of it posted before....</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>package insert|depression|Lipitor|cholesterol|...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>LIPITOR.541.txt</td>\n",
              "      <td>So sad to see so many with problems like mine!...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>muscle pain|joint pain|depression|Lipitor|Lipi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>LIPITOR.810.txt</td>\n",
              "      <td>Within 1 month time developed severe depressio...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>severe depression|headaches|Lipitor|statins|Li...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>LIPITOR.393.txt</td>\n",
              "      <td>I have been on lipitor for 10 years for heart ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>leg weakness|changed|experience|Potassium|cram...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                           Concepts\n",
              "0           0  ...  flatulence|exhaustion|tired|Not sure|Extreme|t...\n",
              "1           1  ...  package insert|depression|Lipitor|cholesterol|...\n",
              "2           2  ...  muscle pain|joint pain|depression|Lipitor|Lipi...\n",
              "3           3  ...  severe depression|headaches|Lipitor|statins|Li...\n",
              "4           4  ...  leg weakness|changed|experience|Potassium|cram...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NaPcgkeJWs6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "d2900020-91da-4bf9-e359-3af70bffeb19"
      },
      "source": [
        "df_test.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ids</th>\n",
              "      <th>body</th>\n",
              "      <th>Uncertainity of Post_Diagnosis</th>\n",
              "      <th>Results and Side-Effects Observed</th>\n",
              "      <th>Medical Assistance</th>\n",
              "      <th>Diet and Maintenance</th>\n",
              "      <th>Information Source</th>\n",
              "      <th>Concepts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>LIPITOR.595.txt</td>\n",
              "      <td>Swelling left arm, very severe itching, intole...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Swelling|itching|left arm|hand|dreams|bruise|v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>ARTHROTEC.101.txt</td>\n",
              "      <td>1st pill taken with food, a few hours after i ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>side effects|depression|cramping|upset|experie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>LIPITOR.367.txt</td>\n",
              "      <td>episode of intense dizziness lasting nearly an...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>dizziness|lassitude|chills|shivers|problem|wor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>LIPITOR.74.txt</td>\n",
              "      <td>After taking Crestor and having muscle pain an...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>liver problems|decreased|itchy|control|crawly|...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>LIPITOR.389.txt</td>\n",
              "      <td>75 yo mother-in-law has memory loss, hair loss...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>lack of appetite|hair loss|stroke|sciatica|los...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                           Concepts\n",
              "0           0  ...  Swelling|itching|left arm|hand|dreams|bruise|v...\n",
              "1           1  ...  side effects|depression|cramping|upset|experie...\n",
              "2           2  ...  dizziness|lassitude|chills|shivers|problem|wor...\n",
              "3           3  ...  liver problems|decreased|itchy|control|crawly|...\n",
              "4           4  ...  lack of appetite|hair loss|stroke|sciatica|los...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygj5c3liJY-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_body =df_train['body'].to_list()\n",
        "test_body =df_test['body'].to_list()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UslXNWT5JbG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"can not \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
        "    text = re.sub('\\W', ' ', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = text.strip(' ')\n",
        "    return text\n",
        "\n",
        "for idx,text in enumerate(train_body):\n",
        "  train_body[idx] = clean_text(text)\n",
        "\n",
        "for idx,text in enumerate(test_body):\n",
        "  test_body[idx] = clean_text(text)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJhmFiW8JdJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "str_total = ' '.join(train_body) + ' '.join(test_body)\n",
        "list_total = str_total.split()\n",
        "dict_count = Counter(list_total)\n",
        "\n",
        "vocab = dict()\n",
        "#vocab['unk']=0\n",
        "for key,val in dict_count.items():\n",
        "  vocab[key] = len(vocab)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icRmMGf6Jfmp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "28c998da-a4cf-4010-8797-a558c6378220"
      },
      "source": [
        "print(len(vocab))\n",
        "vocab"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6640\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'extreme': 0,\n",
              " 'tiredness': 1,\n",
              " 'and': 2,\n",
              " 'flatulence': 3,\n",
              " 'not': 4,\n",
              " 'sure': 5,\n",
              " 'whether': 6,\n",
              " 'it': 7,\n",
              " 'is': 8,\n",
              " 'the': 9,\n",
              " 'lipitor': 10,\n",
              " 'but': 11,\n",
              " 'i': 12,\n",
              " 'am': 13,\n",
              " 'now': 14,\n",
              " 'tired': 15,\n",
              " 'to': 16,\n",
              " 'point': 17,\n",
              " 'of': 18,\n",
              " 'exhaustion': 19,\n",
              " 'work': 20,\n",
              " 'me': 21,\n",
              " 'getting': 22,\n",
              " 'older': 23,\n",
              " 'or': 24,\n",
              " '1': 25,\n",
              " '7': 26,\n",
              " '05': 27,\n",
              " 'continued': 28,\n",
              " 'all': 29,\n",
              " 'posted': 30,\n",
              " 'before': 31,\n",
              " 'here': 32,\n",
              " 'continuation': 33,\n",
              " 'never': 34,\n",
              " 'filled': 35,\n",
              " 'as': 36,\n",
              " 'my': 37,\n",
              " 'instincts': 38,\n",
              " 'told': 39,\n",
              " 'was': 40,\n",
              " 'something': 41,\n",
              " 'more': 42,\n",
              " 'generally': 43,\n",
              " 'a': 44,\n",
              " 'well': 45,\n",
              " 'balanced': 46,\n",
              " 'type': 47,\n",
              " 'person': 48,\n",
              " 'depression': 49,\n",
              " 'did': 50,\n",
              " 'come': 51,\n",
              " 'on': 52,\n",
              " 'rather': 53,\n",
              " 'suddenly': 54,\n",
              " 'about': 55,\n",
              " 'same': 56,\n",
              " 'time': 57,\n",
              " 'when': 58,\n",
              " 'started': 59,\n",
              " 'advice': 60,\n",
              " 'you': 61,\n",
              " 'would': 62,\n",
              " 'be': 63,\n",
              " 'listen': 64,\n",
              " 'your': 65,\n",
              " 'body': 66,\n",
              " 'may': 67,\n",
              " 'trying': 68,\n",
              " 'tell': 69,\n",
              " 'try': 70,\n",
              " 'diet': 71,\n",
              " 'excercise': 72,\n",
              " 'lower': 73,\n",
              " 'cholesterol': 74,\n",
              " 'this': 75,\n",
              " 'pfizer': 76,\n",
              " 'recommendation': 77,\n",
              " 'also': 78,\n",
              " 'see': 79,\n",
              " 'their': 80,\n",
              " 'package': 81,\n",
              " 'insert': 82,\n",
              " 'sorry': 83,\n",
              " 'one': 84,\n",
              " 'gravy': 85,\n",
              " 'train': 86,\n",
              " 'are': 87,\n",
              " 'everyone': 88,\n",
              " 'so': 89,\n",
              " 'sad': 90,\n",
              " 'many': 91,\n",
              " 'with': 92,\n",
              " 'problems': 93,\n",
              " 'like': 94,\n",
              " 'mine': 95,\n",
              " 'progressively': 96,\n",
              " 'worsening': 97,\n",
              " 'joint': 98,\n",
              " 'pain': 99,\n",
              " 'muscle': 100,\n",
              " 'doctor': 101,\n",
              " 'has': 102,\n",
              " 'ran': 103,\n",
              " 'sorts': 104,\n",
              " 'tests': 105,\n",
              " 'oculd': 106,\n",
              " 'find': 107,\n",
              " 'anything': 108,\n",
              " 'wrong': 109,\n",
              " 'pointing': 110,\n",
              " 'arthristis': 111,\n",
              " 'stopped': 112,\n",
              " 'taking': 113,\n",
              " 'went': 114,\n",
              " 'away': 115,\n",
              " 'after': 116,\n",
              " '2': 117,\n",
              " 'days': 118,\n",
              " 'going': 119,\n",
              " 'back': 120,\n",
              " 'within': 121,\n",
              " 'month': 122,\n",
              " 'developed': 123,\n",
              " 'severe': 124,\n",
              " 'inability': 125,\n",
              " 'pull': 126,\n",
              " 'myself': 127,\n",
              " 'from': 128,\n",
              " 'bed': 129,\n",
              " 'headaches': 130,\n",
              " 'everyday': 131,\n",
              " 'felt': 132,\n",
              " 'coming': 133,\n",
              " 'down': 134,\n",
              " 'flu': 135,\n",
              " 'will': 136,\n",
              " 'take': 137,\n",
              " 'statins': 138,\n",
              " 'again': 139,\n",
              " 'referenced': 140,\n",
              " 'have': 141,\n",
              " 'been': 142,\n",
              " 'for': 143,\n",
              " '10': 144,\n",
              " 'years': 145,\n",
              " 'heart': 146,\n",
              " 'maintenance': 147,\n",
              " '20mg': 148,\n",
              " 'iwas': 149,\n",
              " 'changed': 150,\n",
              " '10mg': 151,\n",
              " 'added': 152,\n",
              " 'tricor': 153,\n",
              " '145': 154,\n",
              " 'mg': 155,\n",
              " 'because': 156,\n",
              " 'high': 157,\n",
              " 'ldl': 158,\n",
              " 'do': 159,\n",
              " 'experience': 160,\n",
              " 'leg': 161,\n",
              " 'weakness': 162,\n",
              " 'cramping': 163,\n",
              " 'relieved': 164,\n",
              " 'potassium': 165,\n",
              " 'stiff': 166,\n",
              " 'neck': 167,\n",
              " 'tightness': 168,\n",
              " 'in': 169,\n",
              " 'shoulders': 170,\n",
              " 'recommended': 171,\n",
              " 'week': 172,\n",
              " 'chest': 173,\n",
              " 'numness': 174,\n",
              " 'aching': 175,\n",
              " 'feeling': 176,\n",
              " 'left': 177,\n",
              " 'arm': 178,\n",
              " 'inbetween': 179,\n",
              " 'both': 180,\n",
              " 'shoulder': 181,\n",
              " 'blades': 182,\n",
              " 'wake': 183,\n",
              " 'up': 184,\n",
              " 'shakey': 185,\n",
              " 'headache': 186,\n",
              " 'just': 187,\n",
              " 'tiered': 188,\n",
              " 'called': 189,\n",
              " 'dr': 190,\n",
              " 'he': 191,\n",
              " 'said': 192,\n",
              " 'stop': 193,\n",
              " 'takeing': 194,\n",
              " 'must': 195,\n",
              " 'exercise': 196,\n",
              " 'sudden': 197,\n",
              " 'very': 198,\n",
              " 'urinary': 199,\n",
              " 'tract': 200,\n",
              " 'infections': 201,\n",
              " 'took': 202,\n",
              " '8': 203,\n",
              " 'figure': 204,\n",
              " 'out': 205,\n",
              " 'why': 206,\n",
              " 'these': 207,\n",
              " 'every': 208,\n",
              " '3': 209,\n",
              " '5': 210,\n",
              " 'months': 211,\n",
              " 'too': 212,\n",
              " 'routine': 213,\n",
              " 'check': 214,\n",
              " 'level': 215,\n",
              " 'found': 216,\n",
              " 'sky': 217,\n",
              " 'two': 218,\n",
              " 'ago': 219,\n",
              " 'internist': 220,\n",
              " 'put': 221,\n",
              " 'helped': 222,\n",
              " 'reducing': 223,\n",
              " 'by': 224,\n",
              " '20': 225,\n",
              " 'side': 226,\n",
              " 'effects': 227,\n",
              " 'were': 228,\n",
              " 'terrible': 229,\n",
              " 'had': 230,\n",
              " 'number': 231,\n",
              " 'hipjoints': 232,\n",
              " 'increased': 233,\n",
              " 'tenfold': 234,\n",
              " 'could': 235,\n",
              " 'hardly': 236,\n",
              " 'walk': 237,\n",
              " 'sit': 238,\n",
              " 'drive': 239,\n",
              " 'car': 240,\n",
              " 'experiencing': 241,\n",
              " 'especially': 242,\n",
              " 'around': 243,\n",
              " 'incredible': 244,\n",
              " 'specialist': 245,\n",
              " 'reduce': 246,\n",
              " 'dose': 247,\n",
              " 'day': 248,\n",
              " 'already': 249,\n",
              " 'decreased': 250,\n",
              " 'somewhat': 251,\n",
              " 'learned': 252,\n",
              " 'pharmacist': 253,\n",
              " 'that': 254,\n",
              " 'drug': 255,\n",
              " 'can': 256,\n",
              " 'cause': 257,\n",
              " 'wasting': 258,\n",
              " 'muscles': 259,\n",
              " 'possible': 260,\n",
              " 'end': 261,\n",
              " 'wheelchair': 262,\n",
              " 'start': 263,\n",
              " 'alternative': 264,\n",
              " 'medicines': 265,\n",
              " 'use': 266,\n",
              " 'medicine': 267,\n",
              " 'insomnia': 268,\n",
              " 'aches': 269,\n",
              " '00am': 270,\n",
              " 'slept': 271,\n",
              " 'over': 272,\n",
              " 'plan': 273,\n",
              " 'tomorrow': 274,\n",
              " 'tho': 275,\n",
              " 'prescribed': 276,\n",
              " 'voltaren': 277,\n",
              " 'xr': 278,\n",
              " '100': 279,\n",
              " 'worked': 280,\n",
              " 'wonders': 281,\n",
              " 'thought': 282,\n",
              " 'discovered': 283,\n",
              " 'wonder': 284,\n",
              " 'then': 285,\n",
              " '30': 286,\n",
              " 'at': 287,\n",
              " 'passing': 288,\n",
              " 'massive': 289,\n",
              " 'amounts': 290,\n",
              " 'blood': 291,\n",
              " 'without': 292,\n",
              " 'warning': 293,\n",
              " 'bleeding': 294,\n",
              " 'death': 295,\n",
              " 'er': 296,\n",
              " 'admitted': 297,\n",
              " 'colonoscopy': 298,\n",
              " 'next': 299,\n",
              " 'discover': 300,\n",
              " 'an': 301,\n",
              " 'ulcer': 302,\n",
              " 'bowel': 303,\n",
              " 'which': 304,\n",
              " 'believes': 305,\n",
              " 'caused': 306,\n",
              " 'ok': 307,\n",
              " 'any': 308,\n",
              " 'class': 309,\n",
              " 'hindsight': 310,\n",
              " 'onset': 311,\n",
              " 'knees': 312,\n",
              " 'painfull': 313,\n",
              " 'welts': 314,\n",
              " 'bottom': 315,\n",
              " 'feet': 316,\n",
              " 'cane': 317,\n",
              " 'initially': 318,\n",
              " 'quack': 319,\n",
              " 'md': 320,\n",
              " 'anti': 321,\n",
              " 'inflam': 322,\n",
              " 'drugs': 323,\n",
              " 'subsequently': 324,\n",
              " 'quit': 325,\n",
              " 'symptoms': 326,\n",
              " 'subsided': 327,\n",
              " 'completely': 328,\n",
              " 'however': 329,\n",
              " 'milder': 330,\n",
              " 'reaction': 331,\n",
              " 'zocor': 332,\n",
              " 'dizziness': 333,\n",
              " 'pravachol': 334,\n",
              " 'zetia': 335,\n",
              " 'mild': 336,\n",
              " 'believe': 337,\n",
              " 'permanent': 338,\n",
              " 'damage': 339,\n",
              " 'join': 340,\n",
              " 'action': 341,\n",
              " 'lawsuit': 342,\n",
              " 'relating': 343,\n",
              " 'crumby': 344,\n",
              " 'clinically': 345,\n",
              " 'proved': 346,\n",
              " 'theory': 347,\n",
              " 'yet': 348,\n",
              " 'strongly': 349,\n",
              " '4': 350,\n",
              " 'ended': 351,\n",
              " 'giving': 352,\n",
              " 'multiple': 353,\n",
              " 'sclerosis': 354,\n",
              " '37': 355,\n",
              " 'yr': 356,\n",
              " 'old': 357,\n",
              " 'male': 358,\n",
              " 'diagnosed': 359,\n",
              " 'ms': 360,\n",
              " 'highest': 361,\n",
              " '212': 362,\n",
              " 'gp': 363,\n",
              " 'total': 364,\n",
              " 'while': 365,\n",
              " 'got': 366,\n",
              " '130': 367,\n",
              " 'ish': 368,\n",
              " 'low': 369,\n",
              " 'opinion': 370,\n",
              " 'began': 371,\n",
              " 'attacking': 372,\n",
              " 'cns': 373,\n",
              " 'live': 374,\n",
              " 'proven': 375,\n",
              " 'nor': 376,\n",
              " 'gone': 377,\n",
              " 'road': 378,\n",
              " 'big': 379,\n",
              " 'pharma': 380,\n",
              " 'wants': 381,\n",
              " 'promote': 382,\n",
              " 'treat': 383,\n",
              " 'jan': 384,\n",
              " '2010': 385,\n",
              " 'became': 386,\n",
              " 'present': 387,\n",
              " 'human': 388,\n",
              " 'rat': 389,\n",
              " 'make': 390,\n",
              " 'matters': 391,\n",
              " 'even': 392,\n",
              " 'worse': 393,\n",
              " 'therapy': 394,\n",
              " 'neurologist': 395,\n",
              " 'happens': 396,\n",
              " 'company': 397,\n",
              " 'mfg': 398,\n",
              " 'hmmm': 399,\n",
              " 'female': 400,\n",
              " 'age': 401,\n",
              " '55': 402,\n",
              " 'good': 403,\n",
              " 'health': 404,\n",
              " 'no': 405,\n",
              " 'smoking': 406,\n",
              " 'drinking': 407,\n",
              " 'daily': 408,\n",
              " 'wt': 409,\n",
              " '15': 410,\n",
              " 'lbs': 411,\n",
              " 'than': 412,\n",
              " 'should': 413,\n",
              " 'despite': 414,\n",
              " '40mg': 415,\n",
              " 'year': 416,\n",
              " 'previously': 417,\n",
              " 'lately': 418,\n",
              " 'realized': 419,\n",
              " 'everything': 420,\n",
              " 'hurts': 421,\n",
              " 'crept': 422,\n",
              " 'rn': 423,\n",
              " 'still': 424,\n",
              " 'together': 425,\n",
              " 'until': 426,\n",
              " 'decided': 427,\n",
              " 'right': 428,\n",
              " 'pains': 429,\n",
              " 'arthritis': 430,\n",
              " 'much': 431,\n",
              " 'activity': 432,\n",
              " 'generalized': 433,\n",
              " 'fading': 434,\n",
              " 'explore': 435,\n",
              " 'other': 436,\n",
              " 'options': 437,\n",
              " 'long': 438,\n",
              " 'enough': 439,\n",
              " 'confirm': 440,\n",
              " 'term': 441,\n",
              " 'looking': 442,\n",
              " 'site': 443,\n",
              " 'begin': 444,\n",
              " 'ask': 445,\n",
              " 'patients': 446,\n",
              " 'history': 447,\n",
              " 'last': 448,\n",
              " 'legs': 449,\n",
              " 'definite': 450,\n",
              " 'decrease': 451,\n",
              " 'strength': 452,\n",
              " 'spent': 453,\n",
              " 'fortune': 454,\n",
              " 'orthodics': 455,\n",
              " 'shoes': 456,\n",
              " 'small': 457,\n",
              " 'amount': 458,\n",
              " 'always': 459,\n",
              " 'hyper': 460,\n",
              " 'sensitive': 461,\n",
              " 'convinced': 462,\n",
              " 'immediately': 463,\n",
              " 'test': 464,\n",
              " 'definitely': 465,\n",
              " 'lowers': 466,\n",
              " 'cut': 467,\n",
              " 'half': 468,\n",
              " 'research': 469,\n",
              " 'though': 470,\n",
              " 'hunger': 471,\n",
              " 'pangs': 472,\n",
              " 'brilliant': 473,\n",
              " 'new': 474,\n",
              " 'lease': 475,\n",
              " 'life': 476,\n",
              " 'steps': 477,\n",
              " 'properly': 478,\n",
              " 'longer': 479,\n",
              " 'sideways': 480,\n",
              " 'toddler': 481,\n",
              " 'hip': 482,\n",
              " 'if': 483,\n",
              " 'jar': 484,\n",
              " 'experienced': 485,\n",
              " 'lost': 486,\n",
              " 'pounds': 487,\n",
              " 'relatively': 488,\n",
              " 'little': 489,\n",
              " 'lift': 490,\n",
              " 'arms': 491,\n",
              " 'deteriorated': 492,\n",
              " 'since': 493,\n",
              " 'polymyositis': 494,\n",
              " 'exacerbated': 495,\n",
              " 'taken': 496,\n",
              " 'stopping': 497,\n",
              " 'able': 498,\n",
              " 'luckily': 499,\n",
              " 'being': 500,\n",
              " 'admited': 501,\n",
              " 'hospital': 502,\n",
              " 'seen': 503,\n",
              " 'rheumatologist': 504,\n",
              " 'who': 505,\n",
              " 'knew': 506,\n",
              " 'what': 507,\n",
              " 'she': 508,\n",
              " 'seeing': 509,\n",
              " 'get': 510,\n",
              " 'full': 511,\n",
              " 'statin': 512,\n",
              " 'dangerous': 513,\n",
              " 'heard': 514,\n",
              " 'people': 515,\n",
              " 'uncharacteristic': 516,\n",
              " 'anxiety': 517,\n",
              " 'aggression': 518,\n",
              " 'mood': 519,\n",
              " 'swings': 520,\n",
              " 'cannot': 521,\n",
              " 'stress': 522,\n",
              " 'personality': 523,\n",
              " 'comepletely': 524,\n",
              " 'beginning': 525,\n",
              " 'medication': 526,\n",
              " 'typically': 527,\n",
              " 'easy': 528,\n",
              " 'positively': 529,\n",
              " 'thinking': 530,\n",
              " 'emotionally': 531,\n",
              " 'stable': 532,\n",
              " 'motivated': 533,\n",
              " 'starting': 534,\n",
              " 'gradually': 535,\n",
              " 'descended': 536,\n",
              " 'into': 537,\n",
              " 'world': 538,\n",
              " 'hopelessness': 539,\n",
              " 'lonliness': 540,\n",
              " 'self': 541,\n",
              " 'loathing': 542,\n",
              " 'depressed': 543,\n",
              " 'obsessively': 544,\n",
              " 'focused': 545,\n",
              " 'morbid': 546,\n",
              " 'terrifying': 547,\n",
              " 'thoughts': 548,\n",
              " 'negative': 549,\n",
              " 'off': 550,\n",
              " 'only': 551,\n",
              " '48': 552,\n",
              " 'hours': 553,\n",
              " 'improvement': 554,\n",
              " 'moods': 555,\n",
              " 'journey': 556,\n",
              " 'feel': 557,\n",
              " 'oh': 558,\n",
              " 'yes': 559,\n",
              " 'suffered': 560,\n",
              " 'gas': 561,\n",
              " 'constipation': 562,\n",
              " 'fatigue': 563,\n",
              " 'weight': 564,\n",
              " 'gain': 565,\n",
              " 'bloating': 566,\n",
              " 'occassional': 567,\n",
              " 'bouts': 568,\n",
              " 'diaharrea': 569,\n",
              " 'lowered': 570,\n",
              " 'significantly': 571,\n",
              " 'quality': 572,\n",
              " 'website': 573,\n",
              " 'absolute': 574,\n",
              " 'lifeline': 575,\n",
              " 'seemed': 576,\n",
              " 'unwilling': 577,\n",
              " 'accept': 578,\n",
              " 'possibly': 579,\n",
              " 'related': 580,\n",
              " 'know': 581,\n",
              " 'things': 582,\n",
              " 'they': 583,\n",
              " 'way': 584,\n",
              " 'almost': 585,\n",
              " 'first': 586,\n",
              " 'hearing': 587,\n",
              " 'exact': 588,\n",
              " 'given': 589,\n",
              " 'courage': 590,\n",
              " 'needed': 591,\n",
              " 'family': 592,\n",
              " 'members': 593,\n",
              " 'friends': 594,\n",
              " 'bad': 595,\n",
              " 'reactions': 596,\n",
              " 'stuff': 597,\n",
              " 'trust': 598,\n",
              " 'another': 599,\n",
              " 'under': 600,\n",
              " 'control': 601,\n",
              " 'careful': 602,\n",
              " 'informed': 603,\n",
              " 'word': 604,\n",
              " 'its': 605,\n",
              " 'face': 606,\n",
              " 'value': 607,\n",
              " 'comes': 608,\n",
              " 'dictate': 609,\n",
              " 'understand': 610,\n",
              " 'call': 611,\n",
              " 'shots': 612,\n",
              " 'does': 613,\n",
              " 'go': 614,\n",
              " 'heel': 615,\n",
              " 'walking': 616,\n",
              " 'mid': 617,\n",
              " 'november': 618,\n",
              " 'orthopedist': 619,\n",
              " 'early': 620,\n",
              " 'december': 621,\n",
              " 'plantar': 622,\n",
              " 'fasciitis': 623,\n",
              " 'showed': 624,\n",
              " 'stretching': 625,\n",
              " 'exercises': 626,\n",
              " 'advised': 627,\n",
              " 'inserts': 628,\n",
              " 'wear': 629,\n",
              " 'cushioned': 630,\n",
              " 'house': 631,\n",
              " 'slippers': 632,\n",
              " 'avoid': 633,\n",
              " 'inclines': 634,\n",
              " 'lessened': 635,\n",
              " 'following': 636,\n",
              " 'recommendations': 637,\n",
              " 'uncomfortable': 638,\n",
              " 'lot': 639,\n",
              " 'finally': 640,\n",
              " 'march': 641,\n",
              " 'foot': 642,\n",
              " 'improve': 643,\n",
              " 'etc': 644,\n",
              " 'asked': 645,\n",
              " 'patch': 646,\n",
              " 'appointment': 647,\n",
              " 'sample': 648,\n",
              " 'prescription': 649,\n",
              " 'flector': 650,\n",
              " 'there': 651,\n",
              " 'five': 652,\n",
              " 'each': 653,\n",
              " 'reclosable': 654,\n",
              " 'pouch': 655,\n",
              " 'covers': 656,\n",
              " 'area': 657,\n",
              " 'used': 658,\n",
              " 'puffiness': 659,\n",
              " 'best': 660,\n",
              " 'night': 661,\n",
              " 'although': 662,\n",
              " 'replace': 663,\n",
              " '12': 664,\n",
              " 'bedtime': 665,\n",
              " 'tape': 666,\n",
              " 'edges': 667,\n",
              " 'sock': 668,\n",
              " 'secure': 669,\n",
              " 'three': 670,\n",
              " 'weeks': 671,\n",
              " 'several': 672,\n",
              " 'times': 673,\n",
              " 'certainly': 674,\n",
              " 'past': 675,\n",
              " 'comfortable': 676,\n",
              " 'far': 677,\n",
              " 'great': 678,\n",
              " 'working': 679,\n",
              " 'fine': 680,\n",
              " 'per': 681,\n",
              " 'glad': 682,\n",
              " 'third': 683,\n",
              " 'numb': 684,\n",
              " 'most': 685,\n",
              " 'light': 686,\n",
              " 'headed': 687,\n",
              " 'fun': 688,\n",
              " 'knee': 689,\n",
              " 'suggest': 690,\n",
              " 'nothing': 691,\n",
              " 'inflammation': 692,\n",
              " 'panic': 693,\n",
              " 'system': 694,\n",
              " 'due': 695,\n",
              " 'enzyme': 696,\n",
              " 'reading': 697,\n",
              " 'med': 698,\n",
              " 'ankle': 699,\n",
              " 'neurophaty': 700,\n",
              " 'vision': 701,\n",
              " 'ever': 702,\n",
              " 'nausea': 703,\n",
              " 'diarrhea': 704,\n",
              " 'dark': 705,\n",
              " 'urine': 706,\n",
              " 'barely': 707,\n",
              " 'hope': 708,\n",
              " 'bug': 709,\n",
              " 'tht': 710,\n",
              " 'neighborhood': 711,\n",
              " 'pravochol': 712,\n",
              " 'lowering': 713,\n",
              " 'arthrotec': 714,\n",
              " 'pleased': 715,\n",
              " 'help': 716,\n",
              " 'deal': 717,\n",
              " 'six': 718,\n",
              " 'having': 719,\n",
              " 'vaginal': 720,\n",
              " 'lots': 721,\n",
              " 'clotting': 722,\n",
              " 'horrible': 723,\n",
              " 'periods': 724,\n",
              " 'obgyn': 725,\n",
              " 'came': 726,\n",
              " 'normal': 727,\n",
              " 'doc': 728,\n",
              " 'birth': 729,\n",
              " 'scheduled': 730,\n",
              " 'endometrial': 731,\n",
              " 'biopsy': 732,\n",
              " 'wed': 733,\n",
              " 'ephiphany': 734,\n",
              " 'researching': 735,\n",
              " 'effect': 736,\n",
              " 'across': 737,\n",
              " 'appalled': 738,\n",
              " 'majority': 739,\n",
              " 'females': 740,\n",
              " 'mention': 741,\n",
              " 'heavy': 742,\n",
              " 'extra': 743,\n",
              " 'firmly': 744,\n",
              " 'answer': 745,\n",
              " 'problem': 746,\n",
              " 'telling': 747,\n",
              " 'monday': 748,\n",
              " 'worth': 749,\n",
              " 'cost': 750,\n",
              " 'hassle': 751,\n",
              " 'constantly': 752,\n",
              " 'top': 753,\n",
              " 'procedure': 754,\n",
              " '31': 755,\n",
              " 'endure': 756,\n",
              " 'truly': 757,\n",
              " 'look': 758,\n",
              " 'forward': 759,\n",
              " 'llkely': 760,\n",
              " 'couple': 761,\n",
              " 'else': 762,\n",
              " 'them': 763,\n",
              " 'become': 764,\n",
              " 'dependent': 765,\n",
              " 'function': 766,\n",
              " 'few': 767,\n",
              " 'abdominal': 768,\n",
              " 'some': 769,\n",
              " 'sort': 770,\n",
              " 'g': 771,\n",
              " 'recently': 772,\n",
              " 'e': 773,\n",
              " 'r': 774,\n",
              " 'waiting': 775,\n",
              " 'cardio': 776,\n",
              " 'think': 777,\n",
              " 'really': 778,\n",
              " 'screwed': 779,\n",
              " 'turned': 780,\n",
              " 'cripple': 781,\n",
              " 'morning': 782,\n",
              " 'hips': 783,\n",
              " 'liver': 784,\n",
              " 'stairs': 785,\n",
              " 'bend': 786,\n",
              " 'store': 787,\n",
              " 'twice': 788,\n",
              " 'forty': 789,\n",
              " 'minutes': 790,\n",
              " 'tried': 791,\n",
              " 'motrin': 792,\n",
              " 'asper': 793,\n",
              " 'cream': 794,\n",
              " 'today': 795,\n",
              " 'attack': 796,\n",
              " 'seems': 797,\n",
              " 'easier': 798,\n",
              " 'increasing': 799,\n",
              " 'stiffness': 800,\n",
              " 'attributed': 801,\n",
              " 'running': 802,\n",
              " 'miles': 803,\n",
              " '60': 804,\n",
              " 'minute': 805,\n",
              " 'spin': 806,\n",
              " 'classes': 807,\n",
              " 'weekly': 808,\n",
              " 'resistance': 809,\n",
              " 'training': 810,\n",
              " 'worsened': 811,\n",
              " 'swelling': 812,\n",
              " 'drained': 813,\n",
              " '25': 814,\n",
              " 'cc': 815,\n",
              " 'fluid': 816,\n",
              " 'orothopedic': 817,\n",
              " 'consult': 818,\n",
              " 'x': 819,\n",
              " 'rays': 820,\n",
              " 'mri': 821,\n",
              " 'revealed': 822,\n",
              " 'abnormality': 823,\n",
              " 'worsend': 824,\n",
              " 'spread': 825,\n",
              " 'skipped': 826,\n",
              " '36': 827,\n",
              " 'abated': 828,\n",
              " 'later': 829,\n",
              " 'nearly': 830,\n",
              " 'confirms': 831,\n",
              " 'decision': 832,\n",
              " 'discontinue': 833,\n",
              " 'entirely': 834,\n",
              " 'bring': 835,\n",
              " '235': 836,\n",
              " '175': 837,\n",
              " 'bloodwork': 838,\n",
              " 'indicated': 839,\n",
              " 'abnormilities': 840,\n",
              " 'those': 841,\n",
              " 'results': 842,\n",
              " 'switching': 843,\n",
              " 'choleast': 844,\n",
              " 'counter': 845,\n",
              " 'red': 846,\n",
              " 'rice': 847,\n",
              " 'yeast': 848,\n",
              " 'formula': 849,\n",
              " 'paranoia': 850,\n",
              " 'reduced': 851,\n",
              " 'triglycerides': 852,\n",
              " 'psychiatric': 853,\n",
              " 'harsh': 854,\n",
              " 'sides': 855,\n",
              " 'painful': 856,\n",
              " 'sleep': 857,\n",
              " 'ambien': 858,\n",
              " 'melatonin': 859,\n",
              " '9': 860,\n",
              " 'disappear': 861,\n",
              " 'stomach': 862,\n",
              " 'irritation': 863,\n",
              " 'feels': 864,\n",
              " 'bit': 865,\n",
              " 'pressure': 866,\n",
              " 'head': 867,\n",
              " 'mostly': 868,\n",
              " 'pulled': 869,\n",
              " 'gym': 870,\n",
              " 'pretty': 871,\n",
              " 'gotten': 872,\n",
              " 'stretch': 873,\n",
              " 'helping': 874,\n",
              " 'final': 875,\n",
              " 'spine': 876,\n",
              " 'gave': 877,\n",
              " 'samples': 878,\n",
              " '75': 879,\n",
              " 'how': 880,\n",
              " 'drub': 881,\n",
              " 'comfortably': 882,\n",
              " 'touch': 883,\n",
              " 'toes': 884,\n",
              " 'bending': 885,\n",
              " 'works': 886,\n",
              " 'skin': 887,\n",
              " 'burns': 888,\n",
              " 'cycle': 889,\n",
              " 'menopausal': 890,\n",
              " 'breast': 891,\n",
              " 'erithema': 892,\n",
              " 'dry': 893,\n",
              " 'cuts': 894,\n",
              " 'hands': 895,\n",
              " 'm': 896,\n",
              " 'sweating': 897,\n",
              " 'usually': 898,\n",
              " 'pruritus': 899,\n",
              " 'ani': 900,\n",
              " 'improvements': 901,\n",
              " 'second': 902,\n",
              " 'pills': 903,\n",
              " 'topical': 904,\n",
              " 'exclude': 905,\n",
              " 'idea': 906,\n",
              " 'internet': 907,\n",
              " 'none': 908,\n",
              " 'noticed': 909,\n",
              " 'slightly': 910,\n",
              " 'elevated': 911,\n",
              " 'norm': 912,\n",
              " 'suspect': 913,\n",
              " 'responsible': 914,\n",
              " 'short': 915,\n",
              " 'overly': 916,\n",
              " 'concerned': 917,\n",
              " 'acute': 918,\n",
              " 'tendonitis': 919,\n",
              " 'agony': 920,\n",
              " '24': 921,\n",
              " 'dramatically': 922,\n",
              " 'gaining': 923,\n",
              " 'range': 924,\n",
              " 'motion': 925,\n",
              " 'calcification': 926,\n",
              " 'quickly': 927,\n",
              " 'target': 928,\n",
              " '200': 929,\n",
              " 'widespread': 930,\n",
              " 'interupt': 931,\n",
              " 'upon': 932,\n",
              " 'waking': 933,\n",
              " 'hurt': 934,\n",
              " 'fist': 935,\n",
              " 'standing': 936,\n",
              " 'sitting': 937,\n",
              " 'move': 938,\n",
              " 'someone': 939,\n",
              " 'diminishes': 940,\n",
              " 'throughout': 941,\n",
              " 'lst': 942,\n",
              " 'probably': 943,\n",
              " 'explain': 944,\n",
              " 'overnight': 945,\n",
              " 'appearance': 946,\n",
              " 'discomfort': 947,\n",
              " 'similar': 948,\n",
              " 'confirming': 949,\n",
              " 'inflamation': 950,\n",
              " 'suggested': 951,\n",
              " 'symptons': 952,\n",
              " 'disappears': 953,\n",
              " 'fosamax': 954,\n",
              " '35': 955,\n",
              " 'tighening': 956,\n",
              " 'cramps': 957,\n",
              " 'medicatiion': 958,\n",
              " 'niaspan': 959,\n",
              " 'anyday': 960,\n",
              " 'weak': 961,\n",
              " 'general': 962,\n",
              " 'malaise': 963,\n",
              " 'recommendthis': 964,\n",
              " 'die': 965,\n",
              " 'hubby': 966,\n",
              " 'eye': 967,\n",
              " 'rash': 968,\n",
              " 'forehead': 969,\n",
              " 'sweats': 970,\n",
              " 'difficulty': 971,\n",
              " 'breathing': 972,\n",
              " 'huge': 973,\n",
              " 'athlete': 974,\n",
              " 'matter': 975,\n",
              " 'rashes': 976,\n",
              " 'throat': 977,\n",
              " 'choking': 978,\n",
              " 'easily': 979,\n",
              " 'crackers': 980,\n",
              " 'worst': 981,\n",
              " 'read': 982,\n",
              " 'diminished': 983,\n",
              " 'greatly': 984,\n",
              " 'thanks': 985,\n",
              " 'input': 986,\n",
              " 'aggitated': 987,\n",
              " 'excited': 988,\n",
              " 'lymph': 989,\n",
              " 'nodes': 990,\n",
              " 'modified': 991,\n",
              " 'radical': 992,\n",
              " 'mascetomy': 993,\n",
              " 'steriod': 994,\n",
              " 'relief': 995,\n",
              " 'lived': 996,\n",
              " 'dawned': 997,\n",
              " 'missed': 998,\n",
              " 'weekend': 999,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBSwg9bSJiJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = list()\n",
        "x_test = list()\n",
        "def encode_text(toks,type='train'):\n",
        "  encod_arr = [None]*len(toks)\n",
        "  try:\n",
        "    for idx,tok in enumerate(toks):\n",
        "      encod_arr[idx] = vocab[tok]\n",
        "  except: pass\n",
        "\n",
        "  if type =='train' : x_train.append(encod_arr)\n",
        "  if type =='test' : x_test.append(encod_arr)\n",
        "\n",
        "\n",
        "for item in train_body:\n",
        "  encode_text(item.split(), 'train')\n",
        "\n",
        "for item in test_body:\n",
        "  encode_text(item.split(), 'test')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-5IzGtWJkzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4abcc452-b5a1-4923-e820-30f9013c88c3"
      },
      "source": [
        "print(x_train[0],train_body[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 9, 17, 18, 19, 8, 7, 20, 8, 7, 21, 22, 23, 24, 8, 7, 9, 10] extreme tiredness and flatulence not sure whether it is the lipitor but i am now tired to the point of exhaustion is it work is it me getting older or is it the lipitor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZIRJw99Jn_y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2e4c870-9dc7-446f-a5d3-b7c0c6e79f6d"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def k_dot(vec_x, vec_y):\n",
        "    \"\"\"Dot operation for Keras compatible for theano and tensorflow.\n",
        "    Args:\n",
        "        vec_x: vector.\n",
        "        vec_y: vector.\n",
        "    Returns:\n",
        "        A dot product operation by two vectors.\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(vec_x, K.expand_dims(vec_y)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(vec_x, vec_y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7qfiAN3Jqgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import initializers, regularizers, constraints\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, bias=True, **kwargs):\n",
        "        \"\"\"The implementation of an attention mechanism layer.\n",
        "        Args:\n",
        "            bias: boolean, adapt a bias modification or not.\n",
        "            kwargs: any keyword arguments that base layer accepts.\n",
        "        \"\"\"\n",
        "        self.bias = bias\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"Create a trainable weight variable.\n",
        "        This attention layer accepts the tensor that has three\n",
        "        dimensions: (batch_size, time_steps, input_dim).\n",
        "        Args:\n",
        "            input_shape: 3d Input, (batch_size, time_steps, input_dim).\n",
        "        Raises:\n",
        "            ValueError: if input_shape is not 3 dimensions.\n",
        "        \"\"\"\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name='W', shape=(input_shape[-1], input_shape[-1],),\n",
        "            initializer=keras.initializers.get('uniform')\n",
        "        )\n",
        "\n",
        "        self.b = self.add_weight(\n",
        "            name='bias', shape=(input_shape[-1],),\n",
        "            initializer='zero'\n",
        "        )\n",
        "\n",
        "        self.u = self.add_weight(\n",
        "            name='context_vector', shape=(input_shape[-1],),\n",
        "            initializer=keras.initializers.get('uniform')\n",
        "        )\n",
        "\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        att_weights = self._get_attention_weights(x)\n",
        "\n",
        "        # Reshape the attention weights to match the dimensions of X\n",
        "        att_weights = K.expand_dims(att_weights)\n",
        "        # Multiply each input by its attention weights\n",
        "        # TODO: use backend function for multiply\n",
        "        weighted_input = keras.layers.Multiply()([x, att_weights])\n",
        "\n",
        "        # Sum in the direction of the time-axis.\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n",
        "\n",
        "    def _get_attention_weights(self, x):\n",
        "        \"\"\"Calculate attention weights.\n",
        "        Args:\n",
        "            x: Input array.\n",
        "        Returns:\n",
        "            a: attention weights.\n",
        "        \"\"\"\n",
        "        u_xw = k_dot(x, self.W)\n",
        "        if self.bias:\n",
        "            u_xw += self.b\n",
        "        u_tw = K.tanh(u_xw)\n",
        "        a_tw = k_dot(u_tw, self.u)\n",
        "\n",
        "        # apply softmax to get probability for attention\n",
        "        a = K.softmax(a_tw)\n",
        "\n",
        "        return a"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBLBIemvJuJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import Input, Model\n",
        "from keras.layers import Embedding, Dense, Bidirectional, CuDNNLSTM, TimeDistributed,Lambda\n",
        "\n",
        "max_features = len(vocab)\n",
        "maxlen_sentence = 16\n",
        "maxlen_word = 25\n",
        "batch_size = 32\n",
        "embedding_dims = 50\n",
        "epochs = 10\n",
        "\n",
        "class HAN():\n",
        "    def __init__(self, maxlen_sentence=16, maxlen_word=25, max_features=len(vocab), embedding_dims=50,\n",
        "                 class_num=2,\n",
        "                 last_activation='softmax'):\n",
        "        self.maxlen_sentence = maxlen_sentence\n",
        "        self.maxlen_word = maxlen_word\n",
        "        self.max_features = max_features\n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.class_num = class_num\n",
        "        self.last_activation = last_activation\n",
        "        #self.embedding_matrix = embedding_matrix\n",
        "\n",
        "\n",
        "    def build_word_encoder(self):\n",
        "        # Word part\n",
        "        input_word = Input(shape=(self.maxlen_word,))\n",
        "        embedded_word = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen_word)(input_word)\n",
        "        x_word = Bidirectional(CuDNNLSTM(128, return_sequences=True))(embedded_word)  # LSTM or GRU\n",
        "        word_att = Attention(self.maxlen_word,name='word_attention')(x_word)\n",
        "        model_word = Model(input_word, word_att)\n",
        "        return model_word\n",
        "\n",
        "    def build_sent_encoder(self, sent_encoder):\n",
        "        # Sentence part\n",
        "        sent_lstm = Bidirectional(CuDNNLSTM(128, return_sequences=True))(sent_encoder)  # LSTM or GRU\n",
        "\n",
        "        sent_att = Attention(name='sent_attention')(sent_lstm)\n",
        "        return sent_att\n",
        "      \n",
        "    def get_model(self):\n",
        "        text_input = Input(shape=(self.maxlen_sentence, self.maxlen_word))\n",
        "        # encode sentences into a single vector per sentence\n",
        "        self.model_word = self.build_word_encoder()\n",
        "        # time distribute word model to accept text input\n",
        "        sent_encoder = TimeDistributed(self.model_word)(text_input)\n",
        "\n",
        "        sent_att = self.build_sent_encoder(sent_encoder)\n",
        "        # dense the output to 2 because the result is a binary classification.\n",
        "        output_tensor = Dense(2, activation='softmax', name='classification')(sent_att)\n",
        "        # Create Sentence-level Model\n",
        "        self.model = Model(text_input, output_tensor)\n",
        "        return Model(text_input, output_tensor)\n",
        "\n",
        "    def show_word_attention(self, x):\n",
        "        \"\"\"Show the prediction of the word level attention.\n",
        "\n",
        "        Args:\n",
        "            x: the input array with size of (max_sent_length,).\n",
        "\n",
        "        Returns:\n",
        "            Attention weights.\n",
        "        \"\"\"\n",
        "        att_layer = self.model_word.get_layer('word_attention')\n",
        "        prev_tensor = att_layer.input\n",
        "\n",
        "        # Create a temporary dummy layer to hold the\n",
        "        # attention weights tensor\n",
        "        dummy_layer = Lambda(\n",
        "            lambda x: att_layer._get_attention_weights(x)\n",
        "        )(prev_tensor)\n",
        "\n",
        "        return Model(self.model_word.input, dummy_layer).predict(x)\n",
        "\n",
        "    def show_sent_attention(self, x):\n",
        "        \"\"\"Show the prediction of the sentence level attention.\n",
        "\n",
        "        Args:\n",
        "            x: the input array with the size of (max_sent_num, max_sent_length).\n",
        "\n",
        "        Returns:\n",
        "            Attention weights.\n",
        "        \"\"\"\n",
        "        att_layer = self.model.get_layer('sent_attention')\n",
        "        prev_tensor = att_layer.input\n",
        "\n",
        "        dummy_layer = Lambda(\n",
        "            lambda x: att_layer._get_attention_weights(x)\n",
        "        )(prev_tensor)\n",
        "\n",
        "        return Model(self.model.input, dummy_layer).predict(x)\n",
        "      \n",
        "    "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F12xvD9rQRjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_attention_to_df(sent_tokenized_review, word_att):\n",
        "      \n",
        "\n",
        "      ori_sents = sent_tokenized_review\n",
        "      # split sentences into words\n",
        "      ori_words = [x for x in ori_sents.split()]\n",
        "      # truncate attentions to have equal size of number of words per sentence\n",
        "      ln=len(ori_words)\n",
        "      f=ln%maxlen_word\n",
        "      f1=ln//maxlen_word  \n",
        "      \n",
        "      words_attn=[]\n",
        "      for i in word_att[::-1]:\n",
        "        l=min(len(i),ln)\n",
        "        words_attn.extend(i[::-1][:l])\n",
        "        ln=ln-l\n",
        "        if ln<=0:\n",
        "          break\n",
        "\n",
        "      words_attn.reverse()    \n",
        "        \n",
        "      f1=f1+1\n",
        "      if f==0:\n",
        "        f=maxlen_word\n",
        "        f1=f1-1\n",
        "      mylist1=[]\n",
        "      mylist2=[]\n",
        "      dict_attn=dict(zip(ori_words,words_attn))\n",
        "      dict_items = dict_attn.items()       \n",
        "      k1=0\n",
        "      for i in range(f1):\n",
        "        mylist1.append(dict(zip(ori_words[k1:f+k1],words_attn[k1:k1+f])))\n",
        "        listToStr = ' '.join(map(str, ori_words[k1:k1+f]))         \n",
        "        mylist2.append(listToStr)\n",
        "        k1=f\n",
        "        f=f+maxlen_word \n",
        "        \n",
        "\n",
        "      return pd.DataFrame(zip(mylist1,mylist2),\n",
        "                  columns=['word_att', 'sentence'])\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR4sM2SVMF2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a820204-44b3-45cb-8140-c2ccd678ebf3"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "print('Pad sequences (samples x time)...')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen_sentence * maxlen_word)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen_sentence * maxlen_word)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen_sentence * maxlen_word)\n",
        "x_train = x_train.reshape((len(x_train), maxlen_sentence, maxlen_word))\n",
        "x_test = x_test.reshape((len(x_test), maxlen_sentence, maxlen_word))\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "\n",
        "han=HAN()\n",
        "print('Build model...')\n",
        "model = han.get_model()\n",
        "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
        "categories = ['Uncertainity of Post_Diagnosis', 'Results and Side-Effects Observed', 'Medical Assistance', 'Diet and Maintenance', 'Information Source']\n",
        "id_num=4\n",
        "show_results=[]\n",
        "for category in categories:\n",
        "    print('Reporting Category: ', category)\n",
        "\n",
        "    y_train_label = df_train[category].to_list()\n",
        "    y_test_label = df_test[category].to_list()\n",
        "\n",
        "    y_train = to_categorical(y_train_label, num_classes=None)\n",
        "    y_test = to_categorical(y_test_label, num_classes=None)\n",
        "\n",
        "    model.fit(x_train,y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              callbacks=[early_stopping],\n",
        "              validation_data=(x_test, y_test))\n",
        "    print('Test...')\n",
        "    result = model.predict(x_test)\n",
        "    predictions = [np.argmax(x) for x in result]\n",
        "    print(classification_report(y_test_label,predictions))\n",
        "    \n",
        "    df=word_attention_to_df(train_body[id_num],han.show_word_attention(x_train[id_num]))\n",
        "    show_results.append(df)\n",
        "    #print(train_body[id_num],\"\\n\")\n",
        "    #print(df) \n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pad sequences (samples x time)...\n",
            "x_train shape: (942, 16, 25)\n",
            "x_test shape: (300, 16, 25)\n",
            "Build model...\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train...\n",
            "Reporting Category:  Uncertainity of Post_Diagnosis\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 942 samples, validate on 300 samples\n",
            "Epoch 1/10\n",
            "942/942 [==============================] - 3s 3ms/step - loss: 0.4833 - accuracy: 0.8227 - val_loss: 0.3973 - val_accuracy: 0.8567\n",
            "Epoch 2/10\n",
            "128/942 [===>..........................] - ETA: 1s - loss: 0.4554 - accuracy: 0.8125"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "942/942 [==============================] - 2s 2ms/step - loss: 0.4401 - accuracy: 0.8217 - val_loss: 0.3814 - val_accuracy: 0.8567\n",
            "Epoch 3/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.3605 - accuracy: 0.8482 - val_loss: 0.3620 - val_accuracy: 0.8633\n",
            "Epoch 4/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9289 - val_loss: 0.4243 - val_accuracy: 0.8167\n",
            "Epoch 5/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0716 - accuracy: 0.9798 - val_loss: 0.5436 - val_accuracy: 0.8500\n",
            "Epoch 6/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0207 - accuracy: 0.9968 - val_loss: 0.9527 - val_accuracy: 0.7533\n",
            "Epoch 7/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0153 - accuracy: 0.9968 - val_loss: 0.7721 - val_accuracy: 0.8333\n",
            "Epoch 8/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0245 - accuracy: 0.9979 - val_loss: 0.8578 - val_accuracy: 0.7933\n",
            "Epoch 9/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0056 - accuracy: 0.9989 - val_loss: 0.8563 - val_accuracy: 0.8267\n",
            "Epoch 10/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0095 - accuracy: 0.9989 - val_loss: 0.8689 - val_accuracy: 0.8267\n",
            "Test...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.90       257\n",
            "           1       0.34      0.23      0.28        43\n",
            "\n",
            "    accuracy                           0.83       300\n",
            "   macro avg       0.61      0.58      0.59       300\n",
            "weighted avg       0.80      0.83      0.81       300\n",
            "\n",
            "Reporting Category:  Results and Side-Effects Observed\n",
            "Train on 942 samples, validate on 300 samples\n",
            "Epoch 1/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.6154 - accuracy: 0.8885 - val_loss: 0.0588 - val_accuracy: 0.9900\n",
            "Epoch 2/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0588 - accuracy: 0.9904 - val_loss: 0.0563 - val_accuracy: 0.9900\n",
            "Epoch 3/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0521 - accuracy: 0.9904 - val_loss: 0.0562 - val_accuracy: 0.9900\n",
            "Epoch 4/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0495 - accuracy: 0.9904 - val_loss: 0.0560 - val_accuracy: 0.9900\n",
            "Epoch 5/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0479 - accuracy: 0.9904 - val_loss: 0.0561 - val_accuracy: 0.9900\n",
            "Epoch 6/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0446 - accuracy: 0.9904 - val_loss: 0.0586 - val_accuracy: 0.9900\n",
            "Epoch 7/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0412 - accuracy: 0.9904 - val_loss: 0.0612 - val_accuracy: 0.9900\n",
            "Epoch 8/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0366 - accuracy: 0.9904 - val_loss: 0.0664 - val_accuracy: 0.9900\n",
            "Epoch 9/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0319 - accuracy: 0.9904 - val_loss: 0.0603 - val_accuracy: 0.9900\n",
            "Epoch 10/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0300 - accuracy: 0.9904 - val_loss: 0.0652 - val_accuracy: 0.9900\n",
            "Test...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         3\n",
            "           1       0.99      1.00      0.99       297\n",
            "\n",
            "    accuracy                           0.99       300\n",
            "   macro avg       0.49      0.50      0.50       300\n",
            "weighted avg       0.98      0.99      0.99       300\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Reporting Category:  Medical Assistance\n",
            "Train on 942 samples, validate on 300 samples\n",
            "Epoch 1/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.8312 - accuracy: 0.7197 - val_loss: 0.5623 - val_accuracy: 0.8100\n",
            "Epoch 2/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.5242 - accuracy: 0.7877 - val_loss: 0.4871 - val_accuracy: 0.8100\n",
            "Epoch 3/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.5213 - accuracy: 0.7877 - val_loss: 0.4879 - val_accuracy: 0.8100\n",
            "Epoch 4/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.5185 - accuracy: 0.7877 - val_loss: 0.4856 - val_accuracy: 0.8100\n",
            "Epoch 5/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.5153 - accuracy: 0.7877 - val_loss: 0.4882 - val_accuracy: 0.8100\n",
            "Epoch 6/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.5126 - accuracy: 0.7877 - val_loss: 0.4811 - val_accuracy: 0.8100\n",
            "Epoch 7/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.5016 - accuracy: 0.7866 - val_loss: 0.4486 - val_accuracy: 0.8100\n",
            "Epoch 8/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.4792 - accuracy: 0.7898 - val_loss: 0.4601 - val_accuracy: 0.8100\n",
            "Epoch 9/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.4679 - accuracy: 0.7887 - val_loss: 0.4430 - val_accuracy: 0.8100\n",
            "Epoch 10/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.4378 - accuracy: 0.7951 - val_loss: 0.4497 - val_accuracy: 0.8100\n",
            "Test...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      1.00      0.90       243\n",
            "           1       0.00      0.00      0.00        57\n",
            "\n",
            "    accuracy                           0.81       300\n",
            "   macro avg       0.41      0.50      0.45       300\n",
            "weighted avg       0.66      0.81      0.72       300\n",
            "\n",
            "Reporting Category:  Diet and Maintenance\n",
            "Train on 942 samples, validate on 300 samples\n",
            "Epoch 1/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.3612 - accuracy: 0.8832 - val_loss: 0.3695 - val_accuracy: 0.8800\n",
            "Epoch 2/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.3515 - accuracy: 0.8854 - val_loss: 0.3609 - val_accuracy: 0.8800\n",
            "Epoch 3/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.3418 - accuracy: 0.8854 - val_loss: 0.3557 - val_accuracy: 0.8800\n",
            "Epoch 4/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.3245 - accuracy: 0.8896 - val_loss: 0.3454 - val_accuracy: 0.8800\n",
            "Epoch 5/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.2592 - accuracy: 0.9034 - val_loss: 0.3230 - val_accuracy: 0.8667\n",
            "Epoch 6/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.1673 - accuracy: 0.9501 - val_loss: 0.4206 - val_accuracy: 0.8867\n",
            "Epoch 7/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.1249 - accuracy: 0.9650 - val_loss: 0.3994 - val_accuracy: 0.9033\n",
            "Epoch 8/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0625 - accuracy: 0.9883 - val_loss: 0.3236 - val_accuracy: 0.9033\n",
            "Epoch 9/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0227 - accuracy: 0.9968 - val_loss: 0.3598 - val_accuracy: 0.9133\n",
            "Epoch 10/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.3985 - val_accuracy: 0.9100\n",
            "Test...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.97      0.95       264\n",
            "           1       0.70      0.44      0.54        36\n",
            "\n",
            "    accuracy                           0.91       300\n",
            "   macro avg       0.81      0.71      0.75       300\n",
            "weighted avg       0.90      0.91      0.90       300\n",
            "\n",
            "Reporting Category:  Information Source\n",
            "Train on 942 samples, validate on 300 samples\n",
            "Epoch 1/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.3898 - accuracy: 0.9045 - val_loss: 0.3023 - val_accuracy: 0.9200\n",
            "Epoch 2/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.2581 - accuracy: 0.9321 - val_loss: 0.2827 - val_accuracy: 0.9200\n",
            "Epoch 3/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.2499 - accuracy: 0.9321 - val_loss: 0.2796 - val_accuracy: 0.9200\n",
            "Epoch 4/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.2489 - accuracy: 0.9321 - val_loss: 0.2806 - val_accuracy: 0.9200\n",
            "Epoch 5/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.2494 - accuracy: 0.9321 - val_loss: 0.2792 - val_accuracy: 0.9200\n",
            "Epoch 6/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.2489 - accuracy: 0.9321 - val_loss: 0.2787 - val_accuracy: 0.9200\n",
            "Epoch 7/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.2492 - accuracy: 0.9321 - val_loss: 0.2818 - val_accuracy: 0.9200\n",
            "Epoch 8/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.2472 - accuracy: 0.9321 - val_loss: 0.2785 - val_accuracy: 0.9200\n",
            "Epoch 9/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.2476 - accuracy: 0.9321 - val_loss: 0.2807 - val_accuracy: 0.9200\n",
            "Epoch 10/10\n",
            "942/942 [==============================] - 2s 2ms/step - loss: 0.2458 - accuracy: 0.9321 - val_loss: 0.2777 - val_accuracy: 0.9200\n",
            "Test...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96       276\n",
            "           1       0.00      0.00      0.00        24\n",
            "\n",
            "    accuracy                           0.92       300\n",
            "   macro avg       0.46      0.50      0.48       300\n",
            "weighted avg       0.85      0.92      0.88       300\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXYYas7FUYwP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "c5cc07ad-340b-4cfb-ea99-f5c7a0a1b9ea"
      },
      "source": [
        "print(train_body[id_num],\"\\n\")\n",
        "for i in range(len(show_results)):\n",
        "  print(categories[i])\n",
        "  print(show_results[i])\n",
        "  print('\\n\\n')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i have been on lipitor for 10 years for heart maintenance 20mg iwas changed to 10mg and added tricor 145 mg because of high ldl i do experience leg weakness and cramping cramping relieved with potassium \n",
            "\n",
            "Uncertainity of Post_Diagnosis\n",
            "                                            word_att                                               word\n",
            "0  {'i': 0.043330774, 'have': 0.0440861, 'been': ...  i have been on lipitor for 10 years for heart ...\n",
            "1  {'20mg': 0.023785641, 'iwas': 0.02562928, 'cha...  20mg iwas changed to 10mg and added tricor 145...\n",
            "\n",
            "\n",
            "\n",
            "Results and Side-Effects Observed\n",
            "                                            word_att                                               word\n",
            "0  {'i': 0.0038137329, 'have': 0.001874147, 'been...  i have been on lipitor for 10 years for heart ...\n",
            "1  {'20mg': 0.35244894, 'iwas': 0.237391, 'change...  20mg iwas changed to 10mg and added tricor 145...\n",
            "\n",
            "\n",
            "\n",
            "Medical Assistance\n",
            "                                            word_att                                               word\n",
            "0  {'i': 0.042242263, 'have': 0.038987696, 'been'...  i have been on lipitor for 10 years for heart ...\n",
            "1  {'20mg': 0.03652604, 'iwas': 0.041870013, 'cha...  20mg iwas changed to 10mg and added tricor 145...\n",
            "\n",
            "\n",
            "\n",
            "Diet and Maintenance\n",
            "                                            word_att                                               word\n",
            "0  {'i': 0.033161703, 'have': 0.028974101, 'been'...  i have been on lipitor for 10 years for heart ...\n",
            "1  {'20mg': 0.052196566, 'iwas': 0.066062875, 'ch...  20mg iwas changed to 10mg and added tricor 145...\n",
            "\n",
            "\n",
            "\n",
            "Information Source\n",
            "                                            word_att                                               word\n",
            "0  {'i': 0.052641362, 'have': 0.047112007, 'been'...  i have been on lipitor for 10 years for heart ...\n",
            "1  {'20mg': 0.03417453, 'iwas': 0.044300538, 'cha...  20mg iwas changed to 10mg and added tricor 145...\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_F91DU4WGX8",
        "colab_type": "text"
      },
      "source": [
        "#dummy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTwpWmo3Jx3e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "34d85a0d-b1d7-4314-fcc6-c87ea84f75b5"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "print('Pad sequences (samples x time)...')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen_sentence * maxlen_word)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen_sentence * maxlen_word)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen_sentence * maxlen_word)\n",
        "x_train = x_train.reshape((len(x_train), maxlen_sentence, maxlen_word))\n",
        "x_test = x_test.reshape((len(x_test), maxlen_sentence, maxlen_word))\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "#categories = ['Uncertainity of Post_Diagnosis', 'Results and Side-Effects Observed', 'Medical Assistance', 'Diet and Maintenance', 'Information Source']\n",
        "category='Uncertainity of Post_Diagnosis'\n",
        "y_train_label = df_train[category].to_list()\n",
        "y_test_label = df_test[category].to_list()\n",
        "y_train = to_categorical(y_train_label, num_classes=None)\n",
        "y_test = to_categorical(y_test_label, num_classes=None)\n",
        "han=HAN()\n",
        "print('Build model...')\n",
        "model = han.get_model()\n",
        "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
        "model.fit(x_train,y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              callbacks=[early_stopping],\n",
        "              validation_data=(x_test, y_test))\n",
        "print('Test...')\n",
        "result = model.predict(x_test)\n",
        "predictions = [np.argmax(x) for x in result]\n",
        "print(classification_report(y_test_label,predictions))\n",
        "\"\"\"for category in categories:\n",
        "    print('Reporting Category: ', category)\n",
        "\n",
        "    y_train_label = df_train[category].to_list()\n",
        "    y_test_label = df_test[category].to_list()\n",
        "\n",
        "    y_train = tf.keras.utils.to_categorical(y_train_label, num_classes=None)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test_label, num_classes=None)\n",
        "\n",
        "    model.fit(x_train,y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              callbacks=[early_stopping],\n",
        "              validation_data=(x_test, y_test))\n",
        "    print('Test...')\n",
        "    result = model.predict(x_test)\n",
        "    predictions = [np.argmax(x) for x in result]\n",
        "    print(classification_report(y_test_label,predictions))\"\"\""
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pad sequences (samples x time)...\n",
            "x_train shape: (942, 16, 25)\n",
            "x_test shape: (300, 16, 25)\n",
            "Build model...\n",
            "Train...\n",
            "Train on 942 samples, validate on 300 samples\n",
            "Epoch 1/10\n",
            "864/942 [==========================>...] - ETA: 0s - loss: 0.4979 - accuracy: 0.8009"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-204-765cdb7e2d06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m               validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGaYdRLkJ6GT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "49bca139-bb23-4542-bbeb-a7b619aa60b6"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
              "       [ 9, 10, 11, 12, 13, 14, 15, 16,  9, 17, 18, 19,  8,  7, 20,  8,\n",
              "         7, 21, 22, 23, 24,  8,  7,  9, 10]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igCo43pBKRhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hn=han.show_word_attention(x_train[0])\n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrNsXmctYd6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_attention_to_df(sent_tokenized_review, word_att):             \n",
        "        ori_sents = sent_tokenized_review\n",
        "        # split sentences into words\n",
        "        ori_words = [x for x in ori_sents.split()]\n",
        "        # truncate attentions to have equal size of number of words per sentence\n",
        "        ln=len(ori_words)\n",
        "        f=ln%maxlen_word\n",
        "        f1=ln//maxlen_word  \n",
        "        \n",
        "        words_attn=[]\n",
        "        for i in word_att[::-1]:\n",
        "          l=min(len(i),ln)\n",
        "          words_attn.extend(i[::-1][:l])\n",
        "          ln=ln-l\n",
        "          if ln<=0:\n",
        "            break\n",
        "  \n",
        "        words_attn.reverse()    \n",
        "          \n",
        "        f1=f1+1\n",
        "        if f==0:\n",
        "          f=maxlen_word\n",
        "          f1=f1-1\n",
        "        mylist1=[]\n",
        "        mylist2=[]\n",
        "        dict_attn=dict(zip(ori_words,words_attn))\n",
        "        dict_items = dict_attn.items()       \n",
        "        k1=0\n",
        "        for i in range(f1):\n",
        "          mylist1.append(dict(zip(ori_words[k1:f+k1],words_attn[k1:k1+f])))\n",
        "          listToStr = ' '.join(map(str, ori_words[k1:k1+f]))         \n",
        "          mylist2.append(listToStr)\n",
        "          k1=f\n",
        "          f=f+maxlen_word \n",
        "         \n",
        "\n",
        "        return pd.DataFrame(zip(mylist1,mylist2),\n",
        "                    columns=['word_att', 'word'])"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viaLzYPqqSHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_word_attention(id):\n",
        "  df=word_attention_to_df(train_body[id],han.show_word_attention(x_train[id]))\n",
        "  print(train_body[id],\"\\n\")\n",
        "  print(df)  "
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMjecTxDqLbA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "30e7da98-5938-4917-f6e2-37d5e5aa2123"
      },
      "source": [
        "id_num=4\n",
        "show_word_attention(id_num)"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i have been on lipitor for 10 years for heart maintenance 20mg iwas changed to 10mg and added tricor 145 mg because of high ldl i do experience leg weakness and cramping cramping relieved with potassium \n",
            "\n",
            "                                            word_att                                               word\n",
            "0  {'i': 0.040872823, 'have': 0.040732674, 'been'...  i have been on lipitor for 10 years for heart ...\n",
            "1  {'20mg': 0.034686994, 'iwas': 0.035907034, 'ch...  20mg iwas changed to 10mg and added tricor 145...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYtPddThxLVg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7732f0eb-97fa-491b-cb06-8e73c77e2ea4"
      },
      "source": [
        "len(train_body[8].split())"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTqYlNw94rPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}